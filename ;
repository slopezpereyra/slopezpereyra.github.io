\documentclass[a4paper, 12pt]{article}

\usepackage{xcolor}
\usepackage{mdframed}
\definecolor{shadecolor}{rgb}{0.1,0.1,0.1}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{amsmath, amssymb}
\usepackage{newtxtext} \usepackage{newtxmath}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example} \newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\usepackage[pdftex]{graphicx}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png,.jpg}
\newmdenv[backgroundcolor=orange!25,
            leftline=false,
            rightline=false,
            bottomline=true,
            linewidth=2pt,
            linecolor=black]{myframe}
\newmdenv[backgroundcolor=blue!35,
            leftline=false,
            rightline=false,
            bottomline=true,
            linewidth=2pt,
            linecolor=black]{helpframe}

\begin{document}


\begin{titlepage}
   \begin{center}
       \vspace*{1cm}

       \Huge
       \textbf{Modelos y simulación - Prácticos}

       \vspace{0.5cm}
        FAMAF - UNC
            
       \vspace{1.5cm}
       \large
       \textbf{Severino Di Giovanni}
       \normalsize


       \vspace{5.0cm}
       \begin{figure}[h!]
       \centering
        \includegraphics[width=0.5\textwidth]{../Images/UPA.jpg}
       \end{figure}

       \vfill
            
            
     
   \end{center}
\end{titlepage}


\shipout\null

 \begin{figure}[h!]
 \centering
  \includegraphics[width=0.5\textwidth]{../Images/SeverinoDiGiovanni.jpg}
 \caption{Severino Di Giovanni, el autor de este apunte. Un anarquista
   libertario, murió luchando por la libertad. Como él, otros miles han muerto
   para que nosotros gocemos de los derechos que tenemos. No te dejes engañar
   por los tristes pregoneros del egoísmo. Amá a tu prójimo y no olvides que si
   sus derechos se vulneran, los tuyos también. Ayudá a tu compañero de estudio,
 defendé tu universidad. }
 \end{figure}

\pagebreak
\tableofcontents
\newpage


\section{P1} 

\begin{myframe}
\textbf{(1)} Un geólogo recoge 10 especímenes de A y 12 de B. 15 son elegidos
al azar. De la función de probabilidad de masa del número de especímenes de tipo
A elegidos.
\end{myframe}
~


\textbf{Solución.} Sea $\Omega$ el espacio muestral, con cada $\omega \in \Omega$ una de las
posibles selecciones de $15$ de entre los $22$ especímenes. Claramente,

\begin{equation*}
  \left| \Omega \right| = \binom{22}{15}
\end{equation*}

Sea $X$ la variable aleatoria para el número de especímenes de tipo $A$ en una
selección aleatoria de $\Omega$. Claramente, para tener $X = k$, con $3 \leq k
\leq 10$, la selección debe ser tal que $k$ muestras sean de tipo $A$ y $15 - k$
muestras sean de tipo $B$. Luego 

\begin{equation*}
  P(X = k) = \frac{\binom{10}{k} \binom{12}{15 - k}}{\binom{22}{15}}
\end{equation*}




\pagebreak 

\begin{myframe}
\textbf{(2)} Sean $X, Y$ independientes y exponenciales con parámetros $\lambda,
\mu$ respectivamente. Computar $f_{X | Y}(x \mid y)$ y $P(X < Y)$.
\end{myframe}

~


  $(a)$ Recordemos que $f_{X \mid Y}(x \mid y) = \frac{f_{X, Y}(x,
  y)}{f_Y(y)}$. Como $X, Y$ son independientes, su densidad conjunta es el
  producto de las densidades, y tenemos $f_{X \mid Y}(x \mid y) = (f_X(x)
  f_Y(y))/f_Y(y) = f_X(x)$.

  $(b)$ Veamos que

  \begin{align*}
    P(X < Y) 
    &= \int_{\mathbb{R}} f_Y(y) \int_{-\infty}^y f_X(x) ~ dx ~ dy \\ 
    &=\int_\mathbb{R} \mathcal{I}_{(0, \infty)} \cdot \mu e^{-\mu y}
    \int_{-\infty}^y \mathcal{I}_{(0, \infty)} \lambda e^{-\lambda x} ~ dx ~ dy
    \\ 
    &=\int_0^\infty \mu e^{-\mu y}\int_0^y \lambda e^{-\lambda ~ x}
  \end{align*}

Ahora bien, con $w := -\lambda x, dw = -\lambda dx$, tenemos

\begin{align*}
  \int_0^y \lambda e^{-\lambda x} ~ dx 
  &= -\int_0^{w(y)}  e^w ~ dw \\ 
  &=-\int_0^{-\lambda y} e^w ~ dw \\ 
  &= - \left[ e^{-\lambda y} - e^0 \right] \\ 
  &=1 - e^{- \lambda y}
\end{align*}

Por lo tanto

\begin{align*}
P(X < Y) 
  &= \int_0^\infty \mu e^{-\mu y} \left( 1 - e^{-\lambda ~ y} \right) ~ dy \\ 
  &= \int_0^\infty \mu e^{-\mu y} ~ dy - \int_0^\infty \mu e^{-(\mu + \lambda) y
  } ~ dy \\ 
  &= \int_0^\infty f_{Y}(y) ~ dy - \frac{\mu}{\mu + \lambda}\int_0^\infty (\mu +
  \lambda)e^{-(\mu + \lambda) y} ~ dy \\ 
  &= 1 - \frac{\mu}{\mu + \lambda} \int_0^\infty f_{Z}(y) ~ dy &\left\{ Z \sim
  \mathcal{E}(\mu + \lambda) \right\}  \\ 
  &= 1 - \frac{\mu}{\mu + \lambda} \\ 
  &= \frac{\mu + \lambda}{\mu + \lambda} - \frac{\mu}{\mu + \lambda} \\ 
  &= \frac{\lambda}{\mu + \lambda}
\end{align*}

\pagebreak

\begin{myframe}
\textbf{(3)} Sean $X, Y$ independientes y exponenciales con el mismo parámetro
$\lambda$. Computar la densidad
condicional de $X$ dado que $X + Y = t$.
\end{myframe}

~

\textbf{Solución.} Por def. de densidad condicional, tomando $T := X + Y$,

  \begin{equation*}
    f_{X \mid T}(x\mid t) = \frac{f_{X, T}(x, t)}{f_T(t)} 
  \end{equation*}

Calculemos estas distribuciones.

\begin{align*}
  f_{X, T}(x, t) 
  &= P(X = x \cap X + Y = t) \\ 
  &= P(X = x \cap Y = t - x) \\ 
  &=P(X = x) \cdot P(Y = t - x) &\left\{ X, Y \text{ son independientes}
  \right\} \\ 
  &= f_X(x) \cdot f_Y(t - x) \\ 
  &= \lambda e^{-\lambda x} \mathcal{I}_{(0, \infty)}(x) \cdot \lambda e^{-\lambda(t - x)} ~ \mathcal{I}_{(0,
  \infty)}(t-x)\\ 
  &= \lambda^2e^{-\lambda t} ~ \mathcal{I}_{(0, \infty)}(t-x) \mathcal{I}_{(0,
  \infty)}(x) \\ 
  &= \lambda^2e^{-\lambda t} ~ \mathcal{I}_{(x, \infty)}(t) \mathcal{I}_{(0,
  \infty)}(x) \\ 
\end{align*}

El último paso se justifica porque $\mathcal{I}_{(0, \infty)}(t-x)\mathcal{I}_{(0,
\infty)}(x) = \mathcal{I}_{(0, \infty)}(x) \mathcal{I}_{(x, \infty)}(t)$. En
efecto, $t - x$ pertenece a $(0, \infty)$ si y solo si $t > x$. Ahora bien,

\begin{align*}
  f_T(t) 
  &= f_{X+Y}(t) \\ 
  &= \int_{\infty}^t f_X(x) f_Y(t - x) ~ dx &\left\{ X, Y \text{ independientes}
  \right\} \\ 
  &= \int_{-\infty}^t \mathcal{I}_{(0, \infty)} \cdot \lambda e^{-\lambda x}
  \cdot \mathcal{I}_{(0, \infty)}(t-x) \lambda
  e^{-\lambda(t - x)} ~ dx \\ 
  &=\int_0^t \lambda^2 e^{-\lambda t} \cdot 
  \mathcal{I}_{(x, \infty)}(t) ~ dx \\ 
  &=\lambda^2 e^{-\lambda  t} \int_0^t \mathcal{I}_{(x, \infty)}(t)~ dx \\ 
  &=\lambda^2 e^{-\lambda  t} \int_0^t 1 ~ dx \qquad (\star)\\ 
  &=t\lambda^2 e^{-\lambda  t} 
\end{align*}

El paso a $(\star)$ se justifica como sigue. Si la región de integración es $(0, t)$ y la variable de
integración es $x$, siempre se cumple que $x < t$, y por lo tanto
$\mathcal{I}_{(x, \infty)}(t) = 1$.

  \begin{equation*}
    \therefore \quad f_{X \mid T}(x\mid t) = \frac{f_{X, T}(x, t)}{f_T(t)} 
    = \frac{\lambda^2 e^{-\lambda t}\mathcal{I}_{(0, \infty)(x)}\mathcal{I}_{(x,
    \infty)}(t)}{t \lambda^2 e^{-\lambda t}} = \begin{cases}
      \frac{1}{t} & 0 \leq x \leq t  \\ 
      0 & c.c.
    \end{cases}
  \end{equation*}


\pagebreak 

\begin{myframe}
\textbf{(4)} Sean $N_1(t), N_2(t)$ procesos con $t \geq 0$ y tasas $\lambda = 1,
\mu = 2$ respectivamente. Sea $N(t) = N_1(t) + N_2(t)$. 

$(a)$ Calcular la probabilidad de que $N(1) = 2, N(2) = 5$.

$(b)$ Calcular la probabilidad de que $N_1(1) = 1$ dado $N(1) = 2$.

$(c)$ Calcular la probabilidad de que el segundo arribo en $N_1$ ocurra antes
que el tercer arribo en $N_2(t)$.
\end{myframe}

~

$(a)$ Es muy fácil porque $N(t)$ es un proceso de Poisson con parámetro $\lambda
+ \mu = 3$, con lo cual $P(N(t) = k) = e^{-3} \frac{3^k}{k!}$ para cualquier $k
\geq 0$ entero.

~ 

$(b)$ Claramente, si $N(1) = 2$, la probabilidad de que $N_1(1) = 1$ es la
probabilidad de que $N_2(1) = 1$.

\begin{align*}
  P(N_1(1) = 1 \mid N(1) = 2) 
  &= \frac{P(N_1(1) = 1 \cap N(1) = 2)}{P( N(1) = 2 )} \\ 
  &= \frac{ P(N_1(1) = 1 \cap  N_1(1) + N_2(1) = 2) }{P(N(1) = 2)} \\ 
  &= \frac{ P(N_1(1) = 1 \cap N_2(1) = 1) }{P(N(1) = 2)} \\ 
  &=\frac{ P(N_1(1) = 1) \cdot P(N_2(1) = 1) }{P(N(1) = 2)} &\left\{
  \text{Independencia de } N_1, N_2 \right\} 
\end{align*}

Ahora solo resta ver que 

\begin{equation*}
  e^{-\lambda ~ l} \frac{( \lambda l)^k}{k!} \leq e^{- \mu ~ l} \frac{( \mu l )^k}{k!} \iff
  e^{-\lambda l} \lambda^k \leq e^{-\lambda \mu} \mu^k \iff \lambda  \ln
  \lambda \geq \mu  \ln \mu
\end{equation*}

\begin{align*}
  P(N_1(1) = 1) &= e^{-1} \frac{1^1}{1!} = e^{-1} \approx 0.368 \\ 
  P(N_2(1) = 1)&= e^{-2} \frac{2^1}{1!} = 2e^{-2} \approx 0.27\\ 
  P(N(1) = 2) &= e^{-3} \frac{3^2}{2!} = \frac{9}{2}e^{-3} \approx 0.224
\end{align*}

\begin{equation*}
  \therefore P(N_1(1) = 1 \mid N(1) = 2) \approx \frac{0.368 \cdot 0.27}{0.224}
  = 0.443
\end{equation*}

Otra forma de calcularlo es usando el teorema de Bayes:

\begin{align*}
  P(N_1(1) = 1 \mid N(1) = 2) 
  &= \frac{P(N(1) = 1 \mid N_1(1) = 1) P(N_1(1) =
  1)}{P(N(1) = 1)} \\ 
  &= \frac{P(N_1(1) + N_2(1) = 1 \mid N_1(1) = 1) P(N_1(1) = 1)}{P(N(1) = 1)} \\ 
  &=\frac{ P(N_2(1) = 1) \cdot P(N_1(1) = 1) }{P(N(1) = 2)}
\end{align*}

que es lo mismo que tuvimos antes.

~

$(c)$ Se nos pide la probabilidad de que el segundo arribo en $N_1(t)$ ocurra antes
que el tercer arribo en $N_2(t)$. A primera vista no parece, pero este es
simplemente un problema de conteo. 

Primero, observemos que el evento "el segundo arribo en $N_1(t)$ ocurre antes
que el tercero de $N_2(t)$" es equivalente al evento "de $4$ eventos en $N(t)$,
al menos dos provienen de $N_1(t)$". Sea $\omega$ este evento.

La probabilidad de que un evento dado de $N(t)$ provenga de $N_1(t)$ es
$\frac{\lambda}{\lambda + \mu}$. La probabilidad de que un evento dado de $N(t)$
provenga de $N_2(t)$ es $\frac{\mu}{\lambda + \mu}$.

\begin{align*}
  \therefore ~ P(\omega) 
  &= \binom{4}{2} \frac{\lambda^2\mu^2}{( \lambda + \mu
  )^4} + \binom{4}{3} \frac{\lambda^3 \mu}{(\lambda + \mu)^4} +
  \binom{4}{4}\frac{\lambda^4}{(\lambda+ \mu)^4}
\end{align*}

Sustituyendo $\lambda, \mu$ por sus valores y calculando los binomiales, que son
fáciles, sale el resultado.

\pagebreak 

\begin{myframe}
\textbf{(5)} Sean $X, Y$ independientes con distribuciones de Poisson
parametrizadas bajo $\lambda, \mu$ respectivamente. Probar que 
$Z = X + Y \sim \mathcal{P}(\lambda + \mu)$.
\end{myframe}


Como son independientes, la distribución de su suma es dada por la "convolución"
discreta de sus distribuciones. Si con $\mathbb{N}$ denotamos los naturales y
el cero, entonces

\begin{align*}
  f_{Z}(z)  
  &=\sum_{k \in \mathbb{N} } p_X(k) p_Y(z - k)  \\ 
  &=\sum_{k \in \mathbb{N}} \left( e^{-\lambda} \frac{\lambda^k}{k!} \right)
  \left( e^{- \mu} \frac{\mu^{z - k}}{(z-k)!} \right)  \\ 
  &=e^{-\lambda }e^{-\mu} \sum_{k \in \mathbb{N}} \frac{\lambda^k
  \mu^{z-k}}{k!(z-k)!} \\ 
  &= \frac{\eta}{z!} \sum_{k \in \mathbb{N}} \lambda^k \mu^{z-k}\frac{z!}{k!(z -
k)!} &\left\{ \eta := e^{-\lambda }e^{-\mu} \right\}  \\ 
     &= \frac{\eta}{z!} \sum_{k\in \mathbb{N}} \lambda^k \mu^{z-k} \binom{z}{k}
\end{align*}

Ahora bien, como $Y \sim \mathcal{P}(\mu)$, $Im(Y) = \mathbb{N}$, y por lo tanto
$p_Y(u) = 0$ si $u < 0$. En otras palabras, dado un $k$ arbitrario, 
$p_Y(z - k) \neq 0 \iff z - k \geq 0 \iff k \leq z$.

\begin{equation*}
  &\therefore \frac{\eta}{z!} \sum_{k\in \mathbb{N}} \lambda^k \mu^{z-k}
  \binom{z}{k} 
  = \frac{\eta}{z} \sum_{k = 0}^z \lambda^k \mu^{z-k}\binom{z}{k} =
  \frac{\eta}{z!} (\lambda + \mu)^z = e^{-(\lambda + \mu)}\frac{(\lambda+\mu)^z}{z!}
\end{equation*}

por el teorema del binomio de Newton. 

$\therefore $ $Z \sim \mathcal{P}(\lambda + \mu)$. $\qquad \blacksquare$

\pagebreak 

\section{P3}

\begin{myframe}
\textbf{(2)} (Todas las variables en este problema son uniformes continuas e
independientes en
$(0, 1)$).
En un juego, se simula la variable aleatoria $U$.
Si $U < \frac{1}{2}$ se suman dos números aleatorios. Si $U \geq
\frac{1}{2}$, se suman tres. El resultado $X$ de cualquiera de estas sumas
es el puntaje, y se tiene éxito si $X \geq 1$.

~

$(a)$ Dar la probabilidad de ganar y $(b)$ escribir una simulación para estimar
la probabilidad de ganar.
\end{myframe}

~

$(a)$ Por ley de probabilidad total,

\begin{align*}
  &P(X_1 + X_2 \geq 1 \mid U < \frac{1}{2})P(U < \frac{1}{2}) + P(X_1 +
  X_2 + X_3 \geq 1\mid
  U \geq \frac{1}{2})P(U \geq \frac{1}{2}) \\ 
  =&P(X_1 + X_2 \geq 1)P(U < \frac{1}{2}) + P(X_1 + X_2 + X_3 \geq 1)P(U \geq
  \frac{1}{2})\\
  =&P(X_1 + X_2 \geq 1)\cdot \frac{1}{2} + P(X_1 + X_2 + X_3 \geq
  1)\cdot \frac{1}{2}
\end{align*}

Estudiemos $P(X_1 + X_2 = z)$. Veamos que si $z \in [0, 1]$, entonces

\begin{equation*}
  P(X_1 + X_2 = z) = \int_0^z f_{X_1}(x)f_{X_2}(z-x) ~ dx = \int_0^z 1 ~ dx 
  =z
\end{equation*}

Si $z \in (1, 2]$, entonces hacemos variar $X_1$ entre $[z-1, 1]$ y requerimos
$X_2 = z - X_1$:

\begin{equation*}
  P(X_1 + X_2 = z) 
  = \int_{z-1}^{1} f_{X_1}(x) f_{X_2}(z - x) ~  dx = 2 -z
\end{equation*}



\begin{equation*}
  \therefore ~ f_{X_1 + X_2}(x) = \begin{cases}
    x & 0 \leq x \leq 1 \\ 
    2 - x & 1 < x \leq 2 \\ 
    0 & c.c.
  \end{cases}
\end{equation*}

Por lo tanto,

\begin{equation*}
  P(X_1 + X_2 \geq 1) = \int_1^2 f_{X_1 + X_2}(x) ~ dx = \int_1^2(2-x) ~ dx =
  1 / 2
\end{equation*}

Resta ver que 

\begin{align*}
  P(X_1 + X_2 + X_3 \geq 1) 
  &= P(X_1 + X_2 \geq 1 - X_3) \\ 
  &=\int_0^1  P(X_1 + X_2 \geq 1 - z) P(X_3 = z) ~ dz &\left\{ \text{Por prob.
  total} \right\} \\ 
  &=\int_0^1 \left(1 - \frac{(1-z)^2}{2}\right) \cdot 1 ~ dz &\left\{ \text{Por }
  \star\right\} \\ 
  &=5 / 6
\end{align*}

dado que para $z \in [0, 1]$

\begin{equation*}
  ( \star ) \qquad P(X_1 + X_2 \geq 1 - z) = 1 - P(X_1 + X_1 \leq 1 - z) = 1 - \int_0^{1-z} x ~ dx = 1 - \frac{ (1-z)^2 }{2}
\end{equation*}

\begin{align*}
  \therefore  ~ ~ ~
  &P(X_1 + X_2 \geq 1)\cdot \frac{1}{2} + P(X_1 + X_2 + X_3 \geq
  1)\cdot \frac{1}{2} \\
  = &\frac{1}{2} \cdot \frac{1}{2} + \frac{5}{6} \cdot
  \frac{1}{2}  \\
  = &\frac{1}{4} + \frac{5}{12} \\
  =&\frac{2}{3}
\end{align*}

~

$(b)$ En Python, 


\begin{verbatim}[python]
from random import uniform

def sim_2(n):

  wins = 0

  # S = summation of uniform rvs
  def S(k):
    return sum( [uniform(0, 1) for _ in range(k)] )

  for _ in range(n):

    u = uniform(0, 1)
 
    X = S(2) if u < 0.5 else S(3)
    wins = wins + 1 if X >= 1 else wins

  return wins/n
\end{verbatim}


\pagebreak 

\begin{myframe}
\textbf{(3)} Las máquinas tragamonedas usualmente generan un premio cuando hay un acierto. Supongamos que se genera el acierto con el siguiente esquema: se genera un número aleatorio, y

\begin{itemize}
\item si es menor a un tercio, se suman dos nuevos números aleatorios
\item si es mayor o igual a un tercio, se suman tres números aleatorios .
\end{itemize}

Si el resultado de la suma es menor o igual a 2, se genera un acierto.

~

$a)$ ¿Cuál es la probabilidad de acertar? $(b)$ Simular.
\end{myframe}


Sea $Z_n = \sum_{i=1}^n U_i$ con $U_i \sim \mathcal{U}(0, 1)$ para $1 \leq i
\leq n$. Por ley de probabilidad total, se llega con facilidad a

\begin{equation*}
  P(\text{Ganar}) = P(Z_2 \leq 2) \cdot \frac{1}{3} + P(Z_3 \leq 2) \cdot
  \frac{2}{3}
\end{equation*}

Claramente, $P(Z_2 \leq 2) = 1$, con lo cual solo queda computar $P(Z_3 \leq
2) = P(Z_2 \leq 2 - U_3)$. La densidad de $Z_2$ fue calculada en el punto
$\textbf{(2)}$. Para calcular $P(Z_3 \leq 2)$, podemos hacer variar a $U_3$ en
todo el rango $[0, 1]$ y a $Z_2$ en todo el rango $[ 0, 2 - U_3 ]$:

\begin{align*}
  P(Z_3 \leq Z_2) 
  &= P(Z_2 \leq 2 - U_3) \\ 
  &=\int_0^1 P(U_3 = z) \int_{0}^{2-z} f_{Z_2}(x) ~ dx ~ dz \\ 
  &=\int_0^1 \left[ \int_0^1 f_{Z_2}(x) ~ dx + \int_1^{2 - z} f_{Z_2}(x) ~ dx
  \right] ~ dz \\ 
  &=\int_0^1 \left[ \int_0^1 x ~ dx + \int_1^{2-x}(2-x) ~ dx  \right] ~ dz \\ 
  &= \int_0^1 \left[ \frac{1}{2} - \frac{z^2}{2} + \frac{1}{2}\right]  ~ dz \\ 
  &=\int_0^1 1 - \frac{z^2}{2} ~ dz \\ 
  &= 5 / 6
\end{align*}


Una forma alternativa de calcular esta probabilidad es hacer variar a $Z_2$ en
todo el rango $[0, 2]$ y luego hacer variar a $U_3$ en todo el rango $[0, 2 -
Z_2]$:

\begin{align*}
  P(Z_3 \leq 2) 
  &=P(Z_2 \leq 2 - U_3)\\
  &= \int_0^2 f_{Z_2}(x) \int_0^{2-x} f_{U_3}(y) ~ dy ~ dx \\ 
  &= \int_0^1 f_{Z_2}(x) \int_0^{1} f_{U_3}(y) ~ dy ~ dx + \int_1^2 f_{Z_2}(x)
  \int_0^{2-x} f_{U_3}(y) ~ dy ~ dx &\left\{ \text{Por } \star \right\}  \\
                                    &=\int_0^1 x \int_0^1 1 ~ dy ~ dx +
                                    \int_{1}^2 (2-x) \int_0^{2 - x} 1 ~ dy ~ dx\\
  &=\int_0^1 x ~ dx + \int_1^2 (2-x)^2 ~ dx \\
  &=\frac{1}{2} + \frac{1}{3} \\ 
  &= \frac{5}{6}
\end{align*}

\begin{myframe}
  ($\star$) Si $Z_2$ varía en $[0, 1]$, $Z \leq 2 - U_3$ para cualquier valor de
  $U_3$, con lo cual hacemos a $U_3$ variar en todo el rango $[0, 1]$.  Pero si
  $Z_2$ varía en $[1, 2]$, entonces $U_3$ solo puede tomar valores en $[0, 2 -
  Z_2]$.
\end{myframe}

\begin{align*}
  \therefore \qquad P(\text{Ganar}) 
  &= P(Z_2 \leq 2) \cdot \frac{1}{3} + P(Z_3 \leq 2) \cdot
  \frac{2}{3} \\ 
  &= \frac{1}{3} + \frac{5}{6}\frac{2}{3} \\ 
  &= 0.\overline{8}
\end{align*}

~

$(b)$En Python, una implementación posible es:

\begin{verbatim}
from random import uniform

def sim_3(n):
  wins = 0 

  # S = summation of uniform r.vs
  def S(k): 
    return sum( [uniform(0, 1) for _ in range(k)] )

  for _ in range(n):
    u = uniform(0, 1)
    X = S(2) if u < 1/3 else S(3)
    wins = wins + 1 if X <= 2 else wins

  return wins/n
\end{verbatim}


\pagebreak 

\begin{myframe}
(\textbf{4}) Un supermercado posee 3 cajas. Por una cuestión de ubicación, el
40\% de los clientes eligen la caja 1 para pagar, el 32\% la caja 2, y el 28\%
la caja 3. 

El tiempo que espera una persona para ser atendida en cada caja
distribuye exponencial con medias de 3, 4 y 5 minutos respectivamente. 

~

$(a)$ ¿Cuál es la probabilidad de que un cliente espere menos de 4 minutos para ser
atendido? 

$(b)$ Si el cliente tuvo que esperar más de 4 minutos. ¿Cuál es la
probabilidad de que el cliente haya elegido cada una de las cajas? 


$(c)$ Simule el
problema y estime las probabilidades anteriores con 1000 iteraciones.
\end{myframe}

~ 

Recordemos que la media de la exponencial es $1 / \lambda$, es decir que
$\lambda_1 = 1 / 3, \lambda_2 = 1 / 4, \lambda_3 = 1 / 5$.

El evento "un cliente espera menos de $4$ minutos en ser atendido" se divide en
un sub-eventos correspondiente por cada caja. Más formalmente, si $T$ es el
tiempo de espera de un cliente arbitrario, y $C_i$ el evento de que el cliente
elige la i-ésima caja, entonces por ley de probabilidad total:

\begin{equation*}
  P(T \leq 4) = \sum_{i=1}^3 P(C_i)P(T \leq 4 \mid C_i) = \sum_{i=1}^3 P(C_i)
  P(X_i \leq 4)
\end{equation*}

donde $X_i$ es una exponencial con parámetro $\lambda_i$. En general, 

\begin{equation*}
  \int_0^4 \lambda_i e^{-\lambda_i x} ~dx = 1 - e^{-4\lambda_i}
\end{equation*}

de lo cual se sigue: 

\begin{equation*}
  P(X_i \leq 4) = \begin{cases}
    0.736 & i = 1 \\ 
    0.632 & i = 2\\ 
    0.55 & i = 3
  \end{cases}
\end{equation*}

\begin{equation*}
  \therefore P(T\leq 4) = 0.40 \cdot 0.736 + 0.32 \cdot 0.632 + 0.28 \cdot 0.55
  = 0.65064
\end{equation*}

~

$(b)$  Sale fácil con teorema de Bayes y los resultados de $(a)$:

  \begin{equation*}
    P(C_i \mid T \leq 4) 
    &= \frac{P(T \leq 4 \mid C_i)P(C_i)}{P(T \leq 4)}
  \end{equation*}

~

$(c)$ En Python,

\begin{verbatim}
from numpy.random import exponential
from random import uniform

def sim_4(n):

  wins = 0
  rates = [1/3, 1/4, 1/5]

  def choose_rate(x):
    if x <= 0.4:
      return 0 
    return 1 if u <= 0.4 + 0.32 else 2

  for _ in range(n): 

    u = uniform(0, 1)
    rate = rates[choose_rate(u)]

    e = exponential(1/rate)

    wins = wins + 1 if e <= 4 else wins

  return wins/n 


\end{verbatim}

\pagebreak 

\begin{myframe}
\textbf{(5)} Calcule con el método de Monte Carlo y escriba simulaciones para
cada una de las siguientes integrales. 
\end{myframe}

~ 

\begin{myframe}
$(a)$ $\int_0^1 (1-x^2)^{3 / 2} ~ dx$
\end{myframe}

Sea $f_U(x)$ la densidad de $U \sim \mathcal{U}(0, 1)$. Entonces 

\begin{equation*}
  \theta := \int_0^1(1-x^2)^{3 / 2} ~ dx = \int_0^1 (1-x^2)^{3 / 2} f_U(x) ~ dx =
  \mathbb{E}\left[ g(U) \right] 
\end{equation*}

con $g(x) = (1-x^2)^{3 / 2}$. Por lo tanto, $\theta \approx \frac{1}{n}
\sum_{i=1}^n g(u_i)$ con $u_1, \ldots, u_n$ realizaciones de $U_1, \ldots, U_n
\sim \mathcal{U}(0, 1)$. Es decir, 

\begin{equation*}
  \theta \approx \frac{1}{n} \sum^n (1 - u_i^2)^{3 / 2}
\end{equation*}

En Python,

\begin{verbatim}
from random import uniform
def sim_5a(n):
  uniforms = [uniform(0, 1) for _ in range(n)]
  return 1/n * sum( [ (1 - u**2)**1.5  for u in uniforms ] )

\end{verbatim}

\pagebreak


\begin{myframe}
$(b) \int_2^3 \frac{x}{x^2-1} ~ dx$
\end{myframe}


Si hacemos $u = x - 2$, tal que $du = dx$ y $x = 2 + u$, tenemos 

\begin{equation*}
  \theta := \int_2^3 \frac{x}{x^2 - 1} ~ dx = \int_0^1 \frac{2+u}{(2+u)^2 - 1} ~ du =
  \int_0^1 \frac{2+u}{u^2 - 4u + 3}
\end{equation*}

Entonces $\theta = \int_0^1 g(x)f_U(x) ~ dx = \mathbb{E}\left[ g(U) \right] $
con $U \sim \mathcal{U}(0, 1), g(x) = \frac{2+x}{(2+x)^2 - 1}$. 

\begin{equation*}
  \therefore ~ \theta \approx \frac{1}{n}\sum_{i=1}^n \frac{2 + u_i}{u_i^2
  -4u_i + 3}
\end{equation*}

donde $u_1, \ldots, u_n$ son realizaciones de $U_1, \ldots, U_n \sim
\mathcal{U}(0, 1)$ independientes para $n \geq 1$. En Python, 

  
\begin{verbatim}
def sim_5b(n):
  uniforms = [uniform(0, 1) for _ in range(n)]
  return 1/n * sum( [ (2 + u)/((2+u)**2 - 1) for u in uniforms ] )
\end{verbatim}

Con $n = 1000$ obtuve $\theta \approx 0.487$, y el valor real de la integral es $\theta = 0.49$.

\pagebreak

\begin{myframe}
$(c) \int_0^\infty x(1+x^2)^{-2} ~ dx$
\end{myframe}


Sea $u := \frac{1}{x + 1}$, de manera que 

\begin{equation*}
  du = -\frac{1}{(x+1)^2} ~ dx = -u^2 ~ dx, \qquad x = \frac{1-u}{u} =
  \frac{1}{u} - 1
\end{equation*}

Entonces 

\begin{align*}
  \theta := \int_0^\infty g(x) ~ dx = -\int_0^1 \frac{g(\frac{1}{u} - 1)}{u^2}
  ~ du = -\int_0^1 \frac{(\frac{1}{u} - 1)(1 + (\frac{1}{u} - 1)^2)^{-2}}{u^2} ~
  du
\end{align*}

Luego 

\begin{equation*}
  \theta = -\int_0^1 \psi(u) f_U(u) ~ du = -\mathbb{E}\left[ \psi(U) \right] 
\end{equation*}

con $\psi(u) = g(\frac{1}{u} - 1) / u^2$ y $U \sim \mathcal{U}(0, 1)$. En
consecuencia, 

\begin{equation*}
  \theta \approx \frac{1}{n}\sum_{i=1}^n \psi(u_i)
\end{equation*}

donde $u_1, \ldots, u_n$ son realizaciones de $U_1, \ldots, U_n \sim
\mathcal{U}(0, 1)$ independientes (no confundir con $u$, la variable de
integración) y $n \geq 1$. En Python, 

\begin{verbatim}
def sim_5c(n):
  uniforms = [uniform(0, 1) for _ in range(n)]

  def h(u):
    return (1/u - 1)*(1 + (1/u - 1)**2)**(-2) * (1/u**2)

  return 1/n * sum( [h(u) for u in uniforms] )
\end{verbatim}

Con $n = 1000$, la simulación dio $\varphi \approx 0.504$, y el valor real de la
integral es $\theta = \frac{1}{2}$.

\pagebreak

\begin{myframe}
$(d) \int_{-\infty}^\infty e^{-x^2} ~ dx$
\end{myframe}


Let $g(x) := e^{-x^2}$. Clearly, $g(-x) = e^{-(-x^2)} = g(x)$, which means $g$
is symmetric.

$\therefore ~ \int_{-\infty}^\infty e^{-x^2} ~ dx = 2\int_0^\infty e^{-x^2} ~
dx$. If we let 

\begin{equation*}
  \psi = \frac{1}{x+1}, \qquad d \psi = -\psi^2, \qquad x = \frac{1}{\psi}-1
\end{equation*}

Entonces

\begin{equation*}
  \theta := 2\int_0^\infty e^{-x^2} = 2\int_0^1 \frac{\exp\left(-\left(\left(\frac{1}{\psi} -
  1\right)^2\right)\right)}{\psi^2} f_U(\psi) ~ d\psi
\end{equation*}

donde $f_U$ es la densidad de $U \sim \mathcal{U}(0, 1)$. $\therefore
\theta = \mathbb{E}\left[ h(U) \right] $ con $h(x) =
\exp\left(-\left(\left(\frac{1}{x}-1\right)^2\right) /
x^2$. 

\begin{equation*}
  \therefore \quad \theta \approx \frac{1}{n}\sum_{i=1}^n 
  \frac{\exp\left(-\left(\left(\frac{1}{u_i} - 1\right)^2\right)\right)}{u_i^2}, \qquad n \geq 1
\end{equation*}

con $u_1, \ldots, u_n$ realizaciones de $U_1, \ldots, U_n \sim \mathcal{U}(0,
1)$. En Python,

\begin{verbatim}
from math import exp 
from random import uniform
  
def sim_5d(n):
    def h(u):
        return exp(-(( 1/u - 1 )**2)) / (u**2)

    uniforms = [uniform(0, 1) for _ in range(n)]
    return 2*sum([ h(u) for u in uniforms ]) / n
\end{verbatim}

La simulación con $n = 1000$ dio $\theta \approx 1.73$ que es casi exactamente
$\sqrt{\pi}$, el verdadero valor.

\begin{myframe}
\textbf{Observación.} El ejercicio se puede resolver aplicando directamente la
sustitución sin notar que $g$ es simétrica, aunque se dan pasos previos algo más
engorrosos:

\begin{align*}
  \int_{-\infty}^\infty g(x) ~ dx 
  &= \lim_{t \to -\infty}\int_t^0 g(x) ~dx +
  \lim_{t \to \infty} \int_0^\infty g(x) ~ dx \\ 
  &=-\int_{\lim_{t \to -\infty} \psi(t)}^0 \frac{ g(\frac{1}{\psi} - 1) }{\psi^2}
  ~ d\psi - \int_0^{ \lim_{t \to \infty} \psi(t)} 
  \frac{g(\frac{1}{\psi} - 1)}{\psi^2} ~ d\psi \\ 
  &=\int_0^1 \frac{g(\frac{1}{\psi} - 1)}{\psi^2} ~ d\psi + \int_0^1
  \frac{g(\frac{1}{\psi} - 1)}{\psi^2} ~ d\psi \\ 
  &= 2 \int_0^1 \frac{g(\frac{1}{\psi} - 1)}{\psi^2} ~ d\psi
\end{align*}

A partir de acá todo es igual.
\end{myframe}


\pagebreak

\begin{myframe}
$(e)$ $\theta := \int_0^1 \int_0^1 \exp(( x+y )^2) ~ dx ~ dy$
\end{myframe}


Let $X, Y \sim \mathcal{U}(0, 1)$ independent with densities $f_Y, f_U$. Clearly, their
joint density function is 

\begin{equation*}
  f_{X, Y}(x, y) = f_X(x)f_Y(y) = \mathcal{I}_{(0, 1) \times (0, 1)}
\end{equation*}

Then 

\begin{equation*}
  \theta = \int_0^1 \int_0^1 g(x, y) f_{X, Y}(x, y) ~ dx ~ dy = \mathbb{E}\left[
  g(X, Y)\right] 
\end{equation*}

\begin{equation*}
  \therefore \quad \theta \approx \frac{1}{n} \sum_{i=1}^{n} g(x_i, y_i), \qquad
  n \geq 1
\end{equation*}

where $(x_1, y_1), \ldots, (x_n,y_n)$ are realizations of $(X_1, Y_1),
\ldots, (X_n, Y_n)$, with $X_i, Y_i \sim \mathcal{U}(0, 1)$ for $1 \leq i \leq
n$.

In Python,

\begin{verbatim}
from math import exp 
from random import uniform 

def sim_5e(n):
    def g(x, y):
        return exp((x+y)**2)
    
    X = [uniform(0, 1) for _ in range(n)]
    Y = [uniform(0, 1) for _ in range(n)]

    return sum([ g(x, y) for x, y in zip(X, Y) ]) / n
\end{verbatim}

With $n = 100000$, the estimation was $4.902$, and the true value of the
integral according to Wolfram Alpha is $4.899$.


\pagebreak 

\begin{myframe}
\textbf{(7)} Definamos 

\begin{equation*}
  N = \min_{n} \left\{ n : \sum_{i=1}^n U_i > 1 \right\}, \qquad U_i \sim
  \mathcal{U}(0, 1)
\end{equation*}

$(a)$ Estimar $\mathbb{E}[N]$ generating $n$ values for $N$.

$(b)$ Calculate $E[N]$.

\end{myframe}


Notemos que $Im(N) = \left\{ 2, 3, \ldots \right\} $. Calculemos $p_N(n)$ la
función de probabilidad de masa de $N$. Para ello, observemos que 

\begin{equation*}
  P(N = n) = P\left(\sum_{i=1}^{n-1} U_i \leq 1 \cap U_n > 1 - \sum_{i=1}^{n-1}
  U_i\right)
\end{equation*}

Sea $X(n) = \sum_{i=1}^n U_i$ y, para simplificar la notación, usemos
$\varphi(n, x) := P(X(n) \leq x)$ para $x \geq 0$. Es decir, $\varphi(n, x)$ es
la función de probabilidad acumulada de $X(n)$. Entonces, la expresión de arriba
nos queda

\begin{equation*}
  P(N = n) = P(X(n-1) < 1 \cap U_n > 1 -X(n-1)) 
\end{equation*}

De acuerdo con la ley de probabilidad total,


\begin{align*}
  \varphi(n, x) 
  &= P(X(n) \leq x) \\
  &= P(U_n \leq x - X(n-1)) \\
  &= \int_{\mathbb{R}} P(U_n \leq x-z) P\left( X(n-1) = z \right) ~ dz \\ 
  &=\int_0^{x} (x-z) \frac{d}{dz} \varphi(n-1, z) ~ dz
\end{align*}

donde estamos utilizando el hecho de que la función de densidad es la derivada
de la función de probabilidad acumulada. Esta expresión recursiva de $\varphi(n, x)$
nos permite avanzar: si restringimos $x \in [0, 1]$,


\begin{align*}
  \varphi(2, x) 
  &= \int_0^x(x-z)\frac{d}{dz}\varphi(1, z) ~ dz \\
  &=\int_0^x(x-z) ~ dz \\ 
  &= \frac{x^2}{2}
\end{align*}

Siguiendo probando casos, obtenemos

\begin{align*}
  f(x, 3) &= \int_0^x(x-z)z ~ dz = \frac{x^3}{6} \\
  f(x, 4) &= \int_0^x(x-z)\frac{z^2}{2} ~ dz = \frac{x^4}{24} \\
          &\vdots \\ 
  f(x, n) &= (x-t) \frac{d}{dz}\varphi(x, n-1)(x:=z) ~ dz = \frac{x^n}{n!}
\end{align*}

Es decir, obtenemos una expresión general para la probabilidad acumulada de
$X(n)$, de lo cual se deduce que la densidad de $X(n)$ es 

\begin{equation*}
  \psi(n, x) = \frac{d}{dx} \frac{x^n}{n!} = \frac{nx^{n-1}}{n!} =
  \frac{x^{n-1}}{(n-1)!} = \varphi(n-1, x), \qquad 0 \leq x \leq 1
\end{equation*}

Por lo tanto,

\begin{align*}
  P(N = n) 
  &= P(X(n-1) < 1 \cap U_n > 1 -X(n-1)) \\ 
  &=\int_0^1 \psi(n-1, x) P(U_n > 1 - x) ~ dx \\ 
  &=\int_0^1 \frac{x^{n-2}}{(n-2)!}(1 - P(U_n \leq 1-x)) ~ dx \\ 
  &=\frac{1}{(n-2)!}\int_0^1  x^{n-1} ~ dx \\ 
  &=\frac{1}{(n-2)!} \frac{1^n}{n} \\ 
  &= \frac{1}{n(n-2)!}
\end{align*}

Por lo tanto, 

\begin{equation*}
  \mathbb{E}\left[ N \right] = \sum_{i=2}^\infty \frac{1}{(n-2)!} =
  \sum_{i=0}^\infty \frac{1}{n!} = e
\end{equation*}

La simulación está de acuerdo, porque en Python 

\begin{verbatim}
def sim_7(iterations):

  counts = []
  n, s = 0, 0

  for _ in range(iterations):
    while s <= 1:
      n += 1
      s += uniform(0, 1)
    counts.append(n)
    n, s = 0, 0
 
  return sum(counts)/len(counts)
\end{verbatim}

nos devuelve siempre aproximadamente $e$.

\pagebreak 

\begin{myframe}
$\textbf{(8)}$ Sea  

\begin{equation*}
  N = \max_{n} \left( \prod_{i=1}^n U_i \geq e^{-3} \right) 
\end{equation*}

con $\prod_{i=1}^0 U_i = 1$. Estimar mediante simulaciones $E[N]$ y $P(N = n)$
para $0 \leq n \leq 6$.

\end{myframe}
~

Estimar con simulaciones es aburrido y fácil. Más vale pensemos en el problema y
tratemos de dar $p_N$, la función de probabilidad de masa de $N$, y con ella
$\mathbb{E}\left[ N \right] $ de manera analítica.

Claramente, 

\begin{align*}
  P(N = n) 
  &= P \left( \prod_{i=1}^{n-1} U_i \geq e^{-3} \cap U_n \cdot \prod_{i=1}^{n-1}
  U_i < e^{-3}\right) \\ 
  &= P \left( \prod_{i=1}^{n-1} U_i \geq e^{-3} \cap U_n  <
    \frac{e^{-3}}{\prod_{i=1}^{n-1}U_i }\right) 
\end{align*}

Para simplificar la notación, hagamos $X(n) := \prod_{i=1}^n U_i$ y $\varphi(n,
x) = P(X(n) \leq x)$ para $x \in [0, 1]$. Es decir, $\varphi$ es la CDF de
$X(n)$. Asumimos que $X(0) = 1$, con lo cual $\varphi(0, x) = 1$. Es fácil ver
que $\varphi(1, x) = x$. Veamos además que 

\begin{align*}
  \varphi(2, x) 
  &= P(U_1 U_2 \leq x) \\ 
  &= P(U_1 \leq \frac{x}{U_2}) \\ 
  &= \int_0^1 P(U_1 \leq \frac{x}{z}) P(U_2 = z) ~ dz  \\ 
  &= \int_0^1 P\left( U_1 \leq \frac{x}{z} \right)  ~ dz
\end{align*}

Como $x / z \leq 1$ para $z \in [x, 1]$ y $x / z > 1$ para $z \in [0, x)$,

\begin{align*}
  \int_0^1 P\left( U_1 \leq \frac{x}{z} \right) ~ dz 
  &=\int_0^x P\left( U_1 \leq \frac{x}{z} \right) ~ dz + \int_x^1 P\left( U_1
  \leq \frac{x}{z} \right) ~ dz \\ 
  &= \int_0^x 1 ~ dz + \int_x^1 \frac{x}{z} ~ dz \\ 
  &= x + x \Big[ \ln z\Big]_x^1 \\ 
  &= x - x \ln x
\end{align*}

\begin{myframe}
  Una forma alternativa de dar con la probabilidad buscada es la siguiente:
  
\begin{align*}
  \varphi(2, x) 
  &= P(U_1 U_2 \leq x)  \\ 
  &=\int_0^x \int_0^1 ~ dy ~ dz + \int_x^1 \int_{0}^{x / z} ~ dy ~ dz \\ 
  &=\int_0^x ~ dz + \int_{x}^1 \frac{x}{z} ~ dz \\ 
  &= x + x \int_x^1 \frac{1}{z} ~ dz \\ 
  &= x + x \Big[ \ln z \Big]_{x}^1 \\ 
  &= x + x(\ln 1 - \ln x) \\ 
  &= x - x \ln x
\end{align*}

Esta forma es más simple en este caso particular, pero la que usamos antes
es más útil para el caso general.
\end{myframe}

Generalizando, llegamos a

\begin{align*}
  \varphi(n, x) 
  &= P\left(\prod_{i=1}^{n-1} U_i \leq \frac{x}{U_n}\right)  \\ 
  &= \int_0^1 \varphi\left(n-1, \frac{x}{z}\right) P(U_n = z) ~ dz \\ 
  &=\int_0^1 \varphi\left(n-1, \frac{x}{z}\right) ~ dz \\ 
  &=\int_0^x \varphi \left( n - 1, \frac{x}{z} \right) ~ dz + \int_x^1 \varphi
  \left( n - 1, \frac{x}{z} \right) \\ 
  &= \int_0^x 1 ~ dz + \int_x^{1} \varphi\left( n-1, \frac{x}{z} \right)  \\ 
  &= x + \int_x^1 \varphi\left( n-1, \frac{x}{z} \right) 
\end{align*}

Entonces, usando el hecho de que conocemos $\varphi(2, x)$, tenemos 

\begin{align*}
  \varphi(3, x) 
  &= x + \int_x^1 \varphi(2, \frac{x}{z}) ~ dz  \\ 
  &=x + \int_x^1 \frac{x}{z}-\frac{x}{z} \ln \frac{x}{z} \\ 
  &= x - x\ln x + \frac{x}{2} \ln^2 x
\end{align*}

\begin{align*}
  \varphi(4, x) 
  &= x + \int_x^1 \varphi(3, \frac{x}{z}) ~ dz  \\ 
  &=
  x + \int_x^1 \frac{x}{z} -\frac{x}{z} \ln\left( \frac{x}{z} \right) + \frac{x}{2z} \ln^2 \left( \frac{x}{z} \right)   \\ 
  &=x - x \ln x + \frac{x \ln^2 x}{2} - \frac{x \ln^3 x}{6}
\end{align*}

Vistos estos resultados, notamos (aunque no demostramos formalmente) que:

\begin{equation*}
  \varphi(n, x) = \sum_{k=0}^{n-1} \left( -1 \right)^{k} \frac{x \ln^k x}{k!}
\end{equation*}

Por lo tanto, la función de probabilidad de masa $\psi(n, x)$ del producto de $n$ uniformes
aleatorias es $\frac{d}{dx}\varphi(n, x)$, que es


\begin{align*}
  &\sum_{k=0}^{n-1}(-1)^{k} 
  \frac{\ln^k x + k \ln^{k-1} x}{k!} \\
  =&1 - (\ln x + 1) + \left(\ln x + \frac{ \ln^2 x }{2}\right) -
  \left( \frac{\ln^3 x}{6} + \frac{1}{2}\ln^2 x \right)  + \ldots \\ 
  =& (-1)^{n-1} \frac{\ln^{n-1} x}{(n-1)!}  &\left\{ \text{Suma telescópica} \right\} \\ 
\end{align*}
Luego 


\begin{align*}
  P(N = n) 
  &= P \left( \prod_{i=1}^{n-1} U_i \geq e^{-3} \cap U_n \cdot \prod_{i=1}^{n-1}
  U_i < e^{-3}\right) \\ 
  &= P \left( \prod_{i=1}^{n-1} U_i \geq e^{-3} \cap U_n  < \frac{e^{-3}}{\prod_{i=1}^{n-1}U_i < e^{-3}}
    \right) \\ 
  &= \int_{e^{-3}}^1 P(U_n < \frac{e^{-3}}{z}) P\left( \prod_{i=1}^{n-1} U_i
  = z \right) ~ dz \\ 
  &=\int_{e^{-3}}^1 \left( \frac{e^{-3}}{z} \right) \psi(n-1, z) ~ dz \\ 
  &=\int_{e^{-3}}^1 \left( \frac{e^{-3}}{z} \right) (-1)^{n-2} 
  \frac{\ln^{n-2} z}{(n-2)!} ~ dz \\ 
  &= \frac{ (-1)^{n-2}e^{-3} }{(n-2)!}\int_{e^{-3}}^1
  \frac{ \ln^{n-2} z }{z} ~ dz \\ 
  &= \frac{ (-1)^{n-2}e^{-3} }{(n-2)!} \left( - \frac{(-3)^{n-1}}{n-1} \right)
  \\ 
  &= \frac{3^{n-1}e^{-3}}{(n-1)!} \\ 
  &= \frac{3^{n-1}}{e^3(n-1)!}
\end{align*}

Por lo tanto, 

\begin{align*}
  \mathbb{E}\left[ N \right] 
  &= \sum_{k=1}^\infty \frac{k 3^{k-1}}{e^{3}(k-1)!}\\
  &=\frac{1}{e^3} \sum_{k=0}^\infty 3^{k} \frac{k+1}{k!} \\
  &=\frac{1}{e^3}\left(\sum_{k=0}^\infty k \frac{3^k}{k!} + \sum_{k=0}^\infty
  \frac{3^k}{k!} \right) \\ 
  &=\frac{1}{e^3}\left(\frac{1}{e^{-3}}\sum_{k=0}^\infty k \cdot e^{-3} \frac{3^k}{k!} + \sum_{k=0}^\infty
  \frac{3^k}{k!} \right)
\end{align*}

Ahora somos pillos y vemos que la primera sumatoria es el valor
esperado de una Poisson con parámetro $\lambda = 3$, y por lo tanto es 
$3$. la otra sumatoria es $e^3$, dado que $e^x = \sum_{k=0}^\infty
\frac{x^k}{k!}$. Como $1 / e^{-3} = e^3$,

\begin{equation*}
  \therefore ~ \mathbb{E}\left[ N \right] = \frac{1}{e^3}(3e^3 + e^3) =
  \frac{1}{e^3} \cdot 4e^3 = 4
\end{equation*}

Este valor coincide con la estimación de la siguiente simulación:

\begin{verbatim}
def sim_9(iterations):

  counts = []
  n, s = 0, 1

  for _ in range(iterations):
    while s >= exp(-3):
      n += 1
      s *= uniform(0, 1)
    counts.append(n)
    n, s = 0, 1
 
  return sum(counts)/len(counts)
  
\end{verbatim}




\pagebreak 

\begin{myframe}
\textbf{(9)} Un juego consiste en dos pasos. En el primer paso se tira un dado
convencional. Si sale 1 o 6 tira un nuevo dado y se le otorga al jugador como
puntaje el doble del resultado obtenido en esta nueva tirada; pero si sale 2, 3,
4 o 5 en la primer tirada, el jugador debería tirar dos nuevos dados, y
recibiría como puntaje la suma de los dados. Si el puntaje del jugador excede
los 6 puntos entonces gana.

$(a)$ Calcular la probabilidad de que un jugador gane.  

$(b)$ Estimar mediante simulación.
\end{myframe}

~ 


Sea $X = 1$ si el primer dado es 1 o 6, $X = 0$ si el primer dado es 2, 3, 4 o
5. Claramente $X$ es una Bernoulli con $p = 1 / 3$. Entonces, de acuerdo con la
ley de probabilidad total,

\begin{align*}
  P(\text{Ganar}) = P(2U > 6 \mid X = 1 )P(X = 0) + P(U_1 + U_2 > 6 \mid X = 0
  )P(X = 0 )
\end{align*}

con $U, U_2, U_3$ uniformes discretas en $\left\{ 1,\ldots,6 \right\} $.
Claramente, $U_1 + U_2 > 6$ y $2U$ son independientes de $X$. Por lo tanto 

\begin{equation*}
  P(\text{Ganar}) = P(U > 3) 1 / 3 + P(U_1 + U_2 > 6) 2 / 3
\end{equation*}

Ahora bien, $P(U_1 + U_2 > 6) = 1 - P(U_1 \leq 6 - U_2)$, y

\begin{align*}
  P(U_1 \leq 6 - U_2) 
  &= \sum_{k=1}^6 P(U_1 \leq 6 - k)P(U_2 = k) \\ 
  &=\sum_{k=1}^6 \frac{6-k}{6}\frac{1}{6} \\ 
  &=\frac{1}{6}\sum_{k=1}^6 1 - \frac{k}{6} \\ 
  &=0.416
\end{align*}

Por lo tanto, $P(U_1 + U_2 > 6) = 0.583$. Es fácil ver que $P(U > 3) =
\frac{1}{2}$. Luego 

\begin{equation*}
  P(\text{Ganar}) = \frac{1}{2} \cdot \frac{1}{3} + 0.583 \cdot \frac{2}{3} =
  0.555
\end{equation*}

~ 
$(b)$ En Python,

\begin{verbatim}
from random import randint
def sim_8(iterations):

  wins = 0

  for _ in range(iterations):
    first_dice = randint(1, 6)
    X = first_dice == 0 or first_dice == 6
  
    if X:
      result = 2*randint(1, 6)
    else:
      result = randint(1, 6) + randint(1, 6)

    wins = wins + 1 if result > 6 else wins

  return wins/iterations
\end{verbatim}

\pagebreak 

\section{P4}



\begin{myframe}
  \textbf{(1)} . Se baraja un conjunto de $n = 100$ cartas (numeradas
  consecutivamente del 1 al 100) y se extrae del mazo una carta por vez.
  Consideramos que ocurre un “éxito” si la $i$-ésima carta extraída es aquella
  cuyo número es $i$ $(i = 1,...,n)$. 

  a) Calcule la probabilidad de que $(i)$ las
  primeras $r$ cartas sean coincidencias y dé su valor para $r = 10$. $(ii)$ Haya
  exactamente $r$ coincidencias y estén en las primeras $r$ cartas. Dé su valor para
  $r = 10$. 

  b) Pruebe que $E(X) = Var(X) = 1$ donde $X$ es el número de coincidencias
  obtenidas en una baraja de $n$ cartas. 

  c) Escriba un programa de simulación para
  estimar la esperanza y la varianza del número total de éxitos, y de los
  eventos del inciso $(a)$ con $r = 10$, y compare los resultados obtenidos con 100,
  1000, 10000 y 100000 iteraciones. Use el archivo “Problemas de coincidencias”
  para guiarse.
\end{myframe}


$(a)$ $(i)$ Sea $Y : \Omega \mapsto \mathbb{N}_0$ tal que $Y(\omega) = k$ si y solo si en
$\omega \in \Omega$ las primeras $k$ extracciones son coincidencias. Entonces
$P(Y = 0) = 99 / 100$, y para $k \geq 1$:

\begin{equation*}
  P(Y = k) = \frac{1}{100} \cdot \frac{1}{99} \cdot \ldots \cdot \frac{1}{100
  - (k-1)} = \frac{(100 - k)!}{100!}
\end{equation*}

$(ii)$ Sea $Z : \Omega \mapsto \mathbb{N}_0$ tal que $Z(\omega) = k$ si y solo si,
en $\omega$, las primeras $k$ cartas extraídas son coincidencias y ninguna otra
carta extraída es coincidencia.  Sea $r := 100 - k$ la cantidad de cartas
restantes en el mazo después de las primeras $k$ extracciones.

Las $r$ cartas restantes no tendrán éxitos si y solo si la permutación de ellas
no contiene puntos fijos. La cantidad de permutacaiones sin puntos fijos en un
conjunto de $r$ elementos se denomina el \textit{subfactorial} de $r$ y se
escribe $!r$. Su valor es 

\begin{equation*}
  !r = r! \sum_{k=0}^r \frac{(-1)^k}{k!}
\end{equation*}

Luego, la probabilidad de que una permutación arbitraria de $r$ elementos
carezca de puntos fijos es 

\begin{equation*}
  \frac{!r}{r!} = \frac{r! \sum_{k=0}^r \frac{(-1)^k}{k!}}{r!} = \sum_{k=0}^r
  \frac{(-1)^k}{k!}
\end{equation*}

Notar, como simple observación, que cuando $r \to \infty$ tenemos
$$\sum_{k=0}^r \frac{(-1)^k}{k!} \to \frac{1}{e}$$

Es decir, la probabilidad de que una permutación arbitraria carezca de puntos
fijos se aproxima a $\frac{1}{e}$ a medida que la cantidad de elementos
permutados crece. Pero volviendo a nuestro problema, tenemos entonces:

\begin{equation*}
  P(Z = k) = P(Y = k) \cdot \sum_{i=0}^r \frac{(-1)^i}{i!} =
  \frac{(100-k)!}{100!} \sum_{i=0}^{100 - k} \frac{(-1)^i}{i!}
\end{equation*}

$(b)$ Sea $X$ el número de coincidencias obtenidas en $n$ cartas, o bien el
número de puntos fijos en una permutación aleatoria de $\left\{ 1,
\ldots, n \right\} $.

Como la permutación es aleatoria, la probabilidad de que el $i$-écimo elemento
tome el valor $k$ es $\frac{1}{n}$. Sea $E_k = 1$ si el $k$-écimo elemento es
una coincidencia, $0$ de otro modo. Entonces que $E_k \sim
\text{Bernoulli}( 1 / n)$. Es claro que $X = \sum_{i=1}^n E_i$. Luego 

\begin{equation*}
  \mathbb{E}\left[ X \right] = \sum_{i=1}^n \mathbb{E}\left[ E_i \right] =
  n \cdot \frac{1}{n} = 1
\end{equation*}

Además,

\begin{align*}
  \mathbb{E}\left[ X^2 \right]  
  &= \mathbb{E}\left[ \left( \sum_{i=1}^n E_i
  \right)^2  \right] \\ 
  &= \mathbb{E}\left[ \sum_{k=1}^n\sum_{\substack{i=1\\i \neq k}}^n E_kE_i  +  \sum_{i=1}^n
E^2_{i}  \right] &\left\{ \text{Ver } \star \text{ al final si hay dudas} \right\} 
  \\ 
  &= \sum_{k=1}^n\sum_{\substack{i=1\\i \neq k}}^n \mathbb{E}\left[ E_k E_i \right] +
  \sum_{i=1}^n \mathbb{E}\left[ E_i^2 \right] 
\end{align*}

La razón por la cual dejamos los cuadrados a un lado y los otros productos del
otro es que $E_{i} \cdot E_j$ siguen una distribución cuando $i \neq j$ y otra
cuando $i = j$. Si $i = j$ obviamente $P(E_i E_j = 1) = 1 / n$ y son Bernoulli
con $p = 1 / n$. Si $i \neq j$,

\begin{align*}
  &= P(E_j = 1 \cap  E_i = 1) \\ 
  &=P(E_j = 1 \mid E_i = 1)P(E_i = 1) \\ 
  &= \frac{1}{n(n-1)}
\end{align*}

\begin{align*}
  \therefore  ~ \mathbb{E}\left[ X^2 \right]  
  &= \sum_{k=1}^n\sum_{\substack{i=1\\i \neq k}}^n \mathbb{E}\left[ E_k E_i \right] + \sum_{i=1}^n
  \mathbb{E}\left[ E_i^2 \right] \\
  &=\frac{n(n -1)}{n(n-1)} + 1\\ 
  &= 2
\end{align*}

\begin{equation*}
  \therefore \mathbb{V}\left[ X \right] = \mathbb{E}\left[ X^2 \right] -
  \mathbb{E}^2\left[ X \right] = 2 - 1 = 1
\end{equation*}

\begin{helpframe}
\small
  ($\star$) La expansión de la sumatoria es sencilla: 

  \begin{align*}
    &(E_1 + \ldots + E_n)(E_1 + \ldots + E_n) \\
    =& (E_1^2 + E_1E_2 + \ldots + E_1E_n) \\ 
    +&(E_2E_1 + E_2^2 + \ldots + E_2 E_n)\\
     &\vdots \\ 
    +&(E_nE_1 + E_nE_2 + \ldots + E_n^2) \\ 
    =& (E_1E_2 + E_1E_3 + \ldots + E_1E_n) \\ 
    +&(E_2E_1 + E_2E^3 + \ldots + E_2 E_n) \\
     &\vdots \\ 
    +&(E_nE_1 + E_nE_2 + E_nE_{n-1}) \\ 
    +&(E_1^2 + E_2^2 + \ldots +E_n^2) \\ 
     &=\sum_{k=1}^n\sum_{i=1, i \neq k}^n E_kE_i + \sum_{i=1}^n E_i^2
  \end{align*}
\end{helpframe}
\normalsize

\pagebreak 

$(c)$ En Python, 

\small
\begin{verbatim}
from random import random
from statistics import mean, variance

def random_permutation(l):
  N = len(l)
  for j in range(N-1, 0, -1):
    i = int( (j+1)*random() )
    l[j], l[i] = l[i], l[j]

  return l


def count_fixed_points(permutation):
  count = 0
  for i in range(len(permutation)):
    if permutation[i] == i:
      count +=1

  return count

def sim1(n_iterations):

  L = [i for i in range(100)]
  results = []

  for _ in range(n_iterations):
    permutation = random_permutation(L)
    fixed_points = count_fixed_points(permutation)
    results.append(fixed_points)

  return mean(results), variance(results)

# Example
x, y = sim1(1000)
print(f"E(X) = {x}, V(X) = {y}")
\end{verbatim}

\normalsize
For $n = 1000$ this printed $E(X) = 0.988, V(X) = 1.0288848848848848$.

\pagebreak

\begin{myframe}
  \textbf{(2)} Se desea construir una aproximación de 

  \begin{equation*}
    \sum_{k=1}^N \exp\left( \frac{k}{N} \right), \qquad N = 10 ~ 000
  \end{equation*}

$(a)$ Escriba un algoritmo para estimar la cantidad deseada.

$(b)$ Obtenga la aproximación sorteando 100 números aleatorios.

$(c)$ Escriba un algoritmo para calcular la suma de los primeros 100 términos, y
compare el valor exacto con las dos aproximaciones, y el tiempo de cálculo
\end{myframe}

$(a)$ Sea $f(x) = \exp(x / N)$ y $\theta$ el valor que deseamos
calcular. Dado que 

\begin{equation*}
  \theta = N \cdot \left( \frac{1}{N}\sum_{k=1}^N f(k, N) \right) \approx N
  \cdot \mathbb{E}\left[ f(U, N) \right] 
\end{equation*}

con $U \sim \mathcal{U}\left\{ 1,\ldots, N \right\}  $, podemos generar $n$ variables
uniformes $U_1, \ldots, U_n$ y calcular el promedio de $f(U_1), \ldots, f(U_n)$
para dar con una estimación.

El algoritmo entonces queda 


\small
\begin{quote}

\begin{verbatim}
def sim2(n):

  N = 10000
  def f(x):
    return exp(x/N)

  return N * mean([ f( randint(1, N) ) for _ in range(n)])

# Para comparar
def sim2_true():
  S = 0
  N = 10000
  for k in range(N):
    S += exp(k/N)
  return S
\end{verbatim}

\end{quote}
\normalsize

\pagebreak

\begin{myframe}
  \textbf{(3)} Se lanzan simultáneamente un par de dados legales y se anota el
  resultado de la suma de ambos. El proceso se repite hasta que todos los
  resultados posibles: 2,3,...,12 hayan aparecido al menos una vez. Estudiar
  mediante una simulación la variable $N$, el número de lanzamientos necesarios
  para cumplir el proceso. Cada lanzamiento implica arrojar el par de dados. 

  $(a)$ Describa la estructura lógica del algoritmo que permite simular en computadora
  el número de lanzamientos necesarios para cumplir el proceso. 

  $(b)$ Mediante una
  implementación en computadora, $(i)$ estime el valor medio y la desviación
  estándar del número de lanzamientos, repitiendo el algoritmo: 100, 1000, 10000
  y 100000 veces. $(ii)$ Estime la probabilidad de que N sea por lo menos 15 y la
  probabilidad de que N sea a lo sumo 9, repitiendo el algoritmo: 100, 1000,
  10000 y 100000.
\end{myframe}

Estudiar sólo mediante simulaciones la variable $N$ es aburrido porque
programarlas es fácil. Estudiemos el problema matemáticamente antes de simular. 

Sea $S = X_1 + X_2$ con $X_1, X_2$ independientes y uniformes discretas en
$\left\{ 1,\ldots, 6 \right\} $. La distribución de $S$ (ver 
$\star$ al final de este ejercicio) es

\begin{equation*}
  p_S(x) = \begin{cases}
    \frac{x-1}{36} &2 \leq x \leq 6 \\ 
    \frac{13-x}{36} &7 \leq x \leq 12 \\ 
    0 &c.c.
  \end{cases}
\end{equation*}

Sea $G(n) = \left\{ S_i\right\}_{i=1}^n $ una secuencia aleatoria
de $n$ variables, donde cada $S_i = X_1 + X_2$ es independiente de los demás.

~

Como la imagen de $S$ tiene $11$ elementos (números del 2 al 12), nos interesa
estudiar la probabilidad de que, para $n \geq 11$, se de el evento 

\begin{equation*}
  \mathcal{A}(n) := \forall k : 2 \leq k \leq 12 : k \in G(n)
\end{equation*}

Si $n = 11$, hay una única combinación de números válida (la que contiene todos distintos) que puede darse de $11!$ maneras diferentes (no es lo mismo $\left\{
2, 3, \ldots, 12\right\} $ que $\left\{ 12, 11, \ldots, 2 \right\} $, por
ejemplo). En particular, si $k_1, \ldots, k_{11}$ es el orden en que se dan los
números de una secuencia arbitraria que satisface $\mathcal{A}(n)$, la
probabilidad de la secuencia es 

\begin{equation*}
  P(S_1 = k_1) \cdot \ldots \cdot P(S_{11} = k_{11})
\end{equation*}

Pero $P(S_i = k_i) = P(S = k_i)$ (es decir, la posición en que el número aparece
no afecta la probabilidad). Por ende, el producto arriba es 


\begin{equation*}
  P(S = k_1) \cdot \ldots \cdot P(S = k_{11}) = \prod_{i=2}^{12}P(S = i)
\end{equation*}

Como debemos tener en cuenta que hay $11!$ secuencias $k_1, \ldots, k_{11}$ que
satisfacen la condición, 

\begin{equation*}
  \therefore ~ P(\mathcal{A}(11)) = 11! \prod_{i=2}^{12}P(S = i)
\end{equation*}

Para $n > 11$, el problema es inabordable a mano. Nos damos por satisfechos con el
estudio del experimento que hicimoso hasta acá, porque consideramos que da una
intuición correcta sobre el problema en cuestión.

\pagebreak 

\begin{helpframe}
$(\star)$ Es fácil ver que, si $2 \leq x \leq 6$,

\begin{equation*}
  p_S(x) = \sum_{k=1}^{x-1} p_{X_1}(k)p_{X_1}(x - k) = \frac{x-1}{36}
\end{equation*}

Si $7 \leq x \leq 12$, podemos escribir $x$ como $6 + k$ para $1 \leq k \leq 6$,
y

\begin{equation*}
  P(S = x) = P(S = 6 + k) = \sum_{j=k}^{6} p_{X_1}(j)p_{X_1}(x - j) = \frac{6 -
  k + 1}{36} = \frac{13 - x}{36}
\end{equation*}

donde en el último paso usamos que $k = x - 6$. 

\begin{equation*}
  \therefore ~ p_S(x) = \begin{cases}
    \frac{x-1}{36} &2 \leq x \leq 6 \\ 
    \frac{13-x}{36} &7 \leq x \leq 12 \\ 
    0 c.c.
  \end{cases}
\end{equation*}
\end{helpframe}



\pagebreak 

\begin{myframe}
\textbf{(4)} Implemente cuatro métodos para generar una variable $X$ que toma los
valores del 1 al 10, con probabilidades

\begin{equation*}
  p_1 = 0.11 \qquad p_2 = 0.14 \qquad p_3 = 0.09 \qquad p_4 = 0.08
\end{equation*}
\begin{equation*}
  p_5 = 0.12 \qquad p_6 = 0.10 \qquad p_7 = 0.09 \qquad p_8 = 0.07
\end{equation*}
\begin{equation*}
  p_9 = 0.11 \qquad p_{10} = 0.09 
\end{equation*}

usando

$(a)$ Método de rechazo con una uniforme discreta, buscando la cota c más baja
posible. 

$(b)$ Método de rechazo con una uniforme discreta, usando c = 3. 

$(c)$ Transformada inversa. 

$(d)$ Método de la urna: utilizar un arreglo $A$ de tamaño 100 donde cada valor
$i$ está en exactamente $p_i\cdot 100$ posiciones. El método debe devolver A[k]
con probabilidad 0,01. ¿Por qué funciona?

Compare la eficiencia de los tres algoritmos realizando 10000 simulaciones.

\end{myframe}

$(a)$ Sea $Y \sim \mathcal{U}\left\{ 1,\ldots, 10 \right\} $ uniforme discreta. Para encontrar la $c$ mínima, resolvemos 

\begin{equation*}
  \frac{0.12}{1.10} \leq c \iff 1.2 \leq c
\end{equation*}

con lo cual fijamos $c = 1.2$. Un algoritmo para generar $n$ valores de $X$:

\begin{verbatim}
def sim4_rejection_method(n_generations):

  p = [0.11, 0.14, 0.09, 0.08, 0.12, 0.10, 0.09, 0.07, 0.11, 0.09]

  generations = []

  for _ in range(n_generations):
    while True:
      u = random()
      y = randint(1, 10)
      if u < p[y - 1]/( 1.2 * 0.1 ):
        generations.append(y)
        break
  
  return generations
\end{verbatim}

$(b)$ No es muy distinto de $(a)$, lo salteamos. 

~

$(c)$ Ordenemos las probabilidades de mayor a menor: 

\begin{equation*}
  p_2 = 0.14 \qquad p_5 = 0.12 \qquad p_1 = p_9 = 0.11 \qquad p_6 = 0.10 
\end{equation*}
\begin{equation*}
  p_3 = p_7 = p_{10} = 0.09 \qquad p_4 = 0.08 \qquad p_8 = 0.07
\end{equation*}

Sean $\left\{ x_i \right\}_{i \in \mathbb{N}} = Im(X) $. Recordemos que en el
método de la transformada inversa, generamos una uniforme $U \sim \mathcal{U}(0,
1)$. Si $U \in [F(x_i), F(x_{i+1}))$ (es decir, si $U$ supera la probabilidad
acumulada de $x_i$ pero no la de $x_{i+1}$), producimos la variable $x_{i+1}$.

~ 

Es fácil ver que $F_X$ es dada por


\begin{align*}
  \begin{bmatrix} 
     1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 &9 & 10\\ 
     0.11 & 0.25 & 0.34 & 0.42 & 0.54 & 0.64 & 0.73 & 0.8 & 0.91 & 1
  \end{bmatrix} 
\end{align*}

donde la primera fila es $Im(X)$ y la segunda fila la probabilidad acumulada de
cada valor. Usando el ordenamiento de las variables que dimos al principio,
obtenemos:

\begin{verbatim}
def sim4_inverse_transform(n_generations):

  p = [0.11, 0.14, 0.09, 0.08, 0.12, 0.10, 0.09, 0.07, 0.11, 0.09]
  ordering = [1, 4, 0, 5, 2, 6, 9, 4, 7]                            # := o
  ordered_probabilities = [p[i] for i in ordering]                  # := op
  u = random()
  for i in range(len(p)):
    if u < sum(ordered_probabilities[0:i+1]):
      return ordering[i] + 1

\end{verbatim}

Este algoritmo guardan en $\overrightarrow{p}$ las probabilidades, en
$\overrightarrow{o}$ el orden decreciente de mayor a menor, y en
$\overrightarrow{op}$ las probabilidades ordenadas de mayor a menor. En la
iteración $i$, revisa si $U \sim \mathcal{U}(0, 1)$ es menor a 

\begin{equation*}
  \overrightarrow{op}[0] + \ldots + \overrightarrow{op}[i-1] = \sum_{0=i}^{i-1}
  \overrightarrow{op}[i] 
\end{equation*}

y si lo es, devuelve $\overrightarrow{o}[i] + 1$, donde sumar uno se hace para
pasar de $\left\{ 0,\ldots,9 \right\} $ (útil en Python) a $\left\{ 1, \ldots,
10 \right\} $.

~

$(d)$ El método funciona porque elige de manera uniforme un índice $i \in
\left\{ 0, \ldots, 99 \right\} $ y devuelve $A[i]$. Pero 

\begin{equation*}
  P(A[i] = k) = \frac{\text{\# veces que aparece } k \text{ en }
  A}{\#\text{elementos de } A} = \frac{p_k \cdot 100}{100} = p_k
\end{equation*}

\begin{verbatim}
def sim4_urna():

  p = [0.11, 0.14, 0.09, 0.08, 0.12, 0.10, 0.09, 0.07, 0.11, 0.09]
  times_it_occurs = [int(x * 100) for x in p]
  A = [ [i+1] * times_it_occurs[i] for i in range(len(p))] # list of lists 
  A = sum(A, []) # Python trick, simple but inefficient: joins the list of lists into a single list
  return choice(A) # import choice from random
  
\end{verbatim}

\pagebreak

\begin{myframe}
  \textbf{(5)} Genere una variable binomial $\mathcal{B}(n, p)$ con $(a)$ método de la
  transformada inversa y $(b)$ simulación de $n$ Bernoullis con probabilidad de
  éxito $p$.
\end{myframe}

Puesto que 

$$F_B(k) = \sum_{i=0}^k \binom{n}{k} p^i (1-p)^{n-i}$$

observamos que $F_B(0) = (1-p)^n$ y 

\begin{align*}
  F_B(1) = np(1-p)^{n-1} = np\frac{F_B(0)}{(1-p)} 
\end{align*}


\begin{align*}
  F_B(2) &= \binom{n}{2} p^{2}(1-p)^{n-2} \\
         &= \frac{(n)(n-1)}{2} p^2(1-p)^{n-2} \\ 
         &= \frac{n-1}{2} \frac{p}{(1-p)} F_B(1)
\end{align*}

\begin{align*}
  F_B(3) &= \frac{n(n-1)(n-2)}{3!}p^3 (1-p)^{n - 3}  \\ 
         &= \frac{n-2}{3} \frac{p}{1-p} F_B(2)
\end{align*}

















\pagebreak
\section{Apéndice teórico}

\begin{myframe}
  \textbf{Método de la transformada inversa.} Si $X : \Omega \mapsto \left\{
  x_1, \ldots, x_n, \ldots \right\} $ es variable aleatoria
  discreta, con $x_i < x_{i+1}$ y $p_X(x_i) = p_i$, la FPA $F_X$ satisface
  $F_X(x) = \sum_{i=1}^{i-1} p_i$ para todo $x \in [x_{i-1}, x_i)$.

  Informalmente, entre dos valores contiguos $[x_{i-1}, x_i)$ de $X$, se ha acumulado
  la probabilidad de $x_{i-1}, x_{i-2}, \ldots, x_1$.

  $\therefore $ Existe una biyección que asocia cada $x_i$ con el intervalo 
  $$I_i = [p_0 + \ldots + p_{i-1}, p_0 + \ldots + p_i)$$

  Claramente, $\left\{ I_1, I_2, \ldots \right\} $ es una partición de $[0, 1)$ 
  y la longitud de $I_i$ es $p_i$. 

  $\therefore ~ P(U \in I_i) = p_j$ con $U \sim \mathcal{U}(0, 1)$.

  $\therefore $ Se pueden generar $k$ valores de $X$ generando $k$ veces una
  uniforme $U$ en $(0, 1)$ y produciendo $x_0$ si $U \in I_0$, $x_1$ si $U \in
  I_1$, etc.
\end{myframe}

\pagebreak 


\begin{myframe}
\textbf{Método de aceptación y rechazo.} Sean $X, Y$ variables aleatorias y asuma que $Im(Y)
\subseteq Im(X)$. Asuma además que existe $c > 0$ tal que

\begin{equation*}
  \frac{ f_X(x_j) }{f_Y(x_j)} \leq c
\end{equation*}

Es decir, que la probabilidad de $x_j$ en $X$ es a lo sumo $c$ veces la
probabilidad de $x_j$ en $Y$. Entonces 

\begin{equation*}
  \sum_{x \in Im(X)} f_X(x) \leq c \sum_{x \in Im(X)} f_Y(x) \leq c
\end{equation*}

Como la primera sumatoria es $1$ se sigue que $c \geq 1$. Si $c = 1$ claramente
$X, Y$ siguen la misma distribución, así que asumamos que $c > 1$ (es decir que 
$1 / c < 1$).

Entonces, 
tenemos un algoritmo para generar valores de $X$ usando valores de $Y$:

\begin{enumerate}
  \item Generar $y \in Im(Y)$.
  \item Generar $U \sim \mathcal{U}(0, 1)$
  \item Si $U < \frac{f_X(y)}{c \cdot f_Y(y)}$, devolver $y$ y terminar.
  \item Volver a (1)
\end{enumerate}

Notemos que 

\begin{equation*}
  \frac{f_X(y)}{f_Y(y)} \leq c \Rightarrow  \frac{f_X(y)}{c \cdot f_Y(y)} \leq 1
\end{equation*}

Es decir, la utilidad de asumir una $c$ que satisfaga la primera desigualdad es
que nos permite "comprimir" o normalizar el \textit{ratio} entre las
probabilidades de arriba entre 0 y 1. 
\end{myframe}

~
\pagebreak 






\pagebreak 

\section{Bonus problems}

\begin{myframe}
\textbf{(1)} For a fixed $p$, independently label the nodes of an infinite
complete binary tree $0$ with probability $p$, and 1 otherwise. For what $p$ is
there exactly a $1/2$ probability that there exists an infinite path down the tree
that sums to at most 1 (that is, all nodes visited, with the possible exception
of one, will be labeled 0). Find this value of $p$ accurate to $10$ decimal places.`
\end{myframe}

Let $\mathcal{P}$ be the set of all paths in the tree and let $\ell(v)$ be the
label of vertex $v$. We know $\ell(v) \sim \text{Bernoulli}(p)$. We are
interested in the probability that a path
$p \in \mathcal{P}$, with vertices $v_1, v_2, \ldots$, satisfies 

\begin{equation*}
  \sum_{i=1}^\infty \ell(v_i) \leq 1
\end{equation*}

Let $p = v_0, \ldots, v_{n-1}$ be a path from level zero to level $n-1$, with
$v_0$ the root node. Clearly, $\sum_{i=0}^{n-1} \ell(v_i)$ is a sum of Bernoulli
independent random variables. $\therefore ~ \sum_{i=0}^{n-1} \ell(v_i) \sim
\mathcal{B}(n, p)$.

\begin{equation*}
  \therefore ~ P\left( \sum_{i = 0}^{n-1} \ell(v_i) \leq 1 \right) =
  \sum_{k=0}^1 \binom{n}{k}p^k(1-p)^{n-k} = (1-p)^{n} + np(1-p)^{n-1}
\end{equation*}

Let $\mathcal{P}_n$ be the set of all paths with $n$ vertices, i.e. the set of
paths from the root vertex to a vertex in level $n - 1$.

~ 

The event of a path containing at least two vertices with label $1$ is
equivalent to the event of two vertices in different levels being $1$ and these
vertices being connected. If we consider only vertices up to level $n-1$, then
per each level we have $1, 2, 4, 16, \ldots, 2^{n-1}$ vertices. 

Thus, the vertices in the $k$th level may be considered a binomial experiment
with parameters $n = 2^{k}, p$. Furthermore, each level itself may be considered
a binomial experiment, where success is "at least one label 1 in the vertices of
the level exists".

\begin{align*}
  &P(\text{At least one label in the vertices of level $k$ exists}) 
  \\=~& 1 - P(\text{Zero vertices with label one exist in level $k$}) \\ 
  \\=~& 1 - P(\text{All vertices in label $k$ have label 0}) \\ 
  = ~ &1- p^{2^k}
\end{align*}

So, the binomial experiment of the levels $L$ follows $L \sim \mathcal{B}(n, 1 - p^{2^k})$,
and if we let $\widetilde{ p } = p^{2^k} $ we have

\begin{align*}
  P(L \geq 2) 
  &= 1 - P(L \leq 1) = 1 - \left( (1-\widetilde{ p } )^n +
  n\widetilde{ p }(1 - \widetilde{ p } )^{n-1}  \right) 
\end{align*}

Now, the question is:

\begin{equation*}
  P(\text{Two one-labelled vertices are connected} \mid L\geq 2)
\end{equation*}

The probability that any pair of vertices of different levels is connected




\end{document}



