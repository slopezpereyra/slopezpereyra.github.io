---
title: A neural network with Julia 
subtitle: Notes on a group-theory based approach
---

### General structures 

Let $\mathcal{L}, \mathcal{N}$ denote the general structs for layer and network objects. 

```jl 
mutable struct ğ“› 
    neurons::Vector 
    W::Matrix
    biases::Vector
    f::Function
    function ğ“›(neurons::Vector, W::Matrix, biases::Vector)
        new(neurons, W, biases)
    end
end

mutable struct ğ“
    net::Dict{Int, ğ“›}
    dims::Vector
    nlayers::Int
    nparams::Int

    function ğ“(dims::Vector)
        structure = Dict()
        structure[1] = ğ“›(zeros(Float32, dims[1]), Array{Float32}(undef, 0, 0), [])
        for i in 2:length(dims)
            neurons = zeros(Float32, dims[i])
            weights = rand(Float32, dims[i], dims[i-1])
            biases = rand(Float32, dims[i])
            structure[i] = ğ“›(neurons, weights, biases)
         end
        n::Int32 = 0
        for i in 2:length(dims)
            n += dims[i - 1] * dims[i] + dims[i]
        end

        new(structure, dims, length(dims), n)
    end 
end
```

It is useful to conceive $ğ‘³, ğ‘µ$ the sets of possible layers and networks using 
group theory. The usefulness of such conception will become clear later. For the
moment let us take such formalization to practice via the following definitions.

```jl 
function âŠ•(Lâ‚::ğ“›, Lâ‚‚::ğ“›)
    """Addition operator over the set ğ‘³ of layer objects.
    Observe that (ğ‘³, âŠ•) is a non-abbelian group. In particular,
    the operation

                    â„’â‚ âŠ• â„’â‚‚ = â„’â‚ƒ,          â„’áµ¢âˆˆ ğ‘³

    is neuron-preserving with respect to â„’â‚. This is, 
    â„’â‚ƒ has the same activations as â„’â‚"""

    Lâ‚ƒ = ğ“›(Lâ‚.neurons, Lâ‚.W + Lâ‚‚.W, Lâ‚.biases + Lâ‚‚.biases)
    return Lâ‚ƒ
end

function âŠ—(Î»::Number, L::ğ“›)
    """Scalar-layer multiplication.
    âŠ— is the group action

                âŠ— : â„ Ã— ğ‘³ â†’ ğ‘³ 
    """

    W = Î» * L.W
    b = Î» * L.biases
    return ğ“›(L.neurons, W, b)
end

function âŠ—(Î»::Number, N::ğ“)
    """Scalar-network multiplication âŠ—.
    Might be thought of as a group action 

                âŠ— : â„ Ã— ğ‘µ â†’ ğ‘µ

    This operation is replacing. """

    new = ğ“(N.dims)
    keys = [1:N.nlayers;]
    layers = [âŠ—(Î», N.net[i]) for i in 1:N.nlayers]
    new.net = Dict(keys .=> layers)
    return new
end

function âŠ•(Nâ‚::ğ“, Nâ‚‚::ğ“)
    """Addition operator over the set ğ‘µ of layer objects.
    Observe that (ğ‘µ, âŠ•) is a non-abbelian group. In particular, 

                    ğ§â‚ âŠ• ğ§â‚‚ = ğ§â‚ƒ           ğ§áµ¢âˆˆ ğ‘µ

    is neuron-preserving with respect to ğ§â‚."""

    if Nâ‚.dims != Nâ‚‚.dims
        throw(DimensionMismatch)
    end
    Nâ‚ƒ = ğ“(Nâ‚.dims)
    Nâ‚ƒ.net = mergewith(âŠ•, Nâ‚.net, Nâ‚‚.net)
    return Nâ‚ƒ
end
```

One can now very easily materialize the training of a network via
the following conception. For a given network $\mathcal{N}$, one defines the
*gradient* of each weight $w_{ij}^{(l)}$ in the network as the weight $\nabla
w_{ij}^{(l)}$ of a similar network $\nabla \mathcal{N}$.

> *Note*. I say two networks are *similar* iff their dimensions are the same.

Then 

$$
\begin{align}
    \mathcal{N}(e + 1) = \mathcal{N}(e) - \eta \nabla \mathcal{N}(e)
\end{align}
$$

defines the updated network in epoch $e + 1$. The parameters of $\mathcal{N}(e
+1)$ are shifted towards the direction of steepest decent ---rovided that $(+),
(\cdot)$ are well-defined as network-network and scalar-network operators (which
is precisely what we've done above). 

This formalization of non-abbelian groups for the network and layer objects, by
virtue of which we conceptualized training, is not at all necessary. One can
indeed update a network's parameters without the need to formalize, for example,
network-network operator ---and in fact this is what is generally done. However,
I find this formal approach to have sharper outlines.

### Forward and backward propagation 

Firstly let us defined a simple helper function, 

```jl 
function compute_layer(W::Matrix, a::Vector, b::Vector)
    Y = zeros(Float32, length(b))
    mul!(Y, W, a)
    Y += b
    return Y
end
```

This function performs $\textbf{W}\textbf{a} + \textbf{b}$, the linear
combination (plus bias) that is to be evaluated in some activation function.


```jl 
function fprop(x::Vector, N::ğ“)
    N.net[1].neurons = x
    L = N.nlayers
    for i in 1:(L-1)
        z = compute_layer(N.net[i+1].W, N.net[i].neurons, N.net[i+1].biases)
        N.net[i+1].neurons = N.net[i+1].f(z)
    end
    return(N)
end

function backprop(N::ğ“, yâƒ—::Vector)
    âˆ‡ğ“ = ğ“(N.dims)
    âˆ‚Câˆ‚aá´¸ = 2 * (N.net[N.nlayers].neurons - yâƒ—)
    for l in reverse(2:N.nlayers)
        âˆ‚Ïƒâˆ‚z = dÏƒdx(logit.(N.net[l].neurons))
        P = hadamard(âˆ‚Câˆ‚aá´¸, âˆ‚Ïƒâˆ‚z)
        âˆ‡W = kron(P, transpose(N.net[l-1].neurons))
        âˆ‡ğ“.net[l].W = âˆ‡W
        âˆ‡ğ“.net[l].biases = P
        # Change the dimension of the vector appropriately
        âˆ‚Câˆ‚aá´¸ = zeros(Float32, size(N.net[l].W, 2))
        mul!(âˆ‚Câˆ‚aá´¸, transpose(N.net[l].W), P)
    end
    return âˆ‡ğ“
end
```

The logic justifying these algorithms should be clear to anyone familiar with the
basics of neural network theory. The only difference with a traditional
backpropagation algorithm is that we are updating paramaters via network-network
operations.





### Appendix of math functions 

```jl 
relu(x::Number) = max(0, x)
function relu(X::Vector)
    relu.(X)
end

function softmax(X::Vector)
    X = X .- maximum(X)
    exp.(X) ./ sum(exp.(X))
end

Ïƒ(x::Number) = 1 / (1 + exp(-x))
function Ïƒ(X::Vector)
    Ïƒ.(X)
end

function dÏƒdx(x::Number)
    Ïƒ(x)*(1 - Ïƒ(x))
end

function logit(x)
    if x < 0 || x > 1
        error("Logit input out of bounds")
    end
    log(x / (1 - x))
end


function dÏƒdx(X::Vector)
    broadcast(dÏƒdx, X)
end

function dreludx(x::Number)
    ifelse(x > 0, 1, 0)
end

square(x) = x^2
function cost(final_layer, target)::Float32
    target_vector = zeros(Float32, length(final_layer))
    target_vector[target+1] = 1
    sum(broadcast(square, final_layer .- target_vector))
end

# Hadamard product
function hadamard(A, B)
    C = broadcast(*, A, B)
    return C
end
```
