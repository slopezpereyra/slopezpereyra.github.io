\documentclass[a4paper, 12pt]{article}
\usepackage{graphicx}
% Añade esto en el preámbulo para manejar mejor las rutas de archivos
\usepackage{grffile}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{newtxtext} \usepackage{newtxmath}
\usepackage{amsmath, amssymb}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example} \newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{parskip}
% --- Paquetes básicos ---
\usepackage[utf8]{inputenc}
\usepackage[spanish, es-tabla]{babel}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{graphicx}
\graphicspath{ {../Images/} }

% --- Configuración de párrafo ---
\setlength{\parskip}{1em}
\setlength{\parindent}{0pt}

\begin{document}


\section{Teoría de la información}

La entropía es una medida de cuán «sorprendente» o impredecible es una  variable
aleatoria. Un patrón regular y predecible es poco interesante, mientras un
patrón complejo y de alta impredicibilidad contiene más información. Daremos una
medida de entropía para las respuestas neuronales.

Por el momento, vamos a caracterizar las respuestas neuronales temporalmente y
en función de su \textit{spike count rate} (SPR), o sea el número de disparos por
unidad de tiempo. Definimos $P(r)$ como la probabilidad de observar un SPR de
$r$. La entropía de Shannon, que es la medida más común de entropía, expresa
cuán «sorprendente» es un valor de $r$ en términos de su probabilidad, y
necesita por lo tanto una función compuesta $h\left( P(r) \right) $. En
particular, la entropía de Shannon cuantifica la entropía en términos de la
esperanza de $h\left( P(r) \right) $, donde $h$ se elige de manera tal que
satisfaga dos propiedades deseables.

En primer lugar, $h$ debería ser una función decreciente de $P(r)$, de modo que
los eventos más probables contribuyan menos a la entropía total que los eventos
menos probables. Esto es razonable: si tiramos un dado de seis caras 60 veces y,
en general, obtenemos más o menos 10 veces cada cara, no podemos decir demasiado
acerca del dado. Pero si obtenemos 30 veces el número 6 y sólo 6 veces cada uno
de los otros números, podemos concluir que el dado está sesgado. A esto nos
referimos cuando decimos que los eventos menos probables son más informativos.

Otra propiedad deseable es que la entropía de dos variables aleatorias
independientes debería ser la suma de las entropías individuales. En el caso de
respuestas neuronales, si deseamos establecer la entropía de las respuestas de
dos neuronas independientes, la entropía total debería ser la suma de las
entropías individuales. 

El logaritmo es la única función que satisface estas dos propiedades para toda
probabilidad $P(r)$. Por lo tanto, definimos la entropía de Shannon $H$ como la
esperanza del logaritmo de la probabilidad de las respuestas neuronales:

\begin{equation}
    h\left( P(r) \right) = -\log_2 \left( P(r) \right)
\end{equation}

Usamos el logaritmo base dos por convención, y el signo negativo se añade para
satisfacer la primera propiedad, ya que el logaritmo es una función creciente.
Habiendo determinado la función que mide la «sorpresa» de una respuesta
neuronal, podemos definir la entropía de Shannon $H$ como la esperanza de esta
función:

\begin{equation}
    H := \mathbb{E}\left[ h(r) \right]  = - \sum_r P(r) \log_2 \left( P(r) \right)
\end{equation}

\begin{center}
\begin{tikzpicture}
    \begin{axis}[
        width=10cm, height=7cm,
        xlabel={Probability $P(r)$},
        ylabel={Value},
        xmin=0, xmax=1,
        ymin=0, ymax=4.5,
        grid=major,
        legend pos=north east,
        title={Surprisal $h(P)$ vs. Entropy Term $P \cdot h(P)$},
        axis lines=left,
        samples=200
    ]

    % 1. Surprisal in Bits (Base 2)
    % h(p) = -log2(p) = -ln(p)/ln(2)
    \addplot[
        color=blue,
        thick,
        domain=0.05:1
    ]
    {-ln(x)/ln(2)};
    \addlegendentry{$h(P) = -\log_2(P)$ (Surprisal)}

    % 2. Surprisal in Nats (Natural Log)
    % Comparison of "forms" (bases)
    \addplot[
        color=teal,
        dashed,
        thick,
        domain=0.05:1
    ]
    {-ln(x)};
    \addlegendentry{$h(P) = -\ln(P)$ (Nats)}

    % 3. Entropy Density (The term being summed)
    % f(p) = -p * log2(p)
    \addplot[
        color=red,
        thick,
        dashdotted,
        domain=0.001:1
    ]
    {-x * (ln(x)/ln(2))};
    \addlegendentry{$f(P) = -P \log_2(P)$}

    % Optional: Mark P=0.5 for max entropy contribution
    \draw[dotted, gray] (axis cs:0.368,0) -- (axis cs:0.368,0.53); % 1/e for nats max
    
    \end{axis}
\end{tikzpicture}
\end{center}

La entropía de Shannon en última instancia es una medida que depende de la
variabilidad, pero no nos dice nada acerca de la fuente de esta variabilidad.
Para cuantificar la fuente de la variabilidad usamos la información mutua (MI,
por sus siglas en inglés). La idea que la inspira es simple: digamos que un
mismo estímulo repetido induce cierta variabilidad $V_1$ en el SPR, y que una
diversidad de estímulos distintos induce otra variabilidad $V_2$. Si el SPR es
informativo respecto a la naturaleza del estímulo, esperaríamos que $V_2$ sea
mucho mayor que $V_1$.

Más formalmente, la MI es la diferencia entre la entropía total, por un lado, y
la entropía promedio de respuestas a un mismo estímulo, por otro. Claramente, si
MI es cero es porque las respuestas varían tanto con los mismos estímulos como
ante el total de estímulos, i.e. no tienen un comportamiento específico e
informativo respecto a la naturaleza del estímulo. Definimos entonces:

\begin{equation}
    H_s = - \sum_{r} P \left( r \mid s \right) \log_2 P\left( r \mid s \right) 
\end{equation}

la entropía ante un estímulo particular $s$. El promedio de esta entropía sobre
el espacio de estímulos nos da la así llamada \textit{noise entropy}:

\begin{equation}
    H_{\text{noise}} = \sum_{s} P \left[ s \right] H_s = - \sum_{s, r} P\left( s
    \right) P \left( r \mid s \right)  \log_2 P \left( r \mid s \right) 
\end{equation}

Esta es la entropía asociada a la variabilidad en la respuesta que \textit{no}
depende de cambios en el estímulo, sino que proviene de otras fuentes. Esto
debería ser claro: cada término $H_s$ es la entropía «dentro» de un estímulo
fijo, i.e. la que no depende del estímulo, y $H_{\text{noise}}$ es la esperanza
de este valor. Si la información mutua es la diferencia entre la entropía total
y la noise entropy, tenemos: 

\begin{align*}
    I_m &:= H - H_{\text{noise}}\\ 
        &= - \sum_r P(r) \log_2 P(r) + \sum_{s, r} P(s) P\left( r \mid s \right)
        \log_2 P\left( r \mid s \right) 
\end{align*}

Usando la ley de probabilidad total, según la cual

\begin{equation*}
    P\left( r \right)  = \sum_s P\left( s \right) P\left( r \mid s \right) 
\end{equation*}

obtenemos 

\begin{equation}
    I_m = \sum_{s, r} P \left( s \right) P\left( r \mid s \right) \log_2 \left(
    \frac{P \left( r \mid s \right) }{P\left( r \right) }\right) 
\end{equation}

La probabilidad conjunta $P\left( r, s \right) $ satisface 

\begin{equation*}
    P\left( r, s \right) = P(s) P\left( r \mid s \right)  = P(r) P \left( s \mid
    r\right) 
\end{equation*}

Por lo tanto, 

\begin{equation}
    I_m = \sum_{s, r} P\left( r, s \right) \log_2 \left( \frac{P(r, s)}{P(r)P(s)} \right) 
\end{equation}

Esta forma de escribir la información mutua revela que es simétrica respecto a
$s$ y $r$: la información mutua que un conjunto de respuestas da respecto de un
conjunto de estímulos es la misma que da el conjunto de estímulos respecto a las
respuestas.

Veamos algunos casos básicos. Digamos que las respuestas de una neurona no son
afectadas por la naturaleza del estímulo, i.e. $P(r \mid s) = P(r)$. Entonces se
tiene inmediatamente que $I_m = 0$. En el otro extremo, si cada estímulo $s$
produce una única respuesta $r_s$ distinta a todas las demás, $P(r_s) = P(s)$ 
y $P(r \mid s) = 1 \iff r = r_s$, y $P(r \mid s) = 0$ en el caso contrario. Por
lo tanto, 

\begin{equation*}
    I_m = \sum_{s} P(s) \log_2 \left( \frac{1}{P(r_s)} \right) = -\sum_{s} P(s)
    \log_2 P(s) = H_s
\end{equation*}

Es decir, la información mutua es exactamente la entropía del estímulo. 

\pagebreak 

\section{Redes con tasas de disparo}

\subsection{Recordando}

Recordemos que 

\begin{equation*}
    \rho(t) = \sum_{i=1}^n \delta(t - t_i)
\end{equation*}

es la \textit{neural response function} (NRF). Si $t_1, \ldots, t_n$ son tiempos
de disparo de una neurona, $\rho(t)$ es 1 si $t = t_i$ para algún $i =1,\ldots,
n$, cero de otro modo. El punto es que $t_1, \ldots, t_n$ son valores discretos
pero $\rho(t)$ es una función continua. Usando que 

\begin{equation*}
    \sum_{i=1}^n h(t - t_i) = \int_0^T h(\tau)\rho(t - \tau) ~ d\tau
\end{equation*}

ahora podemos operar continuamente sobre los tiempos de disparo. También se
tenía un firing rate $r(t)$ definido como la probabilidad de obtener un disparo
en el tiempo $t$. 


\subsection{Presynaptic Current}

Sea $\textbf{u}$ el vector con los \textit{firing rates} de $N_u$ inputs
sinápticos sobre una neurona. Si fijamos el tiempo $t = 0$ como el momento en
que un potencial de acción se dispara en uno de los inputs $u_k$, con $1 \leq k
\leq N_u$, entonces la corriente generada por dicho potencial en el soma de la
neurona recipiente puede describirse como 

\begin{equation*}
    I_{\text{generada}} = w_k K_s(t)
\end{equation*}

donde $K_s(t)$ describe la evolución temporal de la corriente y $w_b$ su
amplitud y sentido. Es decir, se impone que 

\begin{equation*}
    \int_{0}^\infty K_s(t) ~ dt = 1
\end{equation*}

Si $w_k > 0$ ha habido excitación, si $w_k < 0$ inhibición. Por ejemplo, un
posible modelo para $K_s$ es la siguiente función: el rápido ascenso muestra la
apertura de canales iónicos y demás procesos bien conocidos, con una caída más
lenta de la corriente.

\begin{tikzpicture}
    \begin{axis}[
        width=10cm, height=7cm,
        axis lines = left,
        xlabel = {Time $t$ (ms)},
        ylabel = {Synaptic Current $K_s(t)$},
        ymin=0, ymax=0.2, % Adjusted for visibility
        xmin=0, xmax=20,
        domain=0:20,
        samples=100,
        grid=major,
        title={Synaptic Kernel (Alpha Function)},
        legend pos=north east
    ]

    % Define the time constant tau
    \def\tau{2} 

    % The Alpha Function: (t / tau^2) * exp(-t / tau)
    % This function integrates to 1.
    \addplot [
        thick,
        blue,
        mark=none
    ]
    { (x / (\tau^2)) * exp(-x / \tau) };
    
    

    \end{axis}
\end{tikzpicture}

Si asumimos que los disparos en una sinapsis específica son independientes los
unos de los otros, la corriente total en un tiempo $t$ resultante de una
secuencia de disparos en tiempos $t_i$ es: 

\begin{equation}
    w_b \sum_{t_i < t} K_s(t - t_i) = w_b \int_{-\infty}^t K_s(t -
    \tau)\rho_b(\tau) ~ d\tau
\end{equation}

La expresión se entiende mejor con un ejemplo. Notemos que el argumento $t -
t_i$ de $K_s$ es el tiempo transcurrido desde el $i$-ésimo disparo y el momento
actual. Digamos tenemos una sinapsis lenta que tarda 10ms en alcanzar su pico
justo después de un disparo. Esto significa que $K_s(10)$ es el valor máximo de
$K_s$ y $K(0)$ es muy pequeño o nulo. Imaginemos ahora que un disparo acontece
justo en el tiempo 100ms. En el tiempo posterior 110ms medimos la corriente en
el valor $110 - 100$, i.e. $K_s(110 - 100) = K_s(10) = K_\text{max}$. Esto tiene
sentido, porque al medir justo diez milisegundos después del disparo, la
corriente inducida por el disparo es máxima. Lo mismo sucedería si medimos un
tiempo mucho más tardío, obteniendo una corriente menor. Se ve entonces que la
ecuación anterior suma las corrientes inducidas por cada disparo anterior al
momento actual en el momento actual.

\begin{tikzpicture}
    % Define parameters for easy tweaking
    \def\tau{2}      % Time constant
    \def\tstar{8}    % The time of the second spike (shift amount)
    \def\ymax{0.2}   % Y-axis limit

    \begin{axis}[
        width=12cm, height=8cm,
        axis lines = left,
        xlabel = {Time $t$ (current moment) [ms]},
        ylabel = {Synaptic Current},
        ymin=-0.01, ymax=\ymax, % Slight negative ymin to show the start clearly
        xmin=0, xmax=25,
        domain=0:25,
        samples=300, % High sample count for smooth curves
        grid=major,
        title={Visualizing the Shift: $K_s(t)$ vs. $K_s(t - t_*)$},
        legend pos=north east,
        legend cell align={left}
    ]

    % --- Curve 1: Standard Kernel at t=0 ---
    % Function: (t / tau^2) * exp(-t / tau)
    \addplot [
        very thick,
        blue,
        mark=none
    ]
    { (x / (\tau^2)) * exp(-x / \tau) };
    \addlegendentry{Standard Kernel $K_s(t)$ (Spike at $t=0$)}

    % --- Curve 2: Shifted Kernel at t=t_star ---
    % Function: K_s(t - t_star). 
    % We use a boolean multiplier (x >= \tstar) to ensure it's zero before the spike arrives.
    \addplot [
        very thick,
        red,
        mark=none,
    ]
    { (x >= \tstar) * ( (x - \tstar) / (\tau^2) ) * exp(-(x - \tstar) / \tau) };
    \addlegendentry{Shifted Kernel $K_s(t - \tstar)$ (Spike at $t=\tstar$)}
    
    % --- Annotations ---
    
    % Mark Spike Time 1 ($t=0$)
    \draw[dashed, blue, thick] (axis cs:0, 0) -- (axis cs:0, \ymax);
    \node[anchor=south west, blue] at (axis cs:0, \ymax/2) {Spike 1 arrives};

    % Mark Spike Time 2 ($t=t_*$)
    \draw[dashed, red, thick] (axis cs:\tstar, 0) -- (axis cs:\tstar, \ymax);
    \node[anchor=south west, red] at (axis cs:\tstar, \ymax/2) {Spike 2 arrives ($t_*=\tstar$)};

    % Illustrate the "Age" concept at an arbitrary time, e.g., t=12
    \def\tmeasure{12}
    % Draw a line at measurement time
    \draw[thin, gray] (axis cs:\tmeasure, 0) -- (axis cs:\tmeasure, \ymax);
    \node[anchor=south, gray, font=\footnotesize] at (axis cs:\tmeasure, \ymax) {Measurement Time $t=\tmeasure$};

    % Show the "age" of the red spike
    \draw[<->, thick, orange] (axis cs:\tstar, 0.02) -- (axis cs:\tmeasure, 0.02);
    \node[anchor=north, orange, font=\footnotesize] at (axis cs: {(\tstar+\tmeasure)/2}, 0.02) {Age $= t - t_* = 4$ms};
    
    % Point to the value being used on the red curve
    \node[circle, fill=red, inner sep=1.5pt] at (axis cs:\tmeasure, { ((\tmeasure - \tstar) / (\tau^2)) * exp(-(\tmeasure - \tstar) / \tau) }) {};


    \end{axis}
\end{tikzpicture}

Si la interacción entre las distintas sinapsis es lineal, entonces la corriente
total generada por todas las sinapsis en la neurona es simplemente la sumatoria
de la ecuación anterior sobre $b = 1, \ldots, N_u$:

\begin{equation}
    I_s = \sum_{b=1}^{N_u} w_b \int_{-\infty}^t K_s(t - \tau)\rho_b(\tau) ~ d\tau
\end{equation}

Sin embargo, no nos interesa expresar la respuesta inducida en función de los
disparos en las neuronas presinápticas, sino en sus tasas de disparo. Por lo
tanto, reemplazamos $\rho_b(\tau)$ por $u_b$:

\begin{equation}
    I_s = \sum_{b=1}^{N_u} w_b \int_{-\infty}^t K_s(t - \tau) \mu_b(\tau) ~ d
    \tau
\end{equation}

Si usamos 

\begin{equation*}
    K_s(t) = \frac{e^{-t / \tau_r}}{\tau_r}
\end{equation*}

entonces 

\begin{equation}
    \tau_s \frac{dI_s}{dt} = -I_s + \sum_{b=1}^{N_u} w_b u_b = -I_s + \textbf{w}
    \cdot \textbf{u}
\end{equation}

Es decir, $I_s$ se describe con una ecuación diferencial.

\subsection{Post-synaptic current}

Si la corriente sináptica $I_s$ es constante, el firing rate de la neurona
post-sinápitca se puede expresar como $v = F\left( I_s \right) $. La función $F$
se llama activation function y suele tomarse como una función de saturación
(e.g. sigmoidea). Más comúnmente, se usa una rectificación de media onda:

\begin{equation}
    F(I_s) = \left[ I_s - \gamma \right]_+
\end{equation}

donde $\gamma \in \mathbb{R}^+$ es el umbral de rectificación. En este contexto,
$\gamma$ modela la propiedad física de las neuronas que hace que disparen si y
solo si la corriente supera un umbral específico. Si $I_s < \gamma$ se cumple
$F(I_s) = 0$, y si $I_s > \gamma$ se cumple $F(I_s) = I_s - \gamma$, donde el
«sobrante» de esta diferencia determina la magnitud de la respuesta. Por
convención, la variable $I_s$ en $F$ no se toma en unidades de nanoampers sino
de Hertz, donde $\gamma$ también está en Hertz. Quiero decir que $\gamma$ no es
la fuerza que tiene que tener la corriente para que se induzca un potencial
post-sinápitco, sino la rapidez de la tasa de disparo en la neurona necesaria
para inducir un potencial post-sinápitco.

Si nos olvidamos del tiempo, $v = F(I_s)$ ya completa el modelo. La ecuación 


\begin{equation*}
    \tau_s \frac{dI_s}{dt} = -I_s + \sum_{b=1}^{N_u} w_b u_b = -I_s + \textbf{w}
    \cdot \textbf{u}
\end{equation*}

si no hay variación en el tiempo da simplemente 

\begin{equation*}
    I_s = \textbf{w} \cdot \textbf{u}
\end{equation*}

Esto genera un output firing rate de 

\begin{equation}
    v_{\infty} = F\left( \textbf{w} \cdot \textbf{u}\right) 
\end{equation}

que nos dice cuánto una neurona responde a una corriente constante.

Otra forma de pensar la tasa de disparo de la neurona post-sináptica es asumir
que la misma no responde instantáneamente a cambios en la corriente sináptica.
La corriente sináptica puede aumentar o decrecer, pero dicho cambio afecta a la
tasa de disparo con cierta fase temporal debida a los cambios estructurales
inducidos (apertura de canales iónicos, por ejemplo). Se toma entonces un modelo
que es un low-pass filtered version de la tasa de disparo anterior:

\begin{equation}
    \tau_r \frac{dv}{dt} = F \left( I_s(t) \right) - v(t)
\end{equation}

En este modelo, el cambio en la tasa de disparo depende de cuánto le falta a la
tasa actual $v(t)$ para alcanzar el estado de equilibrio determinado por
$I_s(t)$. Si el estado de equilibrio es alcanzado, el cambio es cero. Esta
expresión suele escribirse así, pero es lo mismo:

\begin{equation*}
    \tau_r \frac{dv}{dt} = -v + F\left( I_s(t) \right) 
\end{equation*}

donde $\tau_r$ determina la rapidez con que el firing rate se aproxima a su
valor estable para la constante $I_s$, y qué tan precisamente $v$ sigue las
fluctuciones temporales de $I_s(t)$.

Vamos a asumir que $t_s \ll t_r$, es decir que la corriente $I_s$ inducida por
los disparos presinápticas alcanza su equilibrio muy rápido en comparación con
la tasa de disparo de la neurona post-sinápitca. Entonces podemos aproximar $I_s
= \textbf{w} \cdot \textbf{u}$ en la ecuación anterior. Obtenemos: 

\begin{equation}
    \tau_r \frac{dv}{dt} = -v + F \left( \textbf{w} \cdot \textbf{u} \right) 
\end{equation}

Este es el modelo final.


\pagebreak 

\section{The two-process model}

The two-process model posits a homeostatic sleep pressure $H(t)$ which increases
during wake and dissipates during sleep. During wakefulness, $H(t)$ takes the
following general form:

\begin{equation}
    H(t) = \mu_w + \left( H(t_{\text{off}})- \mu_w \right) 
    \exp \left( - \frac{t - t_{\text{off}}}{\chi_w} \right) 
\end{equation}

where $H(t_{\text{off}}) < \mu_w$. $\mu_w$ acts as the upper asymptote
(saturation level). The term $(H(t_{\text{off}}) - \mu_w)$ is a negative scaling
constant. Consequently, while the exponential component decays over time, the
overall function $H(t)$ increases, asymptotically approaching $\mu_w$ as the
'distance' to the saturation point shrinks.

\begin{center}
\begin{tikzpicture}
    \begin{axis}[
        width=10cm, height=7cm,
        xlabel={Time $t$ (hours)},
        ylabel={Sleep Pressure $H(t)$},
        xmin=0, xmax=16,
        ymin=0, ymax=1.6,
        grid=major,
        legend pos=south east,
        title={Two-Process Model: Wake Phase Dynamics},
        /pgf/number format/.cd,
        use comma,
        1000 sep={}
    ]

    % Define the function parameters for reuse or clarity
    % Equation: H(t) = mu + (H_off - mu) * exp(-(t - t_off) / chi)

    % --- Curve 1: Standard Baseline ---
    % mu_w=1.0, chi_w=5, t_off=0, H_start=0.2
    \addplot[
        blue, 
        thick, 
        domain=0:16, 
        samples=100
    ]
    {1.0 + (0.2 - 1.0) * exp(-(x - 0) / 5.0)};
    \addlegendentry{$\mu_w=1.0, \chi_w=5, t_{\text{off}}=0$}

    % --- Curve 2: Late Start, Fast Rise ---
    % mu_w=0.8, chi_w=3, t_off=2, H_start=0.2
    % Note: Domain starts at 2 because equation is undefined/different before wake
    \addplot[
        red, 
        thick, 
        domain=2:16, 
        samples=100
    ]
    {0.8 + (0.2 - 0.8) * exp(-(x - 2) / 3.0)};
    \addlegendentry{$\mu_w=0.8, \chi_w=3, t_{\text{off}}=2$}

    % --- Curve 3: High Saturation, Slow Rise ---
    % mu_w=1.2, chi_w=8, t_off=0, H_start=0.4
    \addplot[
        green!60!black, 
        thick, 
        domain=0:16, 
        samples=100
    ]
    {1.2 + (0.4 - 1.2) * exp(-(x - 0) / 8.0)};
    \addlegendentry{$\mu_w=1.2, \chi_w=8, t_{\text{off}}=0$}

    % Optional: Add dashed lines for the asymptotes (mu_w)
    \addplot[blue, dotted, domain=0:16] {1.0};
    \addplot[red, dotted, domain=2:16] {0.8};
    \addplot[green!60!black, dotted, domain=0:16] {1.2};

    \end{axis}
\end{tikzpicture}
\end{center}

The model for sleep is the same but with different parameters:

\begin{equation}
    H(t) = \mu_s + \left( H(t_{\text{on}})- \mu_s \right) 
    \exp \left( - \frac{t - t_{\text{on}}}{\chi_s} \right) 
\end{equation}

and with $H(t_{\text{on}}) > \mu_s$. This makes $H(t)$ decreasing during sleep.
It is easily seen that

\begin{equation}
    \frac{dH}{dt} = \frac{\mu}{\chi} - \frac{H}{\chi}
\end{equation}

It becomes apparent that the change in homeostatic pressure depends on the
balance between a constant gain in homeostatic pressure $\mu / \chi$ and a loss
which becomes greater as $H \to \infty$. In short, dissipation is proportional
to the instantaneous sleep pressure ($1 / \chi \cdot H$), while gain is
fixed.

Daan, Beersma and Borbély improved the model by introducing a circadian rhythm
$C(t) = \cos \omega t$ and positive and negative threshold curves: 

\begin{align*}
    H^+(t) = H_0^+ + \alpha C(t) \\ 
    H^-(t) = H_0^- + \alpha C(t) 
\end{align*}

It is now understood that whenever the homeostatic pressure $H(t)$ coincides
with $H^+(t)$ the upper threshold, there's a transition from wake to sleep. When
$H(t)$ coincides in its decay with $H^-(t)$, there's a transition from sleep to
wake. Biologically speaking, $H^+$ represents the resistance to falling asleep,
or the maximum pressure the organism can handle before succumbing to sleep.
$H^-$ is interpreted in analogous fashion. No physiological correlates of these
thresholds have yet been found, which is a weak point of the model
counterbalanced only by the fact that it replicates human sleep patterns quite
well. Daan, Beersma and Borbély suggest that these thresholds represent
circadian variations in the sensitivity of brain regions involved in the
generation of sleep and wakefulness to homeostatic sleep pressure. The upper
threshold in particular may be labile to short term environmental,
physiological and psychological changes such as light, stimulants (coffee) and
motivational aspects [Skeldon and Dijk, 2025].

To analyze the system's behavior purely, researchers often simplify the
equations by removing units. This is done by shifting and scaling appropriately.
For instance, in the original model, sleep pressure decays down to a lower limit
$\mu_s$. It is much easier mathematically if things decay to zero. So, we define
a new variable $\tilde{H}$ that represents the \textit{difference} from the
baseline:
\[
H = \tilde{H} + \mu_s
\]
What we do here is equivalent to what is done when speaking of altitude
\textit{above sea level}: it's useful to take $\mu_s$ as reference value and
speak only of \textit{sleep pressure above $\mu_s$}. 

By substituting this into our differential equations, the loss term during sleep
simplifies. Since the asymptote is now 0, the equation becomes:

\begin{equation}
    \frac{d\tilde{H}}{dt} = -\frac{1}{\chi_s}\tilde{H} \quad (\text{during sleep})
\end{equation}

The parameter $\mu_s$ has effectively disappeared. This doesn't mean there is no
homeostatic gain during sleep, but rather that now \textit{gain} or
\textit{loss} is expressed in the sign of $\tilde{ H }$. 

In analogous fashion, the wake phase is normalized so that the full pressure
equals exactly 1. We do this by scaling our variable by the total possible range
of the system ($\mu_w - \mu_s$). We update our definition of $\tilde{H}$ to be a
\textbf{percentage} of the total range:
\[
H = (\mu_w - \mu_s)\tilde{H} + \mu_s
\]
Or, rearranged to solve for our new variable $\tilde{H}$:
\[
\tilde{H} = \frac{H - \mu_s}{\mu_w - \mu_s}
\]

Now, $\tilde{H}$ is a dimensionless number between 0 and 1. 

When we plug this scaled variable back into the original differential equations,
the constants $\mu_w$ and $\mu_s$ cancel out entirely. During wake, the system
raises to 1:

\begin{equation}
    \frac{d\tilde{H}}{dt} = \frac{1}{\chi_w} - \frac{1}{\chi_w}\tilde{H}
\end{equation}

During sleep, it falls to zero:

\begin{equation}
    \frac{d\tilde{H}}{dt} = -\frac{1}{\chi_s}\tilde{H}
\end{equation}

The general form during wake becomes:

\begin{align*}
    \tilde{ H }(t) = 1 + ( \tilde{ H }(t_{\text{off}}) - 1 )
    \exp 
    \left( -\frac{t - t_{\text{off}}}{\chi_w} \right) 
\end{align*}

The general form during sleep: 

\begin{equation}
    \tilde{ H }(t) = \tilde{ H }(t_{\text{on}}) 
    \exp 
    \left( -\frac{t - t_{\text{on}}}{\chi_w} \right) 
\end{equation}

\pagebreak 

\section{A physiological model of sleep}

Here I present a physiological model developed by Phillips and Robinson in a
paper publihsed in 2006. The work is of great interest because previous (and
most) sleep models, such as the two-process model, were strictly
phenomenological and dealt not with the underlying neurophysiology.

The model is based on the dynamics of the ascending arousal system (AAS), a
complex of acetylcholinic and monoaminergic nuclei in the brainstem. The AAS,
which projects difussely to the cerebrum, controls the arousal state of the
brain. Two distinct drives affect the activity of the AAS: the circadian drive
and the homeostatic drive, which are previously integrated in the the
ventrolateral preoptic area (VLPO) of the hypothalamus. Within-system
interactions of the AAS---i.e. interactions between the nuclei conforming the
AAS---produce the ultradian rhythms characteristic of sleep, like NREM and REM
cycles, but are disregarded in the modelling of sleep-wake cycles. 

The flow of information across these systems is relatively simple. The
homeostatic and circadian drives are both transmitted to the AAS from inhibitory
GABAergic projections of the VLPO. The homeostatic signal is caused by the
progressive accumulation of adenosin in the extracellular medium of the basal
fobrain, which causes the neurons in this region to become inhibited. Since
these play an inhibitory role on the VLPO, their own inhibition induces a
desinhibition of the VLPO, which itself then signals the AAS more strongly. The
circadian signal originates in the suprachiasmatic nucleus and projects to the
VLPO via the dorsomedial nucleus of the hypothalamus. It is a fundamentally
endogenous oscillator entrained to the day mainly by the influence of light. 

In general, it is understood that the monoaminergic nuclei of the AAS promotes
wakefulness by forming a mutually inhibitory circuit with the VLPO, which is
active during sleep. These two systems are therefore self-reinforcing and
conform a flip-flop switch. Contrarily, there is no firm evidence of direct
connections from the acetylcholinic nuclei to the VLPO: it rather seems that the
VLPO connects to the acetylcholinic nuclei via inhibitory interneurorns.

Phillips and Robinson's model, hereby termed the PRM, describes three neural
populations: the monoaminergic and acetylcholinic nuclei in the AAS and the
VLPO. As described before, the VLPO integrates the homeostatic and circadian
drives and inhibits the AAS, an aspect which the model must capture. Since
sleep-wake transitions are modulated by the mutual inhibition between the VLPO
and the monoaminergic nuclei, the activity of the acetylcholinic nuclei is
modeled at a constant intermediate level. For further justification of this
decision, see the original paper.

As most papers, Phillips and Robinson do not provide an intuition behind the
mathematical expressions they use. I should wish to provide an explanation not
too detailed that it becomes tedious, but sufficient to make the model
accessible to, say, an undergraduate student.

First of all, the PRM model does not deal with individual neurons but rather
with neural populations. Topologically, the neural populations may be understood
as a dynamically evolving weighted graph: each node is a cluster of neurons with
some average firing rate and average voltage, with edges representing the sense
and magnitude of the interactions between populations. The dynamism of course
comes from fluctuations in the voltage, which in turn must affect the firing
rate. In general, the relation between the firing rate of a cluster and its
voltage is modeled with a sigmoid function, thus capturing the saturation of the
firing rate as voltage increases:

\begin{equation}
    Q_a(t) = \frac{ Q_{\text{max}} }{1 + \exp\left( \frac{ V_a(t) - \mu
    }{\sigma'} \right)  }
\end{equation}

The function $V_a(t)$ is understood to express voltage relative to the average
resting potential of the neurons in the cluster. This means that if $V_a(t) =
10\text{mV}$ and the mean resting potential is $-70\text{mV}$, the actual mean
voltage of the neurons in the cluster is $-60\text{mV}$. The $\mu$ parameter
defines the mean threshold voltage (also relative to resting). This means that
when $V_a(t)$ crosses $\mu$, the firing rate $Q_a(t)$ is at half its maximum
value. Thus, $\mu$ defines a critical voltage level: below it, most neurons in
the cluster are quiescent; above it, most neurons are firing. $\sigma'$ defines
the steepness of the sigmoid around $\mu$, and is related to the standard
deviation because $\sigma = \sigma' \pi / \sqrt{3} $. 

The voltage itself is modeled as a leaky integrator of incoming post-synaptic
currents into the neurons of the cluster. For the VLPO, which receives and
integrates the circadian and homeostatic drives, we have: 

\begin{equation}
    \tau_v \frac{dV_v}{dt} = -V_v + w_{vm} Q_m + D
\end{equation}

where $w_{vm}$ is  the weight of the connection from the monoaminergic nuclei to
the VLPO, $Q_m$ the firing rate of the monoaminergic cluster, and $D$ is the
linear combination of the circadian and homeostatic drives: 

\begin{equation*}
    D = w_{vC} C + w_{vH} H
\end{equation*}

As in the two-process model, the homeostatic drive $H$ is modelled
phenomenologically, aiming to capture the metabolic activity that is high during
wake and low during sleep. Because arousal correlates highly with monoaminergic
activity, the PRM assumes that the homeostatic drive increases proportionally to
the firing rate of the monoaminergic nuclei $Q_m$. Also as in the two-process
model, $C$ is a simple sinusoidal function with a 24 hour period:

\begin{equation}
    C(t) = \cos(\omega t) + \mu_C, \qquad \chi \frac{dH}{dt} = - H + \mu_H Q_m
\end{equation}

Note that this model of $H$ assumes that when $Q_m = 0$, i.e. during sleep, $H$ 
is negligible.

The voltage of the monoaminergic nuclei depends on the excitatory input from the
acetylcholinic nuclei and the inhibitory input from the VLPO, from which readily
follows the following linear leaky integrator model:

\begin{equation}
    \tau_m \frac{dV_m}{dt} = - V_m + w_{mv}Q_v + w_{ma} Q_a
\end{equation}

The firing rates of the VLPO and the monoaminergic nuclei use the same sigmoid
parameters because empirically both populations have similar firing rate
dynamics. 

As detailed in the original paper, mammalian locus coeruleus neurons have a mean
threshold of $-55\text{mV}$ and a resting potential of $-65\text{mV}$, giving a
relative threshold of $\mu = 10\text{mV}$. Maximal firing rates under prolonged
depolarization of single neurons were around $150$sec$^{-1}$, so we set
$Q_{\text{max}} = 150$sec$^{-1}$. $\sigma'$ values within $2-6\text{mV}$ for
cortical neurons are typical, and Phillips and Robinson argue that because
voltage ranges between cortical and brainstem nuclei neurons are similar, a
value of $\sigma' = 3\text{mV}$ is appropriate.

The weights $w_{jk}$, representing the strength and sense of the connections
from population $k$ into population $j$, have known signs. The VLPO and the
monoaminergic populations, as stated earlier, are mutually inhibitory, from
which follows $v_{mv} < 0, v_{vm} < 0$. The acetylcholinic population excites
the monoaminergic population, so $w_{ma} > 0$. The homeostatic drive disinhibits
the VLPO, so $w_{vH} > 0$, while the circadian drive inhibits it, so $w_{vC} <
0$.

In the PRM, the flip-flop switch behavior of the sleep-wake cycle entails 
an equilibrium with two stable fixed points (wake and sleep) and an unstable
fixed point (the threshold between sleep and wake). To find steady states, they
solve the system of equations given by setting the voltage derivatives to zero:

\begin{align*}
    0 &= -V_v + w_{vm} Q_m + D \\
    0 &= - V_m + w_{mv}Q_v + w_{ma} Q_a \\
\end{align*}

From the second equation, we obtain that the equilibrium voltage satisfies

\begin{equation*}
    V_m = w_{mv}Q_v + w_{ma} Q_a
\end{equation*}

The firing rate $Q_m$ at equilibrium is a function of the equilibrium voltage,
so that we may write $Q_m = S(V_m)$. Thus, from the first equation, it follows 

$$0 = -V_v + D + \nu_{vm} S(\nu_{mv} Q_v + \nu_{ma} Q_{ao})$$

So the equilibrium occurs at the roots of the function

\begin{align*}
    f_v(V_v) 
    &= -V_v + D + w_{vm} S(w_{mv} S(V_v) + w_{ma} Q_a) \\
    &= \tau_v
    d\frac{V_v}{dt}
\end{align*}

Phillips and Robinson analyze this function and approximate its roots through
the roots of two other, related but simpler functions. These roots are 

\begin{equation*}
    K_{v1} = D + w_{vm} S(w_{ma} Q_{ao}), \qquad K_{v2} = D + w_{vm}S \left(
    w_{mv}Q_{\text{max}} + w_{ma} Q_{ao} \right) 
\end{equation*}

Similarly, they found the equilibrium points for the monoaminergic population.
Furthermore, since the firing rate of the monoaminergic and the VLPO populations
have the same parameters, they reason that the equilibrium points of both
populations should be roughly equal, reducing the dimensionality of the problem.
Their mathematical analysis is extensive, so I will simply state that they found
the wake equilibrium voltage to fall within $-15\text{mV}$ and $-5\text{mV}$,
and the sleep equilibrium voltage to fall within $0$ and $1.5\text{mV}$.


\pagebreak 

\section{Fundamentos de neurociencia computacional} 

\subsection{Introducción}

Estas notas están basadas en el libro \textit{Theoretical Neuroscience} de Dayan
y Abbott. Esta obra, que se ha convertido en algo así como la biblia de la
neurociencia computacional, no está traducida al castellano. Este es un humilde
intento de democratizar al menos algunos elementos fundamentales de la
neurociencia computacional.

La neurociencia computacional, más que una disciplina, es una perspectiva.
Realizar experimentos es, en el mejor de los casos, costoso, y en el peor
imposible. Es difícil controlar los parámetros involcurados en, digamos, un
potencial de acción, a menos que el experimento tenga una dimensión minúscula.
La perspectiva computacional pretende resolver este problema ofreciendo la
posibilidad de simular las dinámicas neurales. Esto ofrece un control absoluto
de los parámetros, lo cual resulta en una posibilidad de explorar dinámicas que,
en la práctica, son imposibles de estudiar. El costo que se paga es el de la
simplificación: todo modelo es, en última instancia, una más o menos
piadosa mentira.

La neurociencia computacional no es simple: ecuaciones diferenciales, teoría de
la probabilidad, electroestática y circuitos, biología celular y anatomía, todas
forman parte de esta hermosa creación humana. Es difícil pedir a un estudiante
que posea un dominio sobre todas estas áreas. Para nuestra fortuna, vivimos en
un tiempo en que la información abunda, y cualquier laguna puede ser
completada con un poco de dedicación y estudio. En este sentido, la neurociencia computacional es la vocación ideal para el raro, pero a mi criterio
lindo tipo de personalidad que gusta de ser exigida y que aspira, como los
renacentistas, a una vida intelectual amplia y completa.

\subsection{Modelos fenomenológicos}

La animación de abajo muestra una neurona de macaco (o, mejor dicho, una
simulación precisa de una) disparando repetidas veces. A simple vista, parecen
picos aleatorios que saltan aquí y allá en el tiempo. Sin embargo, es
sorprendente la cantidad de información que se puede extraer de un fenómeno tan
simple, así como la riqueza de los modelos matemáticos que lo simulan.

\begin{figure}[h]
    \centering
    % Nota: Si usas pdflatex, te recomiendo convertir el gif a png o jpg.
    \includegraphics[width=0.6\textwidth]{../Images/firing.gif} 
    \caption{Simulación de disparos neuronales.}
\end{figure}

La simulación de arriba es una simulación fenomenológica, en el sentido de que
no intenta reproducir las dinámicas que subyacen a un fenómeno —el cambio en los
voltajes de la membrana celular, el flujo de iones, la morfología de la neurona,
etc.— sino simplemente alguna de sus manifestaciones inmediatamente observables.
Más adelante vamos a adentrarnos en qué hace que una neurona dispare: por el
momento, nos basta con vivir en un mundo simple donde las neuronas disparan ante
ciertos estímulos, y la probabilidad de disparo depende de propiedades simples
del estímulo como su duración o su intensidad.

Al observar los tiempos en los que una neurona dispara, que es lo que muestra la
animación de arriba, nos enfrentamos primero al problema de proporcionar un buen
modelo matemático para una serie de pulsos en el tiempo. La función $\delta$ de
Dirac es la candidata más obvia. En particular, sea $\{t_{1}, \ldots, t_{n}\}$
la secuencia de tiempos en los que ocurrió un pulso. Definimos la
\textbf{función de respuesta neuronal (FRN)} como:

\begin{equation}
\rho (t) := \sum\limits_{i=1}^{n}\delta(t -t_{i})
\end{equation}

La $\delta$ de Dirac es una unidad de probabilidad (o conteo) concentrada en un
punto. $\rho(t)$ no es más que una sumatoria que «verifica» si en el tiempo $t$ ha
habido un disparo, satisfaciendo que la cantidad de disparos en el intervalo de
tiempo $[ t_1, t_2 ]$ es 

\begin{equation}
    \int_{t_1}^{t_2} \rho(\tau) ~ d\tau
\end{equation}

Una propiedad que se sigue directamente de la definición de la $\delta$ de Dirac
es que, para cualquier función $h(t)$ con un comportamiento adecuado, tenemos:

\begin{equation}
\sum\limits_{i=1}^{n} h(t - t_{i}) = \int_{-\infty}^{\infty} \rho(t-\tau)h(\tau) \, d\tau
\end{equation}

La expresión de arriba es una convolución, y es provechoso desarrollar cierta
intuición respecto a lo que significa. Recordemos que, dada una función $f(x)$, 
la función $f(-x)$ es un reflejo de $f(x)$ respecto al eje de $y$ (es $f(x)$
«dada vuelta»). La función $f(c - x)$ es la función $f(-x)$ desplazada
horizontalmente a la derecha $c$ unidades. 

En la convolución, dentro de la expresión $\rho(t - \tau)$, $t$ efectivamente
juega el papel de una constante. Es la función de activación invertida y
desplazada horizontalmente. Si $\rho(t)$ es no-nulo solo si $t$ es un tiempo de
disparo, $\rho(t - \tau)$ es no-nulo solo si $\tau$ instantes antes de $t$ hubo un
disparo. 

Se entiende que $h$ es una expresión que representa la dinámica temporal de un
disparo, de manera tal que $h(0)$ es justo el momento de disparo y $h(t)$ con
$t>0$ representa alguna evolución o dinámica del sistema. Por ejemplo, $h(t)$
podría ser el potencial de membrana.

Si consideramos los dos párrafos anteriores, entonces vemos que $\rho(t - \tau)$
es no-nula solo si hace $\tau$ segundos hubo un disparo. Si ese fue el caso,
incorporamos a la sumatoria el valor $h(\tau)$, que nos dice cuál es el estado
de la dinámica generada por dicho disparo. Como han pasado $\tau$ segundos desde
el disparo, $h(\tau))$ es la forma correcta de incorporar la contribución de
dicho disparo.


\begin{tikzpicture}[scale=1.1]

% -------------------------
% PARAMETERS
% -------------------------
\def\tone{1.5}
\def\ttwo{2.0}
\def\tthree{5.0}

% -------------------------
% AXES STYLE
% -------------------------
\tikzset{
    axis/.style={->, thick},
    delta/.style={very thick, blue},
    hfunc/.style={thick, red},
    conv/.style={thick, purple}
}

% =========================
% PANEL 1: rho(t)
% =========================
\begin{scope}
\draw[axis] (0,0) -- (6,0) node[right] {$t$};
\draw[axis] (0,0) -- (0,2) node[above] {$\rho(t)$};

% Dirac spikes
\draw[delta] (\tone,0) -- (\tone,1.5);
\draw[delta] (\ttwo,0) -- (\ttwo,1.5);
\draw[delta] (\tthree,0) -- (\tthree,1.5);

\node[below] at (\tone,0) {$t_1$};
\node[below] at (\ttwo,0) {$t_2$};
\node[below] at (\tthree,0) {$t_3$};

\node at (3,2.2) {\textbf{(a) FRN $\rho(t)$}};
\end{scope}

% =========================
% PANEL 2: h(t - tau)
% =========================
\begin{scope}[yshift=-4cm]
\draw[axis] (0,0) -- (6,0) node[right] {$\tau$};
\draw[axis] (0,0) -- (0,2) node[above] {$h(\tau)$};

% h(t) : exponential pulse
\draw[hfunc, domain=0:6, samples=100]
    plot (\x, {1.8*exp(-1.5*(\x-1))*(\x>1)});

\node at (3,2.2) {\textbf{(b) Función $h(\tau)$}};
\end{scope}

% =========================
% PANEL 3: Convolution result
% =========================
\begin{scope}[yshift=-8cm]
\draw[axis] (0,0) -- (6,0) node[right] {$t$};
\draw[axis] (0,0) -- (0,2.5) node[above] {};


% sum
\draw[conv, domain=0:6, samples=200]
    plot (\x,
    {
      1.2*(exp(-1.5*(\x-\tone))*(\x>\tone)
      +exp(-1.5*(\x-\ttwo))*(\x>\ttwo)
      +exp(-1.5*(\x-\tthree))*(\x>\tthree))
    });

\node at (3,2.8) {\textbf{(c) $\displaystyle \int \rho(t-\tau)h(\tau)\,d\tau
= \sum_i h(t-t_i)$}};
\end{scope}

\end{tikzpicture}

Como nota, la convolución es conmutativa, y podríamos haber escrito la expresión
integrada como $h(t - \tau) \rho(t)$ manteniendo el mismo resultado y la misma
interpretación. 

Los potenciales de acción son fenómenos estereotípicos. Entre dos potenciales de
acción no existe una diferencia esencial más allá del tiempo en que ocurren. Por
ende, cualquier mecanismo que subyazca a la codificación neuronal debe tener que
ver con la disposición de tales disparos en el tiempo. En este sentido, lo
sincrónico y lo asincrónico son el lenguaje binario del cerebro.

Por esta razón, la caracterización temporal de los trenes de pulsos es de gran
interés. Un concepto básico es la tasa de disparo de una neurona, que puede
conceptualizarse de varias maneras. La más general es

\begin{equation}
r(t) = \frac{1}{\Delta t} \int_{t}^{t+\Delta t} \langle \rho(\tau)\rangle \, d\tau
\end{equation}

donde $\langle \cdot \rangle$ denota el promedio entre ensayos
(\textit{trial-average}). Debería ser claro que $r(t)\Delta t$ proporciona la
probabilidad de que ocurra un pulso en el tiempo $t$, dado que $\Delta t$ es
suficientemente pequeño. Una propiedad importante es que 

\begin{equation}
    \int \langle \rho(t - \tau)\rangle h(\tau) ~ d\tau = \int r(t - \tau)
    h(\tau) ~ d\tau
\end{equation}

Es decir que, en el contexto de integraciones, la tasa de disparo y la FRN son
equivalentes. 

La tasa de disparo también puede definirse como un promedio de muchos
experimentos: 

\begin{equation}
    r = \frac{n}{T} = \frac{1}{T} \int_{0}^T
    \rho(\tau) ~ d\tau
\end{equation}

con $n$ la cantidad de disparos. Por supuesto que puede utilizarse $\langle
n\rangle $ el trial-average de $n$, obteniendo:

\begin{equation}
\langle r \rangle = \frac{\langle n\rangle }{T} = \frac{1}{T} \int_{0}^{T} r(\tau) \, d\tau
\end{equation}

Esta definición es menos general porque es independiente del tiempo. En general,
nos enfocaremos en $r(t)$. Desde luego que una aproximación adecuada de $r(t)$
requeriría una cantidad inmensa (o infinita, si se quiere ser exacato) de
experimentos. Por lo tanto, es provechoso hallar maneras de aproximar $r(t)$ con
uno o pocos experimentos, i.e. con una cantidad limitada de datos. Para este fin
se utilizan filtros lineales.

Un filtro lineal es una convolución de $\rho$ con una función de peso $w$.

\begin{equation}
R(t) = \int_{-\infty}^{\infty} w(\tau)\rho(t - \tau) \, d\tau
\end{equation}

donde $R$ es una aproximación de $r$. Dos funciones $w$ comúnmente utilizadas
son el kernel Gaussiano, que es una sliding window,

\begin{equation}
w(\tau) = \frac{1}{\sqrt{2\pi}\sigma_{w}}\exp \left( \frac{-\tau^{2}}{2\sigma_{w}^{2}} \right)
\end{equation}

y una función de ventana (window function) en ventanas o segmentos de tamaño
$\Delta t$:

\begin{equation}
w(t) = \begin{cases} \frac{1}{\Delta{t}} & t \in \left[\frac{-\Delta{t}}{2}, \frac{\Delta{t}}{2}\right] \\ 0 & \text{en otro caso} \end{cases}
\end{equation}



\begin{tikzpicture}[
    scale=1.2,
    spike/.style={thick, blue},
    gauss/.style={thick, red},
    box/.style={thick, orange},
    axis/.style={->, >=stealth}
]

% Definición de los 20 tiempos de disparo (no uniformes)
\def\spikes{0.5, 0.7, 1.2, 2.5, 2.7, 2.9, 3.2, 4.5, 4.7, 5.8, 6.0, 6.1, 6.3, 7.5, 8.2, 8.4, 8.6, 8.8, 9.2, 9.5}

% --- PANEL A: TREN DE PULSOS rho(t) ---
\begin{scope}[yshift=0cm]
    \draw[axis] (0,0) -- (10.5,0) node[right] {$t$};
    \draw[axis] (0,0) -- (0,1.2) node[above] {$\rho(t)$};
    \foreach \t in \spikes {
        \draw[spike] (\t,0) -- (\t,0.8);
    }
    \node[anchor=west] at (3,1.5) {\textbf{A. Tren de pulsos (Dirac Deltas)}};
\end{scope}

% --- PANEL B: KERNEL GAUSSIANO ---
% Aproximación de suma de Gaussianas: R(t) = sum exp(-(t-ti)^2 / 2sigma^2)
\begin{scope}[yshift=-3.5cm]
    \draw[axis] (0,0) -- (10.5,0) node[right] {$t$};
    \draw[axis] (0,0) -- (0,1.5) node[above] {$R(t)$};
    
    \draw[gauss, samples=200, domain=0:10] plot (\x, {
        0.4*(exp(-(\x-0.5)^2/0.2) + exp(-(\x-0.7)^2/0.2) + exp(-(\x-1.2)^2/0.2) + 
        exp(-(\x-2.5)^2/0.2) + exp(-(\x-2.7)^2/0.2) + exp(-(\x-2.9)^2/0.2) + exp(-(\x-3.2)^2/0.2) +
        exp(-(\x-4.5)^2/0.2) + exp(-(\x-4.7)^2/0.2) + exp(-(\x-5.8)^2/0.2) + exp(-(\x-6.0)^2/0.2) +
        exp(-(\x-6.1)^2/0.2) + exp(-(\x-6.3)^2/0.2) + exp(-(\x-7.5)^2/0.2) + exp(-(\x-8.2)^2/0.2) +
        exp(-(\x-8.4)^2/0.2) + exp(-(\x-8.6)^2/0.2) + exp(-(\x-8.8)^2/0.2) + exp(-(\x-9.2)^2/0.2) +
        exp(-(\x-9.5)^2/0.2))
    });
    \node[anchor=west] at (3,1.8) {\textbf{B. Filtro Gaussiano} (Kernel suave)};
\end{scope}

% --- PANEL C: VENTANA RECTANGULAR (Moving Average) ---
\begin{scope}[yshift=-7cm]
    \draw[axis] (0,0) -- (10.5,0) node[right] {$t$};
    \draw[axis] (0,0) -- (0,1.5) node[above] {$R(t)$};

    % Dibujado manual de una respuesta por bins para ilustrar el concepto
    \draw[box] (0,0) -- (0.3,0) -- (0.3,0.6) -- (1.5,0.6) -- (1.5,0.2) -- (2.2,0.2) -- 
               (2.2,1.2) -- (3.5,1.2) -- (3.5,0.2) -- (4.2,0.2) -- (4.2,0.7) -- (5.0,0.7) -- 
               (5.0,0.2) -- (5.5,0.2) -- (5.5,1.4) -- (6.8,1.4) -- (6.8,0.3) -- (7.2,0.3) -- 
               (7.2,0.5) -- (7.8,0.5) -- (7.8,1.3) -- (9.2,1.3) -- (9.2,0.6) -- (10,0.6);
               
    \node[anchor=west] at (3,1.8) {\textbf{C. Ventana Rectangular} (Bin-average)};
\end{scope}

\end{tikzpicture}

Como muestra el gráfico, los filtros lineales nos dan una idea de la
distribución temporal de los disparos y por ende aproximan adecuadamente a 
$r(t)$.

\subsection{Estímulos externos determinando la tasa de disparo}

En nuestro mundo simplificado, una neurona dispara ante ciertos estímulos. Más
formalmente, existe un estímulo (o una propiedad de un estímulo) $g(t)$ variable
en el tiempo, y existe una función de dicho estímulo $f \circ g$ que determina
la tasa de disparo. Es decir, modelamos 

\begin{equation}
    \langle r \rangle = f \left( g(t) \right) 
\end{equation}

La función $f$ se denomina función de sintonización. En general, $g$ y $\left< r
\right>$ son conocidos y $f$ debe determinarse ajustándola a los datos. Por
ejemplo, en un experimento descrito en \textit{Theoretical Neuroscience}, de
Dayan y Abbott, se mostraba a un macaco una barra de luz que rotaba desde una
situación horizontal hasta una situación vertical. Mientras esto sucedía, se
registraba la activación de una neurona en la corteza visual del animal. Se
observó una variación notable en la tasa de disparo de la neurona en función del
ángulo de la barra de luz. En este caso, $g(t)$ describiría el ángulo de la
barra en el tiempo, y $f(s)$---con $s$ el estímulo, en este caso un
ángulo---debería ajustarse a los datos experimentales usando alguna técnica de
ajuste.

Las funciones de sintonización nos permite describir la tasa promedio de disparo
de una neurona ante un estímulo---o más bien, una propiedad de un
estímulo---$s$. Alternativamente, es provechoso preguntar si los disparos de una
neurona nos permiten describir el estímulo que los provocó. Podemos preguntar
entonces: ¿qué hizo el estímulo en promedio justo antes de que se desatara un
potencial de acción? A este promedio lo denominamos $C(t)$.

Para simplificar ciertas propiedades, asumimos que el estímulo $s(t)$ tiene un
promedio de cero en el tiempo que dura el experimento:

\begin{equation}
    \frac{1}{T} \int_0^T s(t) ~ dt = 0
\end{equation}

También asumimos que el estímulo es periódico con un periodo de $T$, es decir
que $s(T + \tau) = s(\tau)$ para todo $\tau$ y que por ende 

\begin{equation}
    \int_0^T h \left( s(t + \tau) \right) ~ dt = \int_\tau^{T + \tau} h(s(t)) ~
    dt = \int_0^T h(s(t)) ~ dt
\end{equation}

Si existen $n$ disparos $t_1, \ldots, t_n$, $C(\tau)$ es el valor
promedio de un estímulo justo $\tau$ unidades de tiempo antes de que ocurriera
un disparo. Más aún, usamos el promemdio de este valor a lo largo de múltiples
experimentos, indicado otra vez por la notación $\left< \cdot \right>$:

\begin{equation}
    C(\tau) = \left< \frac{1}{n} \sum_{i=1}^n s(t_i - \tau) \right>
\end{equation}

Podemos expresar esto como una convolución con $\rho$:

\begin{equation}
    C(\tau) = \frac{1}{\langle n \rangle } \int_{0}^T \left< \rho(t) \right>s(t -
    \tau) ~ dt = \frac{1}{\langle n\rangle }\int_0^T s(t - \tau) r(t) ~ dt
\end{equation}

La segunda igualdad se debe a la equivalenca de $\left< \rho(t) \right>$ y
$r(t)$ en el contexto de integración.


\begin{tikzpicture}[
    scale=1.2,
    axis/.style={->, thick},
    spike/.style={very thick, blue},
    stim/.style={thick, red},
    sta/.style={thick, purple}
]

% -------------------------
% PARAMETROS
% -------------------------
\def\Tmax{10}
\def\spikes{1.2, 2.1, 2.8, 4.0, 4.3, 5.7, 6.1, 7.4, 8.0, 9.1}

% =========================
% PANEL A: TREN DE DISPAROS
% =========================
\begin{scope}
    \draw[axis] (0,0) -- (\Tmax+0.5,0) node[right] {$t$};
    \draw[axis] (0,0) -- (0,1.4) node[above] {$\rho(t)$};

    \foreach \t in \spikes {
        \draw[spike] (\t,0) -- (\t,1);
    }

    \node at (5,1.7) {\textbf{(A) Tren de disparos}};
\end{scope}

% =========================
% PANEL B: ESTIMULO s(t)
% =========================
\begin{scope}[yshift=-3.5cm]
    \draw[axis] (0,0) -- (\Tmax+0.5,0) node[right] {$t$};
    \draw[axis] (0,-1.5) -- (0,1.5) node[above] {$s(t)$};

    % Estímulo cosenoidal de media cero
    \draw[stim, samples=200, domain=0:\Tmax]
        plot (\x, {1.2*cos(2*pi*0.25*\x r)});

    \node at (5,1.8) {\textbf{(B) Estímulo $s(t)$}};
\end{scope}

% =========================
% PANEL C: C(tau)
% =========================
\begin{scope}[yshift=-7cm]
    \draw[axis] (0,0) -- (4.5,0) node[right] {$\tau$};
    \draw[axis] (0,-1.2) -- (0,1.5) node[above] {$C(\tau)$};

    % Forma típica de STA: oscilación amortiguada
    \draw[sta, samples=200, domain=0:4]
        plot (\x, {1.2*cos(2*pi*0.25*\x r)*exp(-0.8*\x)});

    \node at (2.2,1.8) {\textbf{(C) $C(\tau)$ (STA)}};
\end{scope}

\end{tikzpicture}

Una serie de disparos $t_1,\ldots, t_n$ puede modelarse como un proceso
estocástico, donde en cada tiempo $t$ hay cierta posibilidad de que ocurra un
potencial de acción. No es difícil observar que si $r(t)$, es constantemente
$r$, y si asumimos independenecia entre un potencial de acción y otro, entonces
los potenciales de acción constituyen un proceso de Poisson homogéneo con
parámetro $\lambda = rT$: 

\begin{equation}
    P_T(n) = \frac{(rT)^n}{n!} e^{-rT}
\end{equation}

con $T$ la duración del experimento y $n$ el número de potenciales de acción.
Esto significa que, cuando $r(t)$ constante, la varianza y la media de $n$ son
idénticas. 

Esta simplificación, aunque útil, no es fisiológicamente correcta porque las
neuronas poseen memoria. La ocurrencia de un potencial de acción modifica la
probabilidad de que otros sucedan, y la repetida transmisión de información en
una sinapsis fortalece dicha sinapsis, facilitando futuras transmisiones. Este
fenómeno se conoce como plasticidad sináptica.  

\pagebreak

\subsection{Neuroelectrónica}

Las neuronas son células hermosas que viven sumergidas en un fluido
extracelular, rodeadas de células gliales (materia blanca).El

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{RyC.png} % Prueba quitando la extensión .png
\end{figure}

Un potencial de acción es una inversión rápida y drástica del potencial de
membrana que permite la transmisión de señales a lo largo de la neurona. Este
proceso depende de la conductancia iónica selectiva, regulada por canales de
proteínas que son sensibles al voltaje. En estado de reposo, la membrana es
principalmente permeable al potasio ($K^+$), pero ante un estímulo que
despolariza la célula hasta un umbral crítico, se abren masivamente los canales
de sodio ($Na^+$) dependientes de voltaje. Esto permite que el sodio fluya hacia
el interior a favor de su gradiente electroquímico, invirtiendo la polaridad de
la membrana. Posteriormente, el cierre de estos canales y la apertura de canales
de potasio adicionales restauran el equilibrio negativo original. Este ciclo de
intercambio iónico transforma la energía química almacenada en los gradientes de
concentración en impulsos eléctricos que se propagan de forma autosostenida.

Por convención, el potencial del líquido extracelular en el exterior de una
neurona es cero. Dado que las neuronas inactivas poseen un exceso de iones
negativos en el interior de sus membranas, el potencial de reposo de una neurona
es negativo. Por lo tanto, cada neurona tiene un potencial de equilibrio
determinado por las concentraciones iónicas en su interior. El potencial de las
neuronas puede variar aproximadamente entre $-90\text{ mV}$ y $+60\text{ mV}$,
dependiendo del tipo de neurona y de su estado.

El exceso de carga negativa en el interior hace que los iones negativos sean
atraídos hacia la cara interior de la membrana; y como dichos iones se repelen
entre sí, su distribución tiende a ser uniforme. Es decir que la neurona
funciona como un capacitor con una carga uniformemente distribuida a lo largo de
su membrana. Todo capacitor obedece que $V = \frac{Q}{C}$, o bien $VC = Q$. Si
modelamos $V = V(t)$, y tomando $C$ como constante, se tiene 

\begin{equation}
    C \frac{dV}{dt} = \frac{dQ}{dt} =: -I
\end{equation}

El signo negativo en $-I$ obedece a una convención típica: la corriente de
membrana se define como positiva cuando iones positivos salen de la neurona, y
negativa cuando iones positivos entran a la neurona. El ingreso de una unidad de
carga positiva es equivalente a la salida de una unidad de carga negativa, y por
ende definir el sentido de la corriente en términos de los iones positivos
alcanza para definirlo para todo tipo de iones, positivos y negativos.

Generalmente, nos interesa expresar la capacitancia por unidad de área, es decir
la capacitancia específica $c_m = C / A$. Lo mismo aplica para la resistencia,
definiendo la resistencia específica $r_m = R \cdot A$ y la conductancia
específica $g_m = 1 / r_m$, tanto como para la corriente $i_m = \frac{I}{A}$.
Visto de este modo, la ecuación resulta 

\begin{equation}
    c_m \frac{dV}{dt} = -i_m
\end{equation}

Las áreas (de superficie) de una neurona suelen variar entre $0.01$ y
$0.1\text{mm}^2$, y la capacitancia entre $0.01$ y $0.1\text{nF}$. Una neurona
con una capacitancia de 1 nanofaradaio necesita aproximadamente $7\times
10^{-11}$ coulombs para generar un potencial de reposo de $-70\text{ mV}$. Esto
equivale aproximadamente a $10^9$ iones con una carga unitaria.

El producto entre la capacitancia y la resistencia es una cantidad con unidad de
tiempo llamada constante de tiempo de membrana: $\tau_m = R_m C_m$. Es fácil
observar que $\tau_m = r_m c_m$, es decir es independiente del área. Esta
constante especifica la escala temporal de los cambios en el potencial de
membrana y generalmente varía entre $10$ y $100\text{ ms}$. Yo sé que es un
lugar común, pero la analogía del tanque de agua es útil para entender $\tau_m$.
Si la neurona se piensa como un tanque (capacitor) llenándose a través de una
manguera (resistencia), es obvio que un tanque grande ($C$ alta) con una
manguera estrecha ($R$ alta) tardará mucho en llenarse. La neurona es entonces
"lenta" para responder a estímulos. Si el tanque es pequeño ($C$ baja) y la
manguera ancha ($R$ baja), el proceso sucede "rápido". Esta es la dinámica
capaturada por $\tau_m$.

Es importante notar que el equilibrio de potencial no es un estado estático,
sino consecuencia de que la corriente generada por las fueras eléctricas cancela
el flujo por difusión. Si un ion positivo «flota» en el medio intracelular, y la
membrana tiene un potencial negativo, el potencial de membrana se opondrá el
flujo del ion fuera de la célula. En este caso, sólo habría flujo a través de la
membrana si la energía térmica es suficiente para superar el campo eléctrico. Si
el ion tiene una carga de $zq$, con $q$ la carga de un protón y $z$ un número
real positivo, la energía térmica debe ser de al menos $-zqV$ para que el ion
atraviese la membrana. La probabilidad de que un ion en un medio con temperatura
absoluta $T$ tenga una energía térmica $\geq zqV$ es 

\begin{equation}
    p := P(E_T \geq zqV) = \exp\left( zqV / k_B T \right) = \exp\left( zV / V_T \right) 
\end{equation}

donde $V_T = k_B T / q$ es el voltaje térmico. 

Fuera de la célula, el potencial eléctrico puede ser contrarrestado por un
gradiente. Si la concentración de iones dentro de la célula es menor al a
concentración fuera de la misma, el gradiente electroquímico generado puede
compensar una baja probabilidad $p$.  El flujo hacia dentro de la célula es
proporcional a la concentración en el medio externo. El flujo hacia fuera es
proporcional a la concentración en el medio interno multiplicada por $p$, dado
que sólo los iones con suficiente energía térmica podrán abandonar el medio
celular. El flujo neto será cero cuando ambos flujos sean iguales. Si $E$ denota
la diferencia de potencial que satisface precisamente esta condición, y $C_i,
C_e$ es la concentración interna y externa de iones, se tiene:

\begin{equation}
    C_e = C_i \exp\left( \frac{zE}{V_T} \right) \iff E = \frac{V_T}{z}\ln \left(
    \frac{C_e}{C_i}\right) 
\end{equation}

Esta es la famosa ecuación de Nernst. Los potenciales de equilibrio para el
potasio ($K^+$) típicamente caen entre $-70\text{mV}$ y $-90\text{mV}$; para el
sodio $(\text{Na}^+)$ son $\approx 50\text{mV}$ o más, y para el calcio todavía
mayores, alcanzando $\approx 150\text{mV}$. 

La ecuación de Nernst asume que la conductancia generada por un canal es
específica, i.e. cada canal permite el paso de un único tipo de ion. Algunos
canales no satisfacen esta condición, en cuyo caso el potencial de equilibrio no
es determinado por la ecuación de Nernst sino que toma un valor intermedio entre
los potenciales de equilibrio de cada tipo de ion que atraviesa el canal.

Cada conductancia tiende a llevar el potencial de membrana $V$ a su propio
potencial de equilibrio $E$. Si $V>E$, existe flujo hacia fuera; si $V < E$,
flujo hacia adentro. Esto significa que un canal va a polarizar o despolarizar
la membrana dependiendo de su potencial de equilibrio. Los canales de sodio y el
calcio tienen potenciales de equilibrio positivos y por lo tanto tienden a
despolarizar la membrana. Los canales de potasio tienen valores negativos y
tienden a hiperpolarizarla. Algunas conductancias, como la del cloro
($\text{Cl}^-$), tienen un potencial de equilibrio tan cercano al potencial de
reposo de la membrana que apenas dejan pasar corriente. Como nota, las sinapsis
también tienen potenciales de equilibrio: si dicho potencial es mayor al umbral
de disparo, se dice que la sinapsis es excitatoria; si es menor, que es
inhibitoria.

La corriente total a través de la membrana es la suma de las corrientes a través
de todos sus canales. La dirección de la corriente que fluye a través de la
membrana se define por convención como positiva si los iones abandonan el medio
intracelular. Como distintas neuronas tienen distintos tamaños, es útil hablar
de la corriente por unidad de área, $i_m$. La corriente total sería $i_m A$, con
$A$ la superficie total de la neurona.

Ya establecimois que la corriente neta es nula cuando $V = E_i$, con $E_i$ el
potencial de equilibrio del ion $i$. La diferencia $V - E_i$ determina entonces
la distancia entre el voltaje y el punto de equilibrio, y la corriente por
unidad de área se determina como 

\begin{equation}
    g_i(V - E_i)
\end{equation}

con $g_i$ la conductancia por unidad de área para los canales de tipo $i$. Esta
determinación está justificada porque la corriente aumenta o decrece de manera
aproximadamente lineal con el factor $V - E_i$. Se sigue que 

\begin{equation}
    i_m = \sum_i g_i (V - E_i)
\end{equation}

Es importante notar que este es solo un modelo o aproximación posible de la
corriente, fundamentada en la relación aproximadamente lineal entre la corriente
y el factor $V - E_i$. Otras expresiones, como la fórmula de
Goldman-Hodgkin-Katz, a veces son utilizadas.

Si combinamos esta nueva expresión para $i_m$ con la ecuación que determina la
tasa de cambio del voltaje, obtenemos 

\begin{equation}
    c_m \frac{dV}{dt} = -\sum_i g_i(V - E_i)
\end{equation}

Esta es la expresión detrás de los modelos más elementales de la neurociencia
computacional, como los \textit{single-compartment models}. Estos modelos
describen el potencial de membrana usando una única variable $V$. Insisto en que
el signo negativo es necesario dadas las convenciones que establecimos. Notar
que si el voltaje es menor a un potencial de equilibrio determinado, habrá un
flujo de corriente hacia adentro, y el término de la suma será positivo. Esto es
consistente, porque el ingreso de iones positivos a la célula aumenta el
voltaje.

En otros escritos detallé algunos \textit{single-compartment models}. El más
simple es el que expresa el voltaje ante una corriente externa $I_e$:


\begin{equation}
    c_m \frac{dV}{dt} = - i_m + \frac{I_e}{A}
\end{equation}

El signo positivo de $I_e$ también es mera convención: se define la corriente
externa como positiva cuando ingresa a la neurona, contrario a la corriente de
membrana.

En general, no todos los canales ionicos están abiertos al mismo tiempo. La
conductancia específica de un ion $g_i$ es determinada por la cantidad de
canales abiertos. El producto entre la conductancia de un canal por la densidad
de canales en la membrana se denota $\overline{g_i}$. De este modo, podemos
expresar como la conductancia total como $\overline{g_i} \frac{n_i}{N_i}$, con
$n_i$ la cantidad de canales de tipo $i$ abiertos y $N_i$ la cantidad de canales
en total. Ahora bien, $n_i / N_i$ es equivalente a la probabilidad de tomar una
canal aleatorio y que el mismo esté abierto, denotada por $P_i$. En conclusión,
$g_i = \overline{g_i} P_i$.

En el modelo de Hodgkin-Huxley, dos tipos de canales se utilizaban: transitorios
(\textit{transient}) y persistentes. Las conductancias persistentes dependían de
complejos mecanismos moleculares, que el modelo simplificaba definiendo cada
canal como compuesto de $k$ sub-unidades o \textit{gates}, cada una de las
cuales debe estar abierta para que pueda haber corriente. El estado de las
 \textit{gates} (abierta/cerrada) se modelaba con variables aleatorias
independientes e igualmente distribuidas, de manera tal que si $n$ era la
probabilidad de que una gate esté abierta, $n^k$ era la probabilidad de que un
canal esté abierto. La variable $n$, i.e. la probabilidad de que una sub-unidad
arbitraria de un canal esté abierta, se denominaba \textit{variable de
activación}. 

En la medida en que lidiemos con canales voltaje-dependientes
(\textit{voltage-dependent channels}), las variables de activación son una
función creciente de $V$. En particular, la probabilidad de que una sub-unidad
esté abierta debe ser proporcional a la probabilidad de que la sub-unidad esté
cerrada multiplicada por una tasa de apertura $\alpha_n(V)$. Del mismo modo, la
probabilidad de que una sub-unidad se cierre debe ser proporcional a la
probabilidad de que esté abierta por una tasa de cierra $\beta_n(V)$. En
resumen, se obtenía 

\begin{equation}
    \frac{dn}{dt} = \alpha_n(V) (1 - n) - \beta_n(V) n
\end{equation}

Me voy a ahorrar los detalles, porque ya estudiamos en mi entrada sobre el
modelo de Hodgkin-Huxley, pero esto podía re-escribirse como 

\begin{equation}
    \tau_n(V) \frac{dn}{dt} = n_{\infty}(V) - n
\end{equation}

donde 

\begin{equation}
    t_n(V) = \frac{1}{\alpha_n(V) + \beta_n(V)}, \qquad n_\infty(V) =
    \frac{\alpha_n(V)}{\alpha_n(V) + \beta_n(V)}
\end{equation}

Los elementos clave son $\alpha_n(V), \beta_n(V)$, las tasas de apertura y
cierre dependientes del voltaje. Su derivación es compleja y omitida en esta
entrada.



El modelo de Connor-Stevens, como el de Hodgkin-Huxley, involucra canales de
sodio depolarirzantes,  canales de potasio rectificadores, y conductancias de
fuga (leaky). Sin embargo, posee una conductancia transitoria extra de $K^+$,
llamada corriente $A$. 

\begin{equation}
    i_m = \overline{g}_L(V - E_L) + \overline{g}_{\text{Na}} m^3h (V -
    E_{\text{Na}})
\end{equation}

























\end{document}



