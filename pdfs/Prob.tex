\documentclass[a4paper, 12pt]{article}

\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{amsmath, amssymb}
\usepackage{newtxtext} \usepackage{newtxmath}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example} \newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\usepackage{parskip}

\newcommand\myeq{\stackrel{\mathclap{\normalfont\mbox{def}}}{=}}

\begin{document}

   
\section{P1}

Let us recall that $P : \mathcal{P}(\Omega) \mapsto [0, 1]$ satisfies the following three 
conditions:

\begin{itemize}
    \item $\forall \zeta \in \subseteq \Omega : 0 \leq P(\zeta) \leq 1$
    \item $P(\Omega) = 1$
    \item $\forall \left\{ \zeta \right\}_{I \subseteq \mathbb{N}} : \zeta_i \cap  \zeta_j = \emptyset : P(\bigcup_{i \in I}) \zeta_i = \sum_{i \in I} P(\zeta_i) $
\end{itemize}

In general, I use $\omega$ to denote $|\Omega|$, and we should recall that whenever $P$ is a constant 
map (i.e. when events are equiprobable) we have $\forall A \subseteq \Omega : P(A) = \frac{|A|}{\omega}$.

To keep notation brief, we shall speak of probabilities $P(A)$ without specifying that 
$A \subseteq \Omega$.

We recall the following properties, which follow strictly from the aforementioned 
facts:

\begin{itemize}
    \item $P(\emptyset) = 0$
    \item $P(A^c) = 1 - P(A)$
    \item $A \subseteq B \Rightarrow P(A) \leq P(B)$
    \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
\end{itemize}

\pagebreak

\begin{problem}[Problem 2 of the sheet]
    Prove that $A \subseteq B \subseteq \Omega \Rightarrow P(B- A) = P(B) - P(A)$ and also
    $P(A) \leq P(B)$.
\end{problem}

Let $A \subseteq B \subseteq \Omega$.

\begin{align*}
    B - A &= \left\{ x \in B : x \not\in A \right\}  \\ 
    &=\left\{ x \in \Omega : x \in B \land  x\not\in A \right\} \\
    &= \left\{ x \in \Omega : x \in B \land x \in A^c \right\}  \\ 
    &= B \cap A^c
\end{align*}

Since $P(A \cup B) = P(A) + P(B) - P(A \cap B)$, we have 
$$P(A \cap B) = P(A) + P(B) - P(A \cup B)$$ 

Then 

\begin{align*}
    P(B - A) &= P(B \cap A^c) \\ 
             &=P(B) + P(A^c) - P(B \cup A^c)\\ 
             &=P(B) + \left[ 1 - P(A) \right] - P(B \cup A^c)
\end{align*}

It is easy to see that, since $A \subseteq B$, $B \cup A^c = \Omega$.
We readily obtain


\begin{align*}
    P(B - A) &= P(B) + \left[ 1 - P(A) \right] - P(\Omega) \\ 
    &=P(B) + 1 - P(A) - 1 \\ 
    &= P(B) - P(A)
\end{align*}

\textit{quod erat demonstrandum}. And since $P(B - A) = P(B) - P(A) \geq 0$, we must have 
$P(B) \geq P(A)$.

\pagebreak 

\begin{problem}[3]
    
\end{problem}

 
For ease of mind, let us write here that 

\begin{align*}
    P(A_i) = \begin{cases}
        .22 & i = 1 \\ 
        .25 & i = 2 \\ 
        .28 & i = 3
    \end{cases}
\end{align*}

We are also given 

\begin{equation*}
    P(A_1 \cap A_2) = .11 ~ ~ ~ ~ ~ P(A_1 \cap A_3) = .005 ~ ~ ~ ~ ~ ~ ~ P(A_1 \cap A_3) = .07 ~ ~ ~ ~ ~ ~ P(A_1 \cap A_2 \cap A_3) = .01
\end{equation*}

    (1 : $P(A_1 \cup A_2)$ Observe that $P(A_1 \cap A_2) \neq P(A_1)P(A_2)$,
    which entails the events are not independent. We then have 

    \begin{align*}
        P(A_1 \cup A_2) &= P(A_1) + P(A_2) - P(A_1 \cap A_2) \\ 
                        &= .22 + .25 - .11 \\ 
                        &= .36
    \end{align*}

    (2: $P(A_1^c \cap A_2^c \cap A_3)$) Observe that 

    \begin{align*}
        P(A_1^c \cap A_2^c \cap A_3) &= P( \left[ A_1^c \cap A_2^c \right] \cap A_3^c ) \\ 
                                     &= P(\left[ A_1 \cup A_2 \right]^c \cap A_3^c )\\ 
                                     &=P(( \left[ A_1 \cup A_2 \right] \cup A_3 )^c ) \\ 
                                     &= P(\Omega^c) \\ 
                                     &= P(\emptyset) \\ 
                                     &= 0
    \end{align*}


    (3: $P( (A_1^c \cap A_2^c) \cup A^c )$)

    \pagebreak 

    \begin{problem}[4]
        
    \end{problem}

    Se nos dice que cinco empresas deben firmar contratos de un grupo de $3$ 
    contratos posibles. Se nos dice además que cada empresa firma a lo sumo 
    un contrato. 

    
    \small
    \begin{quote}
    
    Mucho ojo: no se nos dice que cada contrato se da a lo sumo a una empresa, sino que 
    cada empresa firma a lo sumo un contrato. Es decir que varias empresas podrían firmar 
    el mismo contrato.
    \end{quote}
    \normalsize

    Si contamos la opción "no firma ningún contrato", hay $4$ opciones para cada 
    una de las $5$ empresas; es decir, hay $\omega = 4^5$ puntos en el espacio 
    muestral.

    Si cada evento es equiprobable, la probabilidad de que la tercera empresa
    reciba un contrato es 

    \begin{align*} \frac{| \left\{ x \in \Omega : 3\text{era empresa firma
    contrato} \right\}  |}{\omega} \end{align*}

    El numerador puede calcularse restando a $\omega$ la cantidad de casos en que 
    la tercera empresa no firma un contrato. Claramente, la cantidad de tales 
    casos es $4^4$; es decir, hay $4^5 - 4^4 = 768$ casos en que 
    la tercera empresa firma algún contrato. Lo cual nos da una probabilidad 
    de $768 / 4^5 = .75$.

    \textit{Solución alternativa}. Sea $\alpha$ una palabra sobre el alfabeto $\left\{ 0,\ldots,3 \right\} $
    de longitud $5$. ¿Cuántas tales palabras hay? Naturalmente, $4^5 = \omega$.
    Asumamos que $\alpha_3 \neq 0$; es decir que el tercer símbolo de $\alpha$
    es no-nulo. ¿Cuántas palabras así hay? Naturalmente, $4^4 \times 3 = 768$.
    Es decir, $\frac{768}{4^5} = .75$ de las palabras tienen $\alpha_3 \neq 0$,
    lo cual coincide con nuestro resultado anterior.


    \pagebreak 

    \begin{problem}[5]
        
    \end{problem}


    From a set of $25$ buses, $8$ present flaws. $5$ are randomly (in this
    context, uniformly) chosen. We are therefore dealing with equiprobable
    events.  

    There are $\omega = \binom{25}{5}$ possible ways of selecting the $5$ buses. 
    There are $\binom{8}{4}$ possible ways of selecting $4$ out of the $8$
    flawed buses and $\binom{25-8}{1} = \binom{17}{1}$ ways of selecting the 
    remaining bus from the set of non-flawed buses. From this follows that the desired probability is

    \begin{align*}
        \frac{\binom{8}{4} \cdot\binom{17}{1}}{\binom{25}{5}}= \frac{70 \cdot 17}{53130} = .022
    \end{align*}


    With regards to the probability that at least $4$ have flaws, we must take 
    into account the cases where $4$ have flaws and the cases where $5$ have 
    flaws, which are clearly disjoint. The probability then is

    \begin{equation*}
        .022 + \frac{\binom{8}{5}}{53130} = .022 + .001 = .023
    \end{equation*}

    
    \small
    \begin{quote}
    
    \textbf{Note.} This is obviously a problem involving the hypergeometric distribution, 
    closely related to the binomial distribution. But since we have not studied 
    distributions yet, we cannot use this fact.
    
    \end{quote}
    \normalsize
    

    \pagebreak 

    \begin{problem}[6]
        
    \end{problem}

    Let $A, B, C, D, E$ denote the five faculty members. Two papers from a set
    of five are drawn to decide who will be chosen. 

    Observe that the order does not matter; i.e. drawing $A$ and then $B$ is the 
    same than drawing $B$ and then $A$. It follows that 

    \begin{equation*}
        \Omega = \mathcal{P}_2(\left\{ A, \ldots, E \right\} )
    \end{equation*}

    where $\mathcal{P}_i(\zeta) = \left\{ S \in \mathcal{P}(\zeta) : |S| = i \right\} $.
    Naturally, $\omega = 5 \times 4 \cdot \frac{1}{2} = 10$, where we 
    divide by $2$ to exclude equivalent pairs (e.g. $A, B$ and $B, A$).

    
    \small
    \begin{quote}
    
    Alternatively, we could have reasoned that $\omega = \binom{5}{2} = 10$, the number 
    of $2$-element subsets of a $5$-element set.
    
    \end{quote}
    \normalsize

    ($a$) We are asked for the probability of the event $\left\{ A, B \right\} $. It 
    should be obvious that all events $S \in \Omega$ are equiprobable, 
    which entails $P(\left\{ A, B \right\}) = \frac{1}{10} = .1 $.

    
    \small
    \begin{quote}
    
    Alternatively, we could have reasoned the following. There are two ways in
    which $A$ and $B$ may be chosen: $A$ is chosen first and then $B$, or $B$
    is chosen first and then $A$. This gives $\frac{1}{5} \cdot \frac{1}{4} +
    \frac{1}{5} \cdot \frac{1}{4} = \frac{2 \cdot}{5\cdot4} = \frac{2}{20} =
    .1$
    
    \end{quote}
    \normalsize

    ($b$) We are asked for the probability that the selection contains $C$ or
    $D$. It is straightforward to reason that there are $\binom{3}{2} = 3$ sets
    that do not contain neither $C$ nor $D$. From which readily follows that 
    there are $10 - 3 = 7$ sets containing $C$, $D$ or both. $\therefore $
    The desired probability is $\frac{7}{10}$.

    ($c$) Let us change the notation a bit. Let $\left\{ 1,\ldots, 5 \right\} $
    be the professors we used to call $A, \ldots, E$. Let $a_i := \left\{ 3, 6,
    7, 10, 14 \right\} $ be the set of years of teaching of each professor,
    assuming $a_1$ corresponds to $A$, $a_2$ to $B$, etc. We are asked for the
    probability that the selected pair $\left\{ j, k \right\} $ satisfies 
    $a_j + a_k \geq 15$.

    There are two ways to solve this problem: one slow but direct, one 
    pretty but a bit more clever.

    \textit{Direct solution.} It is easy to see that, of all pairs $j, k$, only the following 
    satisfy the requirement:

   
   \small
   \begin{quote}
   
   
    \begin{itemize}
        \item $1, 5 \mapsto a_1 + a_5 = 17$ 
        \item $2, 4 \mapsto a_2 + a_4 = 16$
        \item $2, 5 \mapsto  a_2 + a_5 = 20$.
        \item $3, 4 \mapsto a_3 + a_4 = 17$
        \item $3, 5 \mapsto  a_3 + a_5 = 21$
        \item $4, 5 \mapsto a_4+a_5=24$
    \end{itemize}
   
   \end{quote}
   \normalsize
  
   So only $6$ out of the $10$ possible pairs satisfy the relationship, giving us 
   the desired probability: $\frac{6}{10} = \frac{3}{5} = .6$.

   \textit{Pretty solution.} Draw the $4\times 5$ boolean matrix $\mathcal{A}$
   whose coefficients $\mathcal{A}_{ij}$ are $1$ if $a_i + a_j \geq 15$, $0$
   otherwise. Since upper and lower diagonal entries are equivalent (the matrix
   is symmetric), and because $i \neq j$ in our experiment, the diagonal of the
   matrix should not be considered. This gives the representation

   \begin{align*}
   \begin{bmatrix} 
       ~ & 0 & 0 &0 &1 \\ 
       ~&~&0&1&1 \\ 
       ~&~&~&1&1 \\ 
       ~ &~&~&~&1
   \end{bmatrix} 
   \end{align*}

   where $6$ out of $10$ relevant entries are $1$.


   \pagebreak 

   \begin{problem}[7]
       
   \end{problem}

   Let $M := \left\{ m_1, \ldots, m_4 \right\} $ and $W := \left\{ w_1, \ldots,
   w_4 \right\} $ be alphabets denoting the men and women, respectively. The
   sample space $\Omega$ consists of all permutations in $M \cup W$, which
   readily entails $\omega = 8!$.

   $(a)$ Consider the event $\zeta$ when at least one women $w \in W$ is among the first three elements in the 
   sampled permutation. Then $w$ could be the first, the second or the third element in the permutation, 
   and we impose no condition on the rest of the elements. These events are evidently disjoint. So, 
   if we denote with $e_i$ the event where $w$ is the $i$th element of the permutation, we have 

   \begin{equation*}
       P(\zeta) = \frac{ P(e_1) + P(e_2) + P(e_3) }{8!}
   \end{equation*}

   Each $e_i$ may occur in $7!$ ways, since we fix $w$ at the $i$th element and we must only 
   choose from the remaining $7$ assistants.


   $\therefore $ $P(\zeta) = \frac{ 3 \cdot 7! }{8!} = \frac{3}{8}$

   
   \small
   \begin{quote}
   
   \textbf{Note.} If you are interested in being very formal, this are the rigorous 
   steps taken above.
   
   (1) $\zeta = e_1 \cup e_2 \cup  e_3 \Rightarrow P(\zeta) = P(e_1 \cup e_2 \cup e_3)$.

   (2) $( \forall i, j : e_i \cap e_j = \emptyset ) \Rightarrow P(\zeta) = P(e_1) + P(e_2) + P(e_3)$. 
   
   (3) Since events are equiprobable, $P(e_i) = |e_i| / \omega$. 

   (4) $|e_i| = 7!$ because $e_i$ is a permutation of $7$ elements.

   (5) $P(\zeta) = |e_1|/\omega + |e_2| / \omega + |e_3|/\omega = \frac{3\times 7!}{8}$.
   
   \end{quote}
   \normalsize
   


   $(b)$ Let $\varrho$ denote the event where, after the first five meetings,
   all female assistants have been met. Let $(p_1,\ldots, p_8) \in \varrho$ be an arbitrary
   permutation, and denote it with $\overrightarrow{p}$. (Remember that an event is a subset of the sample space, and
   hence a set, so the expresssion $\overrightarrow{p} \in \varrho$ is well defined.)


   The definition of $\varrho$ entails that one and only one $m_j$ exists in $p_1, \ldots, p_5$,
   since all $w_1, \ldots, w_4$ must lie in this sequence. So the number of 
   ways in which we may construct $\overrightarrow{p}$ (i.e. the cardinality 
   of $\varrho$) is readily determined by the number of ways in which we can 
   place exactly one $m_j$ among the first elements of $\overrightarrow{p}$.

   There are $5$ positions to place $m_j$, and $4$ elements in $M$ to choose
   from. Assuming $m_j$ was placed at the $k$th position, we know there are
   $4!$ ways of placing the $4$ women among the remaining positions in $p_1,
   \ldots, p_5$. So there are $5 \times 4 \times 4!$ ways to construct $p_1,
   \ldots, p_5$ for $\overrightarrow{p} \in \varrho$. 

   The positions $p_6, p_7, p_8$ must be chosen from the remaining 
   $3$ men, so there are $3 \times 2$ possibilities. 

   $\therefore $ $|\varrho| = 5\times 4 \times 4! \times 3 = 60 \times 4!$.

   $\therefore $ $P(\varrho) =  \frac{ |\varrho| }{\omega} = \frac{60 \times 4!}{8!} = .036$
  
   \pagebreak

   \section{Conditional probability} 

   \begin{problem}[8]
       A box contains $6$ red balls and $4$ green balls. A second box contains 
       $7$ red balls and $3$ green balls. A ball is randomly chosen from 
       the first box and placed into the second. Then a ball is drawn 
       from the second box and placed into the first.
   \end{problem}


   (1) Let $R_i$ denote the event of choosing a red ball in the $i$th draw, and
   $G_i$ the event of choosing a green ball in the $i$th draw. Evidently,
   $P(R_1) = \frac{6}{10}$. 

   If the event $R_1$ occurs, when the second draw is made, the second box 
   contains $8$ red balls and $3$ green balls, entailing that 
   $P(R_2 \mid  R_1) = \frac{8}{11} $.

   So, using the conditional probability formula, the event when both balls are
   red, $R_1 \cap R_2$, has probability $P(R_1 \cap R_2) = P(R_1) \cdot P(R_2
   \mid R_1) = \frac{6}{10} \cdot \frac{8}{11} = .436$.

   (2) We now inquire the probability that the number of red and green balls in the 
   first box are the same at the beginning and the end of the experiment. Naturally, 
   this entails either drawing both times a red or both times a green ball. We have 
   already computed the probability of drawing both times a red ball. The probability 
   of drawing both times a green ball is similarly computed: 

   \begin{align*}
       P(G_1 \cap G_2) &= P(G_1) \cdot P(G_2 \mid G_1) \\ 
       &= \frac{4}{10} \cdot \frac{4}{11} \\ 
       &= .145
   \end{align*}


   Since $R_1 \cap R_2$ and $G_1 \cap G_2$ are obviously disjoint, the probability that 
   either of them occurs is simply $.436 + .145 = .581$, the sum of the probabilities 
   of both events.

   \pagebreak 


   \begin{problem}[10]
       Given events $A, B$ with $P(B) > 0$, prove $P(A \mid B) + P(\overline{A} \mid B) = 1$.
   \end{problem}

   ($a$) We know 

   \begin{equation*}
       P(A \mid B) = \frac{P(A \cap B)}{P(B)}, ~ ~ ~ P(\overline{A} \mid B) = \frac{P(\overline{A} \cap B)}{P(B)}
   \end{equation*}

   It follows 

   \begin{align*}
       P(A \mid B) + P(\overline{A} \mid B) &= \frac{P(A \cap B) + P(\overline{A} \cap B)}{P(B)}  \\ 
                                            &= \frac{P\left((A \cap B) \cup (\overline{A} \cap B)\right)}{P(B)} &\left\{ A \cap B \text{  and } \overline{A} \cap B \text{ are disjoint}\right\} \\ 
                                            &= \frac{ P\left( ( A \cup \overline{A} ) \cap B \right)    }{P(B)} \\ 
                                            &= \frac{ P(\Omega \cap B) }{P(B)} \\ 
                                            &= \frac{P(B)}{P(B)} \\ 
                                            &= 1
   \end{align*}

   \textit{quod erat demonstrandum}.

   $(b)$ Assume $P(B \mid A) > P(B)$. We want to prove $P(\overline{B} | A) <
   P(\overline{B})$. 

   By assumption,

   \begin{align*}
       \frac{P(A \cap B)}{P(A)} > P(B) &\Rightarrow P(A \cap B) > P(B)P(A)
   \end{align*}


   We want to prove

   \begin{align*}
       \frac{P(\overline{B} \cap A)}{P(A)} < P(\overline{B}) \equiv P(\overline{B} \cap A) < P(\overline{B})P(A)
   \end{align*}

   \pagebreak 

   \begin{problem}[]
       One every $25$ adults have a disease. Let $s$ be a subject. If $s$ has the disease, the diagnostic 
       test is positive $.99$ of the times. If $s$ does not have the disease, the diagnostic test 
       is positive $.02$ of the times.
   \end{problem}

   ($a$) We are asked to find the probability of a result being positive. The law of total 
   probability readily states that

   \begin{align*}
       P(\text{positive}) &= P(\text{positive} | \text{enfermo})P(\text{enfermo}) + P(\text{positive} | \text{sano})P(\text{sano})  \\ 
       &= .99 \cdot \frac{1}{25} + .02 \cdot \frac{24}{25} \\ 
       &= .395 + .0192 \\ 
       &= .058
   \end{align*}

   (b) We are requested to find $P(\text{enfermo} | \text{positivo})$. Observe that we know the 
   "reverse" of this; i.e. $P(\text{positivo} | \text{enfermo})$. This type of 
   scenario calls for Bayes theorem, which states 

   \begin{align*}
       P(\text{enfermo}|\text{positivo}) &= \frac{P(\text{positivo}| \text{enfermo})P(\text{enfermo})}{P(\text{positivo})} \\ 
                                         &= \frac{.99 \cdot \frac{1}{25}}{.058} \\ 
                                         &=.682
   \end{align*}

   (c) Similarly, 

   \begin{align*}
       P(\text{sano} | \text{negativo}) &= \frac{P(\text{negativo} | \text{sano}) P(\text{sano})}{P(\text{negativo})} \\ 
                                        &= \frac{.98 \cdot \frac{24}{25}}{1-.058} \\ 
                                        &=.998
   \end{align*}


   \pagebreak 

   \begin{problem}[12]
       
   \end{problem}

   Recall that there are two equivalent definitions of independence. Two events 
   $\varphi, \psi$ are independent if $P(\varphi \cap \psi) = P(\varphi)P(\psi)$,
   or equivalently if $P(\varphi \mid \psi) = P(\varphi)$. We must use both 
   definitions in order to prove the properties.

   $(a)$ We are required to prove $P(\overline{A} \cap B) = P(\overline{A})P(B)$. Observe that 

   \begin{align*}
       P(\overline{A} \cap B) = P(B) - P(A \cap B)
   \end{align*}

   (This follows from the fact that $B = (A \cap B) \cup (\overline{A} \cap B)$ and the fact that 
   the events in the union are obviously disjoint.) Then, due to the independence of 
   $A$ and $B$,

   \begin{align*}
       P(\overline{A} \cap B) &= P(B) - P(A)P(B)\\ 
                              &= P(B)\left[ 1 - P(A) \right]  \\ 
                              &=P(B)P( \overline{A} )
   \end{align*}

  \textit{quod erat demonstrandum}.

  $(c)$ By DeMorgan's law, $(\overline{A} \cap \overline{B}) = \overline{(A \cup B)}$. Then 

  \begin{align*}
      P(\overline{A} \cap \overline{B}) &= 1 - P(A \cup B) \\
                                        &=1 - \left[ P(A) + P(B) - P(A \cap B) \right]  \\ 
                                        &=1 - \left[ P(A) + P(B) - P(A)P(B) \right] \\ 
                                        &=1 - P(A) - P(B) + P(A)P(B) \\ 
                                        &=P(\overline{A}) - P(B) + P(A)P(B) \\ 
                                        &=P(\overline{A}) - P(B)\left[1 - P(A) \right]  \\ 
                                        &=P(\overline{A}) - P(B)P(\overline{A}) \\ 
                                        &=P(\overline{A})(1 -P(B)) \\ 
                                        &=P(\overline{A})P(\overline{B})
  \end{align*}

  \pagebreak 

  \begin{problem}[13]
      A collection $\chi$ of 10 items has 2 satisfying the predicate $\varphi : \chi \mapsto \left\{ 0, 1 \right\} $.  
      Two random samples $x_1, x_2$ are taken from $\chi$. Let $A = \left\{ \varphi(x_1) \right\} $, $B = \left\{ \varphi(x_2) \right\} $.
      Compute $P(A), P(B), P(A \cap B)$. Are $A$ and $B$ independent?
  \end{problem}

  Obviously, $P(A) = \frac{2}{10} = \frac{1}{5}$ and 

  \begin{align*}
      P(B) &= P(B \mid A)P(A) + P(B \mid \overline{A})P(\overline{A}) \\ 
           &= \frac{1}{9} \frac{2}{10} + \frac{2}{9} \frac{8}{10} \\ 
           &=.\frac{2}{90} + \frac{16}{90} \\ 
           &=\frac{18}{90} \\ 
           &= \frac{2}{10} \\ 
           &= \frac{1}{5}
  \end{align*}

  We know 

  \begin{equation*}
      P(A \cap B) = P(B \mid A)P(A) = \frac{1}{9} \frac{2}{10} = \frac{1}{45}
  \end{equation*}

  Then $P(A)P(B) = \frac{1}{5} \cdot \frac{1}{5} \neq P(A \cap B)$. The events are not independent.

\pagebreak 

\begin{problem}[14]
    From a deck of 52 spanish cards, 4 players $p_1, \ldots, p_4$ receive $13$ cards each.
\end{problem}

$(a)$ Let $w, x, y, z$ be the four types of cards. The probability of $p_1$ receiving 
the 13 cards of type $w$ is $1 / \binom{52}{13}$. The same logic gives that the probability of 
all of them receiving the corresponding hands is 

\begin{equation*}
    P(\zeta) = \prod_{i=0}^{3} \frac{1}{\binom{52 - 13i}{13}}
\end{equation*}

where $\zeta$ denotes the corresponding event.

$(b)$ If a player has all cards of same type, it has all cards of that type
(13). There are $4!$ ways of distributing the types among the players. Then the
probability of this event is $4! P(\zeta)$.

$(c)$ There are $4$ aces: $ \mathcal{A} = x_{13}, y_{13}, w_{13}, z_{13}$. There are 
$\binom{48}{13}$ hands without these elements. $\therefore $ The probability 
is $1 / \binom{48}{13}$.

$(d)$ For each player to receive an element in $\mathcal{A}$, each must get exactly 
one such element.

The probability of $A$ receiving exactly one $A$ is $\frac{4}{\binom{51}{12}}$, where 
$4$ accounts for which element it receives and $\binom{51}{12}$ is the number of 
hands containing that fixed ace. Similar logic gives that the probability of all 
players reciving exactly one ace is 

\begin{align*}
    4! \prod_{i=0}^{3} \frac{1}{\binom{51 - 12i}{12}}
\end{align*}


\pagebreak 

\section{P2}

(0.a) Assuming any number in $\mathbb{N}_{100}$ has the same probability of being chosen on a list 
$(a_1, \ldots, a_5)$, we have

\begin{equation*}
    P(\overrightarrow{x} \in \Omega) = \frac{1}{|\Omega|} = \frac{1}{100^5}
\end{equation*}

(0.b) A random variable associated to this experiment might be the sum of the elements 
in the list: $X(\overrightarrow{a}) = \sum a_i$.

~ ~ ~ 

(1.a) A probability mass function must satisfy $\sum p(x_i) = 1$. Only the second given 
function satisfies this.

(1.b) 

\begin{align*}
    P(2 \leq X \leq 4) = P(X = 2) + P(X = 3) + P(X=4) = .1 + .1 + .3 = .5
\end{align*}

(1.c) The cumulative distribution function of $X$ describes $P(X \leq x)$  for every $x$
in the range of the random variable. Thus, we have

\begin{align*}
    P(X \leq x) = \begin{cases}
        0 & X < 0 \\ 
        .4 & 0 \leq X < 1 \\ 
        .5 & 1 \leq X < 2 \\ 
        .6 & 2 \leq X < 3 \\ 
        .7 & 3 \leq X < 4\\ 
        1 & 4 \leq X
    \end{cases}
\end{align*}

(1.d) Asssume $P(x) = k(5 - x)$ for $x = 0,\ldots, 4$. We need 

\begin{align*}
    k \sum_{x=0}^{5}(5 - x) = 1 \iff k \left[ 5^2 + 5 \cdot 4 + \ldots + 5 \right] = 1
\end{align*}

In other words, we need $k \left[ 5 + 4 + 3 + 2 +1 \right] = k\left[ 15 \right] = 1$, 
i.e. we need $k = \frac{1}{15}$.


(2.a) The probability that at most three lines are in use is 

\begin{align*}
    \sum_{x = 0}^{3} p(x) = .1 + .15 + .2 + .25 = .7
\end{align*}

(2.e) Let us study the event where 2, 3 or 4 lines are not being used.
Two lines not being used equates to using $\leq 4$ lines. Three lines 
not being used equates to $\leq 3$ lines not being used. 4 lines not 
being used equates to $\leq 2$ lines being used. In other words, 
the event is

\begin{equation*}
    E = (X \leq 4) \cup (X \leq 3) \cup (X \leq 2) = (X \leq 4)
\end{equation*}

The probability is then $\sum_{x \leq 4} p(x) = .9$

(2.f) For at least 4 lines not being used we need to four lines not being used,
or 5 not being used, or 6 not being used. This means using 2, 1 or 0 lines. 
The probablity is then $.45$.

(3.a) Let us recall that for any $x_j \in Im(X)$

\begin{align*}
    F(x_i) = \sum_{i=1}^{j} p_X(x_i)
\end{align*}

From this readily follows that 

\begin{align*}
    p_X(x_j) &= F(x_j) - \sum_{i=1}^{j-1} p_X(x_i) \\ 
             &=F(x_j) - F(x_{j - 1})
\end{align*}

So we have a nice formula to derive $p_X(x_j)$ given the CDF. In this case, we have

\begin{align*}
    p_X(x) = \begin{cases}
        .3 & x = 1 \\ 
        .1 & x = 3 \\ 
        .05 & x = 4 \\ 
        .15 & x = 6 \\ 
        .4 & x = 12
    \end{cases}
\end{align*}

It is easy to verify that $\sum_{x \in Im(X)} p_X(x) = .3+.1+.05+.15+.4 = 1$.

(3.b) 

\begin{align*}
    P(3 \leq X \leq 6) &= F(6) - F(3) \\ 
                       &= .6 - .4 \\ 
                       &= .2 \\ 
    P(X \geq 4) &= 1 - P(X < 4) \\ 
                &=1 - F(4) \\ 
                &=1 - .45 \\ 
                &= .55
\end{align*}

(4) Five persons $S = \left\{ s_1, \ldots, s_5 \right\} $. Only $s_1, s_2$ have property $R$.
Samples are drawn from $S$ randomly; on each draw property 
$R$ is verified. Let $X$ be the number of verifications made 
until a sample satisfying $R$ is drawn.

(4.a) Evidently, $\Omega = \left\{ T, NT_1, NT_2 NNT_1, NNT_2, NNNT_1,
NNNT_2\right\} $, where $N$ denotes a negative draw and $T_i$ a positive test
comming from drawing $s_1$ or $s_2$. This model gives $X : \Omega \mapsto
\mathbb{R} \right\} $ defined as $X(\omega) = |\omega|_N$ the number of $N$s in
$\omega$.

The events are clearly not independent, since once a non-positive draw is made,
the probability of obtaining a positive draw increases. Let us observe

\begin{align*}
    p_X(0) &= P(T) = \frac{2}{5} \\ 
    p_X(1) &= P(NT_1 \cup NT_2) = \frac{3}{5} \cdot \frac{2}{4} = \frac{3}{10} \\ 
    p_X(2) &= P(NNT_1 \cup NNT_2) = \frac{3}{5} \cdot \frac{2}{4} \cdot \frac{2}{3} = \frac{1}{5} \\ 
    p_X(3) &= \frac{3}{5} \cdot \frac{2}{4} \cdot \frac{1}{3} = \frac{1}{10}
\end{align*}

It is easy to verify that that $\sum p_X(X) =  1$. The probability that $R$ is
not true in the first two draws is simply $p_X(2) + p_X(3) = \frac{3}{10}$.

(5.a) Let $X$ denote the number of points traversed. To find $p_X$ we need
only examine the following. First of all, at least one point is traversed,
which entails $Im_X = \mathbb{N}$. Now, the probability that only one
point is traversed is simply $\frac{1}{3}$. The probability that two
points are traversed is $\frac{2}{3} \cdot \frac{1}{3}$. That of three
points being traversed is $\frac{2}{3} \cdot \frac{2}{3} \cdot
\frac{1}{3}$. In general, 

\begin{align*}
    p_X(x) = \left( \frac{2}{3} \right)^{x - 1} \frac{1}{3}
    = \frac{2^{x-1}}{3^x}
\end{align*}

Observe that the sum of this p. mass function is a geometric series and

\begin{align*}
    \frac{1}{3} \sum_{x = 1}^{\infty} \left( \frac{2}{3} \right) ^{x-1}
    = \frac{1}{1-\frac{2}{3}} = \left( \frac{1}{3} \right)  3 = 1
\end{align*}

which is what we expect.

The CDF is 

\begin{align*}
    F(n) &= \sum_{x=1}^{n} p_X(x) \\  
    &= \sum_{x=1}^{n} \frac{2^{x-1}}{3^x} \\ 
\end{align*}

\pagebreak 

Suppose that only 20\% of motorists come to a complete stop at an intersection
with a flashing red light in all directions when there are no other visible
vehicles.

\begin{enumerate}
    \item What is the probability that, out of 20 randomly selected motorists arriving at the intersection under these conditions:
    \begin{enumerate}
        \item At most 5 will come to a complete stop?
        \item Exactly 5 will come to a complete stop?
        \item At least 5 will come to a complete stop?
    \end{enumerate}
    \item How many of the next 20 motorists would you expect to come to a complete stop?
\end{enumerate}

The variable $X =$ number of selected motorists that stop follows a binomial
distribution $\mathcal{B}(p = .2, n = 20)$, where we consider a success the
event where the motorist stops. Recall that 

\begin{align*}
    p_X(X = k) = \binom{n}{k}p^k(1-p)^{n - k}
\end{align*}

which is perhaps the most intuitive distribution. From this of course follows that 

\begin{align*}
    P(X \leq 5) &= \sum_{k=1}^{5} p_X(k) \\ 
                &=\binom{20}{0}(1-p)^{0.8} + \binom{20}{1}0.2(0.8)^{19} + \ldots + \binom{20}{5}0.2^5(0.8)^{15} \\ 
                &= .80421
\end{align*}

The probability that at least $5$ success occur is simply $1 - P(X \leq 4)$,
which can be derived from the formula above. The expected value of a binomial
distribution is $np = 0.2 \cdot 20 = 4$.

\pagebreak 


A particular type of tennis racket is manufactured in medium and extra-large
sizes. 60\% of all customers at a certain store look for the extra-large size.

\begin{enumerate}
    \item Among 10 randomly selected customers who want this type of racket, what is the probability that at least 6 will look for the extra-large size?
    \item Among 10 randomly selected customers who want this type of racket, what is the probability that the number of customers looking for the extra-large size is within one standard deviation of the mean?
    \item The store currently has 6 rackets of each model. What is the probability that the next ten customers looking for this racket will be able to purchase the model they want from the current stock?
\end{enumerate}

(1) We have again a binomial distribution with $n = 10, p = \frac{6}{10}
= \frac{3}{5}$. The probability comes directly from the binomial distribution
formula.

(2) This problem is more interesting. Recall that the expected value of
a binomial random var. is $np$, or in this case $0.6\cdot 10 = 6$. The variance
is $np(1-p) = 6(0.4) = 2.4$, which entails the standard deviation is
$\sqrt{2.4} \approx 1.55 $. Hence, we are asked for the probability that the
number of success falls in the range $6 \pm 1.55 = [4.5, 7.5]$.
Flooring and ceiling this interval, the question simply becomes 
what is the probability of $X$ falling in $\left\{ 4, 5, 6, 7 \right\} $, 
which  

\begin{align*}
    P(X \leq 7) - P(X \leq 3) = \sum_{k=4}^{7} \binom{10}{k}0.6^k(0.4)^{10 - k}
\end{align*}

both easily derivable from the binomial distribution formula.

\pagebreak 

\section{P3}

\begin{problem}
    Let $X$ a r.v. with density function $f(x) = \frac{x}{2}$ whenever $0 \leq
    x \leq 2$, and zero otherwise. (1) Calculate $P(X\leq 1), P(\frac{1}{2}
    \leq X \leq \frac{3}{2}), P(\frac{3}{2} \leq X)$. Then (2) find the 
    CDF of $X$ and (3) the expected value, variance and standard deviation 
    of $X$. (4) If we define $h(X) = X^2$, what is the expected value of 
    $Y  = h(X)$?
\end{problem}

(1) First of all, observe that $F(x) = \frac{1}{2}\int x dx = \frac{x^2}{4}$.
Incidentally, this is the CDF of $X$, which was requested in point (2). 
We then have

\begin{align*}
    P(X\leq 1) &= \int_{0}^{1} f(x) ~ dx \\ 
               &= \frac{ 1^2  }{4}- \frac{ 0^2 }{4} \\ 
               &=\frac{1}{4} \\ 
    P(\frac{1}{2} \leq X \leq \frac{3}{2}) &= F(\frac{3}{2}) - F(\frac{1}{2}) \\ 
                                           &=\left( \frac{9}{16} \right) - \left( \frac{1}{16} \right)  \\ 
                                           &= \frac{8}{16} \\ 
                                           &= \frac{1}{2}
\end{align*}

(3) The expected value is 

\begin{equation*}
    \int_{\mathbb{R}} f(x) x ~ dx &= \int_{0}^{2} \frac{x^2}{2} = \left[ \frac{x^3}{6} \right]_0^{2}  = \frac{8}{6} = \frac{4}{3} \approx 1.33
\end{equation*}

The simplest way to calculate the variance is to recall that $\mathbb{V}\left[ X \right] = \mathbb{E}\left[ X^2 \right] - \mathbb{E}\left[ X \right]^2   $. Then, we observe that 

\begin{align*}
    \mathbb{E}\left[ X^2 \right] &= \frac{1}{2} \int_0^2 x^3 ~ dx \\ 
                                 &= \frac{1}{2} \left[ \frac{x^4}{4} \right]_0^2 \\ 
                                 &= \frac{1}{2} \left( \frac{16}{4} - 0 \right)  \\ 
                                 &=\frac{4}{2} \\ 
                                 &= 2
\end{align*}

It follows that $\mathbb{V}\left[ X \right] = 2 - (\frac{4}{3})^2 = 2 -
\frac{16}{9} = \frac{2}{9}$.

(4) The expected value of $Y = h(X)$ is 

\begin{equation*}
    \int_0^2 h(x) f(x) ~ dx = \frac{1}{2} \int_0^2 x^3~ dx = \mathbb{E}\left[ X^2 \right] = 2
\end{equation*}

\pagebreak 

Let 

\begin{equation*}
    f_X(x) = \begin{cases}
        kx^2 & 0 \leq x \leq 2 \\ 
        0 & \text{otherwise}
    \end{cases}
\end{equation*}

the PDF of r.v. $X$. The value of $k$ must be s.t. it satisfies 

\begin{align*}
    \int_{\mathbb{R}} f(x) ~ dx = 1
\end{align*}

Since $\int f(x) = \frac{kx^3}{3}$ at the only interval of interest (i.e. $[0, 2]$), we know that $k$
must satisfy 

\begin{equation*}
    \left[ kx^3 / 3 \right]_{0}^{2} = 1 \iff \frac{k8}{3} = 1 \iff k = \frac{3}{8}
\end{equation*}

The CDF of $X$ is precisely the function we have derived above; i.e. $(kx^3) / 3$. With the value 
of $k$ found, we know 

\begin{equation*}
    F_X(x) = \frac{ x^3 }{8}
\end{equation*}

The $.75$ percentile of the distribution is the value $\varphi \in \mathbb{R}$ s.t. 
$F_X(\varphi) = .75$. Therefore, $\varphi$ satisfies

\begin{align*}
    \frac{\varphi^3}{8} = .75 \iff \varphi^3 = 6 \iff \varphi = \sqrt[3]{6} \approx 1.81
\end{align*}


The expected value of $X$ is 

\begin{align*}
    \mathbb{E}\left[ X \right] ~&\myeq \int_{\mathbb{R}} x f(x) ~ dx \\ 
                                &= \frac{3}{8}\int_0^2  x^3 ~ dx \\ 
                                &=\frac{3}{8} \left[ \frac{x^4}{4} \right]_0^2  \\ 
                                &=\frac{3}{2} 
\end{align*}

To compute the standard deviation, we first observe that 

\begin{align*}
    \mathbb{E}\left[ X^2 \right] &= \frac{3}{8}\int_0^2 x^4 ~dx\\ 
                                 &= \frac{3}{8} \left[ \frac{x^5}{5} \right]_0^2 \\ 
                                 &=\frac{3}{8} \frac{32}{5} \\ 
                                 &= 2.4
\end{align*}

Then we can compute $\mathbb{V}\left[ X \right] = \mathbb{E}\left[ X^2 \right]
- \mathbb{E}\left[ X \right]^2 = 2.4 - 2.25 = .15  $. From which follows that
$\sigma_X = \sqrt{.15} \approx .387 $. 

The probability that $X$ is less than one standard deviation from its mean is
the probability that $X$ falls in the range $[\frac{3}{2}-.387, \frac{3}{2} +
.387]$. If we let $l, u$ denote the lower and upper bounds of this interval,
the probability is simply given by $F_X(u) - F_X(l)$.

\pagebreak 

Let $X \sim \mathcal{U}\left( 25, 35 \right) $. The density function of $X$
is then given by 

\begin{equation*}
    f_X(x) = \begin{cases}
        \frac{1}{10} & 25 \leq x \leq 35 \\ 
        0 & \text{otherwise}
    \end{cases}
\end{equation*}

The CDF is then 

\begin{equation*}
    F_X(x) = \int_{25}^{35} f(x) ~ dx = \frac{x-25}{10}
\end{equation*}

We can verify that $\int_{\mathbb{R}} f(x) ~ dx = F_X(35) - F_X(25) = 1 - 0  =
1$. The probability of $X \geq 33$ is simply $1 - P(X \leq 33) = 1 - F_X(33) =
1- \frac{8}{10} = \frac{1}{5}$.

Given $a \in \mathbb{R}$ s.t. $25 < a < a+2 < 35$ the probability of $X$
falling in $[a, a+2]$ is 

\begin{align*}
    \int_a^{a+2} f(x) ~ dx &= F(a + 2 ) - F(a) \\ 
                           &= \frac{a+2 - 25}{10} - \frac{a - 25}{10} \\ 
                           &=\frac{1}{5}
\end{align*}

The expected value of $X$ is 

\begin{align*}
    \mathbb{E}\left[ X \right] &= \int_{\mathbb{R}} x f(x) ~ dx \\ 
                               &= \int_{25}^{35} \frac{x}{10} ~ dx \\ 
                               &= \frac{1}{10} \left[ \frac{x^2}{2} \right]_{25}^{35}  \\ 
                               &=\frac{1}{10}\left( \frac{35^2}{2} - \frac{25^2}{2} \right)  \\ 
                               &= \frac{300}{10} \\ 
                               &= 30
\end{align*}

We could have also used the simple formula for the uniform distribution, which
states that $\mu = \frac{1}{2}(a+b)$. Now we observe that 

\begin{align*}
    \mathbb{E}\left[ X^2 \right] &= \frac{1}{10} \int_{25}^{35}x^2 ~ dx \\ 
                               &= \frac{1}{10} \left[ \frac{x^3}{3} \right]_{25}^{35}  \\ 
                               &=\frac{1}{10}\left( \frac{35^3}{3} - \frac{25^3}{3} \right)  \\ 
                               &= \frac{2725}{3} \\ 
                               &\approx 908.3
\end{align*}

From which follows that $\mathbb{V}\left[ X \right] = 908.3 - 30^2 = 8.3$. $\therefore ~ \sigma = \sqrt{8.3} $.


\pagebreak 

\begin{problem}
    Two teams, $A$ and $ B$, compete in a game. The probability that 
    $A$ wins is $P(A) = .5$. They will continue competing until 
    $A$ wins two games.
\end{problem}

(1) The probability of $B$ winning $x$ games is $.5^x$. 

(2) Let $N$ be the random variable representing number of games played. Since
games are played until $A$ wins two games, the probability of $N = 4$ is the
probability that $A$ wins two out of four games. In particular, $A$ must win a
game before the fourth and then the fourth. 

Since the probabilities that $A$ and $B$ win are equiprobable, the probability
is $\frac{1}{2}^{4} + \frac{1}{2}^4 + \frac{1}{2}^4 = \frac{3}{16}$.

(3) The probability that \textit{at most} four games are played is
$\sum_{i=1}^{4} P(N = i)$. Naturally, $P(N=1) = 0$, since two games must be
played. $P(N = 2) = \frac{1}{4}$, of course. And $P(N = 3) = 2 (\frac{1}{2})^3
= 1/4$. So we have $P(N \leq 4) = 0 + \frac{1}{4} + \frac{1}{4} +
\frac{3}{16}$, which resolves to $\frac{11}{16}$.


\pagebreak 

\textbf{Ej. 6.}  Let $\mu=8.8, \sigma=2.8$. $(a)$ The probability of $X \sim (\mu, \sigma^2)$ being 
at most $10$ is given by 

\begin{align*}
    P(X \leq 10) = \Phi( \frac{10 - \mu}{\sigma}) &= \Phi(1.2 / 2.8) \\ 
                                   &= \Phi(0.428) \\ 
                                   &\approx \Phi(0.43) \\ 
                                   &\approx .666
\end{align*}

So quite curiously the probability approximates a nice fraction: $\frac{2}{3}$.

$(b)$ The probability of $X$ being between $5$ and $10$ is simply 

\begin{equation*}
    P(X \leq 10) - P(X \leq 5) = \frac{2}{3} - \Phi(-1.357) = .666 - .085 = .581
\end{equation*}

where the second probability is computed easily with the table.

($d$) The process of drawing $5$ samples of $X$, $\overrightarrow{x} = (x_1,
\ldots, x_5)$ randomly, where we consider the case $x_i \in [5, 10]$ a success,
is a binomial process with $n = 5$, $p = P(X \in [5, 10]) = .581$.
If we let $Y$ denote the number of successes, then

\begin{align*}
    P(Y = 1) &= \binom{5}{1} \cdot .581 \cdot (1 - .581)^{4} \\ 
    &= 5 \cdot .581 \cdot .419^4 \\ 
    &= .089
\end{align*}

\pagebreak 

\textbf{Ej. 7} Let $X \sim \mathcal{N}(\mu, \sigma)$. We know $10\%$ of the
samples of $X$ fall above $10.256$ and $5\%$ fall below $9.671$. 
We are asked to find $\mu, \sigma$. We are essentialyl 
requested to find $\mu, \sigma$ s.t. the following 
equations are both satisfied:

\begin{align*}
    \begin{cases}
        \Phi(\frac{ 10.256 - \mu }{\sigma}) &= .9 \\ 
        \Phi(\frac{9.671 - \mu}{\sigma}) &= .05
    \end{cases}
\end{align*}


(The first equation comes from the fact that $1 - \Phi(\frac{10.256  - \mu}{\sigma}) = .1$).


\small
\begin{quote}

\textbf{NOTE.} There is very little difference between $10.256$ and $9.671$. However,
the difference in probabilities is huge. Furthermore, both probabilities 
fall on the opposite tails of the distribution. This means our result must be
some distribution with \textit{very low variance} and centered somewhere
between $10.256$ and $9.671$. We can use this as sanitiy check.
\end{quote}
\normalsize


This equations entail that we need (approximately)

\begin{align*}
    \begin{cases}
        \frac{10.256 - \mu}{\sigma} &= 1.29 \\
            \frac{9.671 - \mu}{\sigma} &= -1.64
    \end{cases}
\end{align*}

The first equation yields 

\begin{equation*}
    \mu = -1.29\sigma + 10.256
\end{equation*}

Using this in the second equation gives 

\begin{align*}
    9.671 + 1.29\sigma - 10.256 = -1.64\sigma \iff 2.93\sigma = .585
\end{align*}

This gives $\sigma = .199$, which we approximate with $\sigma = .2$. Then 
$\mu = -1.29\cdot .2 + 10.256 = 9.998$, which we aproximate with 
$\mu = 10$.

$\therefore \mu = 10, \sigma = .2$.

\pagebreak 

\textbf{Ej. 10}. We only look at point $(b)$. We are given $X \sim \mathcal{N}(70, 3^2)$.
We are asked for $\varphi \in \mathbb{R}$ s.t. $I = (70 - \varphi, 70 + \varphi)$
covers $95\%$ of all samples.

For the interval $I$ to cover $95\%$ of the distribution, it must 
leave $2.5\%$ unoccupied on each tail. Which means $\varphi$ must 
satisfy

\begin{align*}
    \begin{cases}
        \Phi\Big( \frac{70 - (70 - \varphi)}{3} \Big) &= 0.025 \\
        \Phi\Big(\frac{70- (70 + \varphi)}{3}\Big) &= 0.975
    \end{cases}
\end{align*}


Using the normal distribution table, this entails 

\begin{align*}
    \begin{cases}
        \frac{70 - (70 - \varphi)}{3} \B&= -1.96 \\ 
        \frac{70- (70 + \varphi)}{3}&= 1.96
    \end{cases}
\end{align*}

Both equations give $\varphi = -1.96(3) = -5.88$. Of course, the sign doesn't mean anything here,
since $\varphi$ simply determines the amplitude of the centering around $\mu$.  

$\therefore  ~ \varphi = 5.88.$

\pagebreak 

\section{Exponential distribution}

The exponential distribution is 

\begin{align*}
    f_X(x; \lambda) = \begin{cases}
        \lambda e^{-\lambda}x & x \geq 0 \\ 
        0 & \text{otherwise}
    \end{cases}
\end{align*}

Its CDF (in the proper interval) is $F_X(x; \lambda) = 1 - e^{-\lambda x}$, its mean is $\frac{1}{\lambda}$, and its variance $\frac{1}{\lambda^2}$.

\textbf{Problem 11.} We are given $X \sim \text{Exp}(.01386)$. $(a)$ $P(X \leq 100)$ is 
computed directly from the CDF: 

\begin{equation*}
    F_X(100; .01386) = 1 - e^{-100 \cdot .01386} = 1 - e^{-1.386} = .7499 \approx \frac{3}{4}
\end{equation*}

The probability $P(100 \leq X \leq 200)$ is simply given by $F_X(200) - F_X(100)$, which 
is simple to compute.

\textbf{Problem 12.} A sistem has five componentes connected thus:

\begin{equation*}
    c_1 \to c_2 \to  \ldots \to  c_5
\end{equation*}

If one component fails, the whole system fails. Each component has a duration
$\delta_i \sim \text{Exp}(.01)$, and failures in one component are independent
of failures in others. Let $A_i$ denote the event of the $i$th component 
lasting for at least $t$ hours. Let $X$ the time at which the system fails,
i.e. the least duration among all systems.

$(a)$ The event $\left\{ X \geq t \right\} $ is equivalent to what event 
where $A_1, \ldots, A_5$ appear?


\small
\begin{quote}

    The event $\left\{ X \geq t \right\} $ is equivalent to the event $\bigcap_{i=1}^{5} A_i$, i.e. 
    the event where each component lasts at least $t$ hours.

\end{quote}
\normalsize


$(b)$ Using the independence of events $A_i$, find $P(X \geq t)$ and find $F(t) = P(X \leq t)$, 
as well as the PDF of $X$.


\small
\begin{quote}

Since the events are independent, given what was said in $(a)$, this probability is 

\begin{align*}
    P(X \geq t) = \prod_{i=1}^{5} P(A_i) &= \prod_{i=1}^{5} P(\delta_i \geq t) \\ 
                           &= \prod_{i=1}^{5} \left( 1 - P(\delta_i \leq t) \right) \\
                           &= \Big( 1 - \left( 1 - e^{-t \cdot .01} \right) \Big)^{5} \\ 
                           &= e^{-t \cdot 0.1 \cdot 5}\\
                           &= e^{-t \cdot 0.5}
\end{align*}

Same reasoning gives $F_X(t) = (1 - e^{-t \cdot 0.5})$, which means the PDF is 

\begin{align*}
    f_X(t) = \frac{d}{dt} \left[ 1 - e^{-t \cdot 0.5} \right] = -\frac{d}{du} e^{u} \frac{d}{dt} u
\end{align*}

with $u = -t \cdot 0.5$. Since $\frac{d}{dt} u = -\frac{1}{2}$ we have 

\begin{equation*}
    f_X(t) = -e^u \cdot (- \frac{t}{2}) = \frac{e^{-\frac{t}{2}}}{2}
\end{equation*}

\end{quote}
\normalsize




$(c)$ Assume $n$ components exist, each with duration $\delta \sim \text{Exp}(\lambda)$. What is the distribution of $X$?


By the same token, 

\begin{align*}
    F_X(x) &= \prod^{n}_{i=1} P(\delta_i \geq t) = e^{-t \frac{n}{10}}
\end{align*}

\pagebreak 

\section{Prob. Conjuntas}

\textbf{Problem (1)} Let 

\begin{align*}
    f(x, y) = \begin{cases}
        kxy & 0 \leq x \leq 1 \land  0 \leq y \leq 1 \land  x + y \leq 1 \\ 
        0 &\text{otherwise}
    \end{cases}
\end{align*}

with $k \in \mathbb{R}^{+}$. 

$(a)$ Determine $k$ so that $f$ is a joint PDF of $( X, Y )$.


\small
\begin{quote}

We require that 

\begin{align*}
    \int_{0}^{1} \int_{0}^{1-x} f(x, y) dy ~ dx &= k \int_0^1 x \int_0^{1-x} y ~ dy ~ dx \\ 
                                                &= k \int_0^1 x \left[ \frac{y^2}{2} \right]^{1-x}_0 ~ dx \\ 
                                                &= k \int _0^1 x \left( \frac{( 1-x )^2}{2} \right)  ~ dx \\ 
                                                &= \frac{k}{2} \int_0^1 x\left( 1 -2x + x^2 \right)  ~ dx \\ 
                                                &= \frac{k}{2} \int_0^1 x - 2x^2 +x^3 ~ dx \\ 
                                                &= \frac{k}{2} \Big( \left[ \frac{x^2}{2} \right]_0^{1} - 2 \left[ \frac{x^3}{3} \right]_0^1 + \left[ \frac{x^4}{4} \right]_0^1   \Big) \\ 
                                                &=\frac{k}{2} \left( \frac{1}{2} - \frac{2}{3} + \frac{1}{4} \right)  \\ 
                                                &=\frac{k}{2} \cdot \frac{1}{12} \\ 
                                                &= \frac{k}{24}
\end{align*}

\end{quote}
\normalsize

This entails $k$ must be $24$ in order for $f$ to be a joint PDF of $(X, Y)$.


$(b)$ The marginal probability distribution of $Y$ is 

\begin{align*}
    f_Y(y) &= 24\int_0^1 xy ~ dx &= 24y \int_0^1 x ~ dx \\ 
                       &=24y y \left[ \frac{x^2}{2} \right]_0^1 \\ 
                       &= 24 y \cdot \frac{1}{2} \\ 
                       &= 12y
\end{align*}


Same reasoning gives $f_X(x) = 12x$.

$(c)$ Since $f_X(x) f_Y(y) = 12x \cdot 12y = 12^2xy \neq f(x, y)$,
the r.vs are not independent.

\pagebreak 

\textbf{Problem 3} Let 

\begin{align*}
    f(x, y) = \begin{cases}
        k(x + y) & 0 \leq x \leq 10 \land  0 \leq y \leq 10 \\ 
        0 & \text{otherwise}
    \end{cases}
\end{align*}

$(a)$ The value of $k$ can be found solving the equation

\begin{equation}
    k \int_0^{10} \int_0^{10} (x + y) dy ~ dx = 1
\end{equation}

Observe that 

\begin{align*}
    \int_0^{10}(x+y) dy &= x\int_0^{10} dy + \int_0^{10} y ~ dy \\ 
                        &= 10x + \frac{10^2}{2} \\ 
                        &= 10x + 50
\end{align*}

The left-hand side of equation $(1)$ then becomes 

\begin{align*}
    k \int_0^{10} 10x + 50 ~ dx &= k \left[ 10 \left[ \frac{x^2}{2} \right]^{10}_0 + 500  \right]  \\ 
                                &=k \left[ 500 + 500 \right] \\ 
                                &=1000k
\end{align*}

Hence, the solution to $(1)$ is $k = \frac{1}{1000}$.

$(b)$ Now that we know that the PDF is $f(x, y) = (x+y) / 1000$ (for $x, y \in 0, 10]$), 
we can find $P(X + Y < 5)$:

\begin{equation*}
    \int_{x=0}^{5} \int_{y = 0}^{5 - x} \frac{x + y}{1000} ~ dy ~ dx
\end{equation*}

Observe that 

\begin{align*}
    \frac{1}{1000} \int_0^{5 - x} x + y ~ dy &= \frac{1}{1000} \left[ x(5-x) + \frac{(5-x)^2}{2} \right] \\ 
                                            &=\frac{1}{1000} \frac{10x - 2x^2 + 25 - 10x + x^2}{2} \\ 
                                            &=\frac{25 - x^2}{2000}
\end{align*}

Then 

\begin{align*}
    P(X + Y < 5) &= \frac{1}{2000}\int_0^{5} 25 - x^2 ~ dx \\ 
                 &= \frac{1}{2000} \left( 25(5) - \frac{5^3}{3} \right)  \\ 
                 &=\frac{1}{2000}\left( \frac{2\cdot 5^3}{3} \right)  \\ 
                 &= \frac{250}{6000} \\ 
                 &= \frac{125}{3000} \\ 
                 &= \frac{25}{600} \\ 
                 &= \frac{1}{24}
\end{align*}

$(c)$ It is easy to see that 

\begin{equation*}
    f_Y(y) = \frac{1}{1000}  \int_0^{10} x + y ~ dx &= \frac{1}{20} + \frac{y}{100}
\end{equation*}

\begin{equation*}
    f_X(x) = \frac{1}{1000} \int_0^{10} x + y ~ dy = \frac{1}{20} + \frac{x}{100}
\end{equation*}

Note that 

\begin{align*}
    f_X(x) \cdot f_Y(y) &= \frac{5+y}{100}\frac{5+x}{100} \\ 
                        &=\frac{25 + 5x +5y + yx}{100^2} \\ 
                        &\neq f(x, y)
\end{align*}

The variables are not independent.

$(d)$ It is straightforward to compute

\begin{equation*}
    \mathbb{E}\left[ Y \right]  = \mathbb{E}\left[ X \right] &= \int_0^{10} \frac{5x + x^2}{100} ~ dx = \frac{35}{6}
\end{equation*}

Now, observe that 

\begin{equation*}
    \mathbb{E}\left[ X Y \right] &= \int_0^{10} \int_0^{10} \left( xy \right) \frac{x+y}{100} ~dy ~ dx = \frac{100}{3}
\end{equation*}

The covariance is 

\begin{align*}
    Cov(X, Y) &= \mathbb{E}\left[X  Y  \right] - \mathbb{E}\left[ X \right] \mathbb{E}\left[ Y \right]  \\ 
    &= \frac{100}{3} - ( \frac{35}{6} )^2 \\ 
    &= -0.694
\end{align*}

In short, there is a slight negative correlation between the variables.


\pagebreak 

Let $Y \sim \Gamma(\alpha = 100, \beta = 20)$. Let $L = 30Y + 2Y^2$. Compute 
the expected value and variance of $L$.

Let us recall that $\mathbb{E}\left[ Y \right] = \alpha\beta$, and 
$\mathbb{V}\left[ Y \right] = \alpha\beta^2$. From this, using linearity of the expectation, 


\begin{align*}
    \mathbb{E}\left[ L \right] &= 30\mathbb{E}\left[ Y \right] + 2 \mathbb{E}\left[ Y^2 \right] 
\end{align*}

Observe that 

\begin{align*}
    \mathbb{E}\left[ Y^2 \right] &= \mathbb{V}\left[ Y \right] + \mathbb{E}^2\left[ Y \right] \\ 
                                 &= (100)20^2 + (100\cdot 20)^2
\end{align*}


from which $\mathbb{E}\left[ L \right] $ is deduced.

\pagebreak 

Let $X, Y$ be two Poisson-distributed independent r.vs with parameters
$\lambda_X, \lambda_Y$, respectively.

$(a)$ Since $X, Y$ are independent, their joint PDFs is 

\begin{align*}
    f_{X, Y}(n, m) &= f_X(n)f_Y(m) \\ 
                   &= \left( \frac{ \lambda_X^{n}e^{-\lambda_X} }{n!} \right) \left( \frac{\lambda_Y^m e^{-\lambda_Y}}{m!} \right)  \\ 
                   &=\frac{ \lambda_X^n \lambda_Y^m \exp( -\lambda_X - \lambda_Y )}{n!m!}
\end{align*}

$(b)$ The probability that \textit{at most} one event occurs considering both
vars is the probability that $(X, Y) \in \left\{ (0, 0), (1, 0), (0, 1)
\right\} $. This are obviously disjoint events, from which we get the
probability $f_{XY}(0, 0) + f_{XY}(0, 1) + f_{XY}(1, 0)$. Observe 
that this is naturally equal to 

\begin{align*}
    \sum_{n=0}^{1} \sum_{m=0}^{1 - n} f_{XY}(n, m)
\end{align*}

$(c)$ The general expression for the probability that the total number of
events, either $X$ or $Y$, is exactly equal to $\varphi$, must be an expression
for $P(X + Y = \varphi)$. This is of course the probability of $(X, Y) \in \{(0,
\varphi), (1, \varphi - 1), \ldots, (\varphi - 1, 1), (\varphi, 0)\}$.
All of these events are obviously disjoint. So

\begin{align*}
    P(X + Y = \varphi) &= \sum_{n=0}^{\varphi} f_{XY}(n, \varphi - n)
\end{align*}

\pagebreak 

Let $X, Y$ two independent and exponentially distributed r.vs with $\lambda = 1$.

$(a)$ The joint PDF of $X, Y$ (using the fact that they are independent) is 

\begin{align*}
    f_{XY}(x, y) &= f_X(x) f_Y(y) \\ 
                 &= \left( \lambda e^{-\lambda x} \right) \left( \lambda e^{- \lambda y} \right) \\ 
                 &=\lambda^2 \exp(-\lambda x - \lambda y) \\ 
                 &= \exp(-x - y)
\end{align*}

for $x, y \geq 0$, and zero otherwise.

$(b)$ The probability that both $X$ and $Y$ fall in $[0, 1000]$ is 

\begin{equation*}
    \int_{x=0}^{1000} \int_{y=0}^{1000} f_{XY}(x, y) ~ dy ~ dx &= \int_{x=0}^{1000} \int_{y=0}^{1000} e^{-x} e^{-y} ~ dy ~ dx
\end{equation*}

Let $u = -x, w = -y$. Clearly, $\frac{dw}{dy} = -1 \Rightarrow dw = -dy$. Similarly,
$du = -dx$. Finally, if $y = 1000, w = -1000$. So,

\begin{align*}
    \int_{0}^{1000} e^{-y} &= -\int_0^{-1000} e^w ~ dw \\ 
                           &= - \left( - \int_{-1000}^{0} e^w ~ dw \right)  \\
                           &= \left[ e^{w} \right]_{-1000}^{0} \\ 
                           &= \left[ e^0 - e^{-1000} \right] \\ 
                           &= \left[ 1 - e^{-1000} \right] \\ 
                           &= 1 - \frac{1}{e^{1000}}
\end{align*}

So the probability is 

\begin{align*}
    \left( 1 - \frac{1}{e^{1000}} \right) \int_{x=0}^{1000} e^{-x} ~ dx
\end{align*}

The same reasoning, substituting with $u = -x$, gives that $\int_0^{1000} e^{-x} ~ dx = 1 - e^{-1000}$, 
and hence the result is

\begin{equation*}
    \left( 1 - e^{-1000} \right)^{2}
\end{equation*}

$(c)$ The probability that the total duration of both events is at most $2000$ is $P(X + Y \leq 2000)$,
which resolves to 


\begin{align*}
    \int_{x=0}^{2000} \int_{y = 0}^{2000 - x} f_{XY}(x, y) ~ dy ~ dx &= \int_{x=0}^{2000} \int_{y = 0}^{2000 - x} e^{-x} e^{-y} ~ dy ~ dx
\end{align*}

We already know $\int_0^b e^{-y} ~ dy = 1 - e^{-b}$, which means we have obtain 

\begin{align*}
    P(X + Y \leq 2000) &= \int_{x=0}^{2000} e^{-x} \left( 1 - e^{-2000 + x} \right) ~ dx\\ 
                       &=\int_0^{2000}e^{-x} ~ dx - \int_{x=0}^{2000} e^{-2000} ~ dx \\ 
                       &=\left( 1 - e^{-2000} \right) - e^{-2000} (2000 - 0) \\ 
                       &= \left( 1 - e^{-2000} \right) - 2000 e^{-2000} \\ 
                       &=1 - 2001 e^{-2000} 
\end{align*}

\pagebreak 

Let $X$ an r.v. with CDF

\begin{align*}
    F(x) = \begin{cases}
        0 & x < -1 \\
        a & -1 \leq x < \frac{1}{2} \\
        \frac{3}{4} & \frac{1}{2} \leq x < 1\\
        b & 1 \leq x \\
    \end{cases}
\end{align*}

and assume $E(X) = -\frac{1}{8}$.


$(a)$ It is necessary that $b = 1$ for the CDF to make sense. The value of 
$a$ can be found by recalling two things: (1) that $p(x_i) = F(x_i) - F(x_{i-1})$,
and that $\sum_{x \in Im(X)} x p(x) = E(X)$. So we have 

\begin{align*}
    \sum_{i = 1}^{3} x_i \left( F(x_i) - F(x_{i-1}) \right) &= (-1)a + \frac{1}{2} \left( \frac{3}{4} - a \right) + \left( 1 - \frac{3}{4} \right) \\ 
                                                            &= -a + \frac{3}{8} -\frac{a}{2} + \frac{1}{4} \\ 
                                                            &= - \frac{3a}{2} + \frac{5}{8} \\ 
                                                            &= E(X)
\end{align*}

Since $E(X) = -\frac{1}{8}$ we have 

\begin{align*}
    - \frac{3a}{2} + \frac{5}{8} = -\frac{1}{8} \iff -\frac{3a}{2} = -\frac{3}{4} \iff -3a = -\frac{3}{2}
\end{align*}

This entails $a = \frac{1}{2}$.

$(b)$ The PMF of $X$ is 

\begin{align*}
    p_X(x) = \begin{cases}
        \frac{1}{2} & x = -1 \\ 
        \frac{1}{4} & x = \frac{1}{2} \\ 
        \frac{1}{4} &= x = 1
    \end{cases}
\end{align*}

The variance of $X$ is $E(X^2) - E^2(X)$. The second term is known. The first can be computed:

\begin{align*}
    E(X^2) &= \sum_{x \in Im(X)} x^2 p_X(x) \\ 
           &= (-1)^2 \frac{1}{2} + \frac{1}{2^2} \frac{1}{4} + 1^2 \frac{1}{4} \\ 
           &=\frac{1}{2} + \frac{1}{2} + \frac{1}{4} \\ 
           &=\frac{5}{4}
\end{align*}



\pagebreak 

\section{Parte 2: Estimación}

\subsection{Desigualdad de Chebyshev}

\begin{theorem}
    Sea $X$ r.v. con media $\mu$ y varianza $\sigma^2$. Sea $t \in \mathbb{R}^+$. Luego 

    \begin{equation}
        P(X \geq t) \leq \frac{E(X)}{t}
    \end{equation}

    Además, 

    \begin{equation}
        P\left( |X - \mu| \geq t \right)  \leq \frac{\sigma^2}{t^2}
    \end{equation}

    Por último, de lo anterior se deduce que, escribiendo $t$ como $\sigma k$,
    con $k \in \mathbb{R}$,

    \begin{equation}
        P\left( |X-\mu| \geq k\sigma \right) \leq \frac{1}{k^2}
    \end{equation}
\end{theorem}

El teorema anterior es útil porque ciertas probabilidades $P(X > a)$ pueden expresarse 
como $P(X - \mu > a - \mu)$, habilitando una aplicación del teorema.

\begin{definition}
    If $\left\{ X_n \right\} $ is a sequence of random variables, we say $\left\{ X_n \right\} $
    converges in probability to $Y$ if for any $\epsilon > 0$ 

    \begin{equation*}
        \lim_{n \to \infty} P(|X_n - Y| \geq \epsilon) = 0
    \end{equation*}
\end{definition}

\begin{theorem}[Weak large numbers]
    Let $X_1, \ldots, X_n$ random vars with equal mean $\mu$ and equal 
    variance $\sigma^2$. Let $\overline{X}_n$ be the mean 
    value of the sequence.

    For every $\epsilon > 0$,

    \begin{align*}
        &P(|\overline{X}_n - \mu| \geq \epsilon) \leq \frac{\sigma^2}{n \epsilon^2} \\
        &P(|\overline{X}_n - \mu| \leq \epsilon) \geq 1 - \frac{\sigma^2}{n \epsilon^2}
    \end{align*}

    This entails 

    \begin{align*}
        &\lim_{n \to \infty} P(|\overline{X}_n - \mu| \geq \epsilon) = 0 \\ 
        &\lim_{n \to \infty} P(|\overline{X}_n - \mu| \leq \epsilon) = 1
    \end{align*}

    Therefore, $\left\{ \overline{X}_n \right\} $ converges in probability 
    to the random variable that is centered at $\mu$.

\end{theorem}

Intuitively, this means that if a sequence $\{ X_n \}$ converges in probability
to $Y$, we can make the probability of $(|X_n - X| < \epsilon)$ arbitrarily
close to $1$ for any $\epsilon$, taking a sufficiently large $n$.























\end{document}



