\documentclass[a4paper, 12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{newtxtext} \usepackage{newtxmath}
\usepackage{amsmath, amssymb}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{problem}{Problem}
\newtheorem{example}{Example} \newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\begin{document}

\section{Set theory}%
\label{sec:Set theory}

We take the standard definition of set: an unordered collection of elements. For any given set $A$,
we presume the existence of a corresponding set $\mathbb{U}$ which we call \textit{universe}, s.t. 
$A \subseteq \mathbb{U}, \overline{A} \in \mathbb{U}$, and $A \cup \overline{A} = \mathbb{U}$.
We also take the standard definition of set difference:

\begin{equation*}
    A  - B := \left\{ a \in A : a \not\in B \right\} 
\end{equation*}

and define the \textit{symmetric diference}

\begin{equation*}
    A \Delta B := (A - B) \cup (B -A)
\end{equation*}

For example, if $\mathcal{P}$ is the set of prime numbers and $\mathcal{A} = \left\{ n \in \mathbb{N} : n \equiv 0 \mod 2 \right\} $, we have 

\begin{align*}
    \mathcal{P} \Delta \mathcal{A} &= \left\{ a \in \mathcal{P} : a \not\in \mathcal{A} \right\} \cup \left\{ a \in \mathcal{A} : a \not\in \mathcal{P} \right\}  \\ 
                                   &= (\mathcal{P} - \left\{ 2 \right\} ) \cup (\mathcal{A} - \left\{ 2 \right\} ) \\ 
                                   &= (\mathcal{P} \cup \mathcal{A}) - \left\{  2 \right\} 
\end{align*}

the set containing all prime numbers that are not even---all primes except 2---and all
even numbers that aren't prime---all even numbers except 2---.

\begin{theorem}
    $A \subseteq B$ if and only if $B^c \subseteq A^c$.
\end{theorem}

If $I$ is a countable set of indexes $i_1, i_2, \ldots$, potentially finite,
such that $A_i$ is a set, we say $\left\{ A_i : i \in I \right\} $ is an
indexed family of sets, and we denote it by $\left\{ A_i \right\}_{i \in I} $.
We analogously define an indexed family of set elements $\left\{ a_i
\right\}_{i \in I} $.







\pagebreak

These notes are extremely limited and sketchy. The reason is that I was familiar
with probability theory before taking this class. Thus, I did not need to take
many notes. My fellow student will do good in using this document only to
corroborate exercises, problems, and final exams. 

\section{Preliminaries}

Let $\Omega$ denote the sample space of an experiment; i.e. the set of all
values which may result from an experiment. If $A \subseteq \Omega$ we say $A$
is an event. If $\mathcal{A}$ is a $\sigma$-algebra over $\Omega$ we say
$\mathcal{A}$ is a family of events. 

\begin{quote}
    A $\sigma$-algebra on a set $X$ is a non-empty collection of subsets of $X$
    that is closed under complement, countable unions and countable
    intersections. It is usual to take $\mathcal{A} = \mathcal{P}(\Omega)$.
\end{quote}

As usual, if $\mathcal{A}$ a $\sigma$-algebra over $\Omega$, for every $A \in
\mathcal{A}$ we define $P(A)$ as the function that satisfies the following
axioms: 

\begin{itemize}
    \item $P(A) \geq 0$ 
    \item $P(\Omega) = 1$ 
    \item If $A_1, A_2, \ldots \in \Omega, A_i \cap A_j = \emptyset$  for all
        $i \neq j$, then 

        \begin{align*}
            P(A_1 \cup A_2 \cup  \ldots) = \sum_{i=1}^{\infty} P(A_i)
        \end{align*}
\end{itemize}

A probabilistic model is a 3-uple $(\Omega, \mathcal{A}, P)$. We will assume
from now on that $\Omega$ refers to a sample space, $\mathcal{A}$ to
$\mathcal{P}(\Omega)$, and $P$ to the probability function.

A random variable is a function $X : \Omega \mapsto \mathbb{R}$.

\section{Elementary laws}

\subsection{Union, intersection, conditionality, etc.}

This is a collection of notes. Their justification should be intuitively
accessible if one stops and think of their formulas in terms of the subspaces of
$\Omega$ involved.

Let $A, B \in \Omega$. The probability that $A$ occurs given that $B$ has
occurred is 

\begin{align*}
    P(A \mid B) = \frac{P (A \cap B)}{P(B)}
\end{align*}    

Observe that this gives a formula for $P(A \cap B)$. Furthermore, 

\begin{align*}
    P(A \cap B) = P(A)P(B \mid A) = P(B)P(A \mid B)
\end{align*}

If $A, B$ are independent, $P(A \cap B) = P(A)P(B)$.

\begin{align*}
    P(A \cup B) = P(A) + P(B) - P(A \cap B)
\end{align*}

If $A, B$ are mutually exclusive then $P(A \cap B) = \emptyset$ and $P(A \cup B)
= P(A) + P(B)$.

It is useful to remember the following property too. Since $P(A \cup B) = P(A) +
P(B)$ we have that $P((A \cap B) \cup (A - B)) = P(A \cap B) + P(A - B)$, which
implies 

\begin{align*}
    P(A - B) = P(A) - P(A \cap B)
\end{align*}

\subsection{The law of total probability and Bayes' rule}

Let $B_1, \ldots, B_k$, $k \in \mathbb{N}$, s.t. 

\begin{align*}
    \Omega = B_1 \cup \ldots \cup B_k \\ 
    \forall i, j \in [1, k] : i \neq j : B_i \cap  B_j = \emptyset
\end{align*}

Then $\left\{ B_1, \ldots, B_k \right\} $ is a partition of $\Omega$. If $A
\subseteq \Omega$ then it can be decomposed using a partition $\{B_1, \ldots,
B_k\}$ as $A = (A \cap B_1) \cup \ldots (A \cap B_k)$.

\begin{theorem}
    If $\left\{ B_1, \ldots, B_k \right\} $, $k \in \mathbb{N}$, is a partition
    of $\Omega$ s.t. $P(B_i) > 0$ for all $1\leq i \leq k$, then for any $A
    \subseteq \Omega$

    \begin{align*}
        P(A) = \sum_{i=1}^{k} P(A \mid B_i) P(B_i)
    \end{align*}
\end{theorem}


\small
\begin{quote}

\textbf{Proof.} Let $A \subseteq \Omega$. Because $B_1, \ldots, B_k$ partition
$\Omega$, $(A \cap B_i) \cap  (A \cap B_j) = A \cap \emptyset = \emptyset$.
Thus, the two events are mutually exclusive. Thus

\begin{align*}
    P(A) &= P \left( \left( A \cap B_1 \right) \cup \ldots \left( A \cap B_k
    \right)   \right)  \\ 
        &= P(A \cap B_1) + \ldots + P(A \cap B_k) \\ 
        &= \sum_{i=1}^{k} P(A \mid B_i)P(B_i)
\end{align*}

\end{quote}
\normalsize

\begin{theorem}[Bayes' Rule]
    Assume $\left\{ B_1, \ldots, B_k \right\} $ is a partition of $\Omega$ and
    $P(B_i) > 0, i = 1, \ldots, k$. Then 

    \begin{align*}
        P(B_j \mid A) = \frac{P(A \mid B_j) P(B_j)}{\sum_{i=1}^{k}P(A \mid
        B_i)P(B_i)}
    \end{align*}
\end{theorem}

The proof follows from the definition of conditional probability and the law of
total probability.


\section{Discrete random variables}

A random variable $X : \mathcal{D}_X \subseteq  \Omega \mapsto \mathbb{R}$ is
discrete iff $\mathcal{D}_X$ is finite or countably infinite. 

If $Y$ is a random variable then expression $(Y = y) = \left\{ \zeta \in \omega
: X_{\zeta} = y \right\} $. In other words, $(Y = y)$ denotes the subset of
$\Omega$ whose elements are assigned the value $y$ by the random variable.


\small
\begin{quote}

\textbf{Example.} In a coin toss, a random variable $X$ may assign to the sample
point "heads" the value $1$ and the sample point "tails" the value $-1$. Then
$(X = 1) = 1$, etc.

\end{quote}
\normalsize

We define $P(Y = y) = \sum_{\zeta \in \Omega : Y_{\zeta} = y} P(\zeta)$. The
probability distribution of $Y$ is the general function  

\begin{align*}
    p : \mathbb{R} &\mapsto [0, 1] \\ 
    y &\mapsto P(Y = y)
\end{align*}

Since the probability distribution $p$ is defined as the probability of given
sets of events, it follows that $0 \leq p(y) \leq 1$ for all $y$ and
$sum_{y}p(y) = 1$.

We asume the reader knows the definition of expected value. Let $g : \mathbb{R}
\mapsto \mathbb{R}$. Then $g \circ Y$ (or simply $g(Y)$) has expected value 

\begin{align*}
    \mathbb{E}\left[ g(Y) \right] = \sum_{y \in Im(Y)} g(y)p(y)
\end{align*}


\small
\begin{quote}

    \textbf{Proof.} $P(g(Y) = g_i) = \sum_{y \in Im(Y), g(y) = g_i} p(y)$. Let
    this probability function for $g(Y)$ be called $p_g(y)$. Then 

    \begin{align*}
        \mathbb{E}[g(Y)] &= \sum_{y \in Im(g \circ Y)} y p_g(y) \\ 
                         &= \sum_{y \in Im(g \circ Y)} y \left[ \sum_{x \in
                         Im(Y), g(x)= y} p(x) \right] \\ 
                         &= \sum_{y \in Im(g \circ Y)}  \left[ \sum_{x \in
                         Im(Y), g(x)= y} y p(x) \right] \\ 
                         &= \sum_{x \in Im(y)} g(x)p(x)
    \end{align*}

\end{quote}
\normalsize

\begin{definition}
    Let $\mu = \mathbb{E}\left[ Y \right] $. Then 

    \begin{align*}
        \mathbb{V}\left[ Y \right] = \mathbb{E} \left[ ( Y - \mu )^2 \right] 
    \end{align*}
\end{definition}

\begin{theorem}
    Let $Y$ a random variable with p.m.f. $p$ and $g_1, \ldots, g_k$ functions
    of $Y$. Then 

    \begin{align*}
        \mathbb{E} \left[ g_1(Y) + \ldots + g_k(Y) \right] = \mathbb{E}\left[
        g_1(Y) \right] + \ldots + \mathbb{E}\left[ g_k(Y) \right] 
    \end{align*}
\end{theorem}

\begin{theorem}
    \begin{align*}
    \mathbb{V}[Y] = \mathbb{E}[Y^2] - \mathbb{E}[Y]^2
    \end{align*}
\end{theorem}

This is also easy to prove from the definition of $\mathbb{V}$. 

\pagebreak

\section{Finales}

\subsection{Final 2003-12}

\begin{problem}
    Prove \textit{a.} $P(A \cup B) = P(A) + P(B) - P(A \cap B)$, \textit{b.}
    $P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B
    \cap C) + P(A \cap B \cap C)$, \textit{c.} $A \subset B \Rightarrow P(A) \leq
    P(B) \land P(B - a) = P(B) - P(A)$.
\end{problem}


\small
\begin{quote}


Let $(\Omega, \mathcal{A}, P)$ be an arbitrary probabilistic model and let $A,
B, C \in \Omega$. 

\textit{(1)} Consider the set $A \cup B$ and let $A \cap B = I$. Observe that

\begin{align*}
    A \cup B = (A \cap B) \cup (A - B) \cup (B - A) \\ 
\end{align*}

Then $P(A \cup B) = P \left( (A \cap B) \cup (A - B) \cup (B-A) \right) $. Since
the intersection of these events is empty, by the axioms of the probability
function we have $P(A \cup B) = P(A \cap B) + P(A - B) + P(B - A)$. Using the
fact that $P(X - Y) = P(X) - P(X \cap Y)$ we have 

\begin{align*}
    P(A \cap B) + P(A) - P(A \cap B) + P(B) - P(B \cap A) = P(A) + P(B) - P(A
    \cap B)
\end{align*}

\textit{(2)}  

\begin{align*}
    P(A \cup B \cup C) &= P(A \cup B) + P(C) - P\left( (A \cup B) \cap C
    \right)\\ 
                       &=P(A) + P(B) + P(C) - P(A \cap B) - P\left( (A \cap C)
                       \cup (B \cap C) \right) \\ 
                       &=P(A) + P(B) + P(C) - P(A \cap B)\\ 
        -&\left[ P(A \cap C) + P(B \cap C) - P(A \cap B \cap C) \right]  \\ 
         &=P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A
         \cap B \cap C) \blacksquare
\end{align*}


\end{quote}
\normalsize

\pagebreak 

\begin{problem}
    Define the variance of a random variable $X$. Show that $\mathbb{V}\left[ cX
    \right] = c^2 \mathbb{V}\left[ X \right], \mathbb{V}\left[ X + Y \right] =
    \mathbb{V}[X] + \mathbb{V}\left[ Y \right]   $ if $X, Y$ independent.
\end{problem}


\small
\begin{quote}

\textit{(1)} The variance of a random variable $X$ is $\mathbb{V}\left[ X
\right] = \mathbb{E}\left[ (X - \mu)^2 \right] $ where $\mu = \mathbb{E}\left[ X
\right] $.

\textit{(2)} Observe that 

\begin{align*}
    \mathbb{V}\left[ cX \right] &= \mathbb{E}\left[ (cX)^2 \right] -
    \mathbb{E}\left[ cX \right]^2 \\ 
                                &=c^2 \mathbb{E} \left[ X^2 \right] -
                                \mathbb{E}\left[ cX \right] \mathbb{E}\left[ cX
                                \right]  \\ 
                                &= c^2 \mathbb{E}\left[ X^2 \right] -c^2
                                \mathbb{E}\left[ X \right] ^2 \\ 
                                &c^2 \left( \mathbb{E}\left[ X^2 \right] - \mu^2
                                \right)  \\ 
                                &c^2 \mathbb{V}\left[ X \right]
\end{align*}

\textit{(3)} 

\begin{align*}
    \mathbb{V}\left[ X + Y\right] &= \mathbb{E}\left[ (X+Y)^2 \right] -
    \mathbb{E}\left[ X + Y \right]^2 \\ 
                                  &=\mathbb{E}\left[ X^2 + 2XY + Y^2 \right] -
                                  \left( \mu_X + \mu_Y \right)^2 \\ 
                                  &=\mathbb{E}\left[ X^2 \right] + 2
                                  \mathbb{E}\left[ XY \right]  +
                                  \mathbb{E}\left[ Y^2 \right] - \mu_X^2 -
                                  2\mu_x\mu_Y - \mu_Y^2 \\ 
    &= \mathbb{E}\left[ X^2 \right] - \mu_X^2 + \mathbb{E}\left[ Y^2 \right] -
    \mu_Y^2 + 2\mathbb{E}\left[ X \right] \mathbb{E}\left[ Y \right] -
    2\mu_x\mu_Y &\{\text{Independence}\}\\ 
    &= \mathbb{V}\left[ X \right] + \mathbb{V}\left[ Y \right] 
\end{align*}

\end{quote}
\normalsize

\pagebreak 

\begin{problem}
    Give a $95\%$ confidence interval for the mean $\mu$ assuming the variance
    $\sigma^2$ is known. Then assuming the variance us unknown.
\end{problem}


\small
\begin{quote}

\textit{(1)} Given a sample $\textbf{x} = X_1, X_2, \ldots, X_n$ with $X_i \sim
\mathcal{N}(\mu, \sigma)$, we can use the fact that $\overline{X} \sim
\mathcal{N}(\mu, \frac{\sigma}{\sqrt{n} } )$ to construct the statistic

\begin{align*}
    Z = \frac{\overline{X} - \mu}{\sigma} \sqrt{n} 
\end{align*}

With sufficiently large $n$, $Z \sim \mathcal{N}(0, 1)$. We want to choose a
value of $Z$ s.t. it occupies $.975$ of the area under the standard normal
curve. Such value is $Z = 1.96$. The confidence interval is then 

\begin{align*}
    \left[ \overline{X} - 1.96 \frac{\sigma}{\sqrt{n} }, \overline{X} + 1.96
    \frac{\sigma}{\sqrt{n} } \right] 
\end{align*}

If $\sigma$ is unknown we would simply use $\hat{\sigma} = \frac{1}{n-1}\sum
(X_i
- \overline{X})^2$ as an estimator and keep everything else the same.

\textit{(2)} If the variance is unknown \textit{and} the sample size is $n \leq
30$, then we must use $\hat{\sigma}$ as before, but use the $t$-Student
distribution. Namely, our confidence interval will now be 

\begin{align*}
    \overline{X} \pm t_{0.025} \hat{\sigma}
\end{align*}

The degrees of freedom of the $t$-Student distribution depends on $n$, of
course.

\end{quote}
\normalsize

\pagebreak 

\begin{problem}
    The number of kids that come to a vending machine during an hour is a
    discrete random variable $Y$ with values in $\{0, 8, 18, 30\}$.
\end{problem}

\textit{(1)} If $P(Y = 8) = \frac{1}{4}, P(Y = 18) = \frac{1}{3},
\mathbb{E}\left[ Y \right] = 13$, what is the value of $P(Y = 30)$?


\small
\begin{quote}

    We know $\mathbb{E}\left[ Y \right] = \sum_{y \in Im(Y)}yp(y) = 8\cdot
    \frac{1}{4} + 18\cdot \frac{1}{3} + 0p(0) + 30p(30) = 13$. Then 

    \begin{align*}
        8 + 30p(30) = 13 \Rightarrow p(30) = \frac{5}{30} = \frac{1}{6}
    \end{align*}

\end{quote}
\normalsize

\textit{(2)} What is the value of $P(Y = 0)$?


\small
\begin{quote}

    We require that $\sum_{y \in Im(Y)}p(y) = 1$. We have 

    \begin{align*}
        \sum_{y \in Im(Y)} p(y) &= \frac{1}{4} + \frac{1}{3} + \frac{1}{6} +
        p(0)\\ 
                                &= \frac{3}{4} + p(0)
    \end{align*}

    Then $\frac{3}{4} + p(0) = 1 \Rightarrow p(0) = \frac{1}{4}$

\end{quote}
\normalsize

\textit{(3)} Find $P(12 \leq Y \leq 20)$ and $P(Y \neq 30)$. 


\small
\begin{quote}

$P(12 \leq Y \leq 20) = P(18) = \frac{1}{3}$. $P(Y \neq 30) = \frac{1}{4} +
\frac{1}{4} + \frac{1}{3} = \frac{5}{6}$ (Consistent with the fact that $1 -
\frac{1}{6} = \frac{5}{6}$)

\end{quote}
\normalsize

\textit{(4)} If each sell makes $1.30$ dollars and it costs $8$ to maintain the
machine for an hour, what is the expected value of the net profit in an hour?


\small
\begin{quote}

The expected number of kids to approach the vending machine is 13. Each spends
$1.30$ dollars with an expected profit of $16.9$. Minus the cost we have an
expected net profit of $8.9$.

\pagebreak 

\begin{problem}
    Let $X_1, X_2, \ldots, X_n$ random sample where each $X_i$ has density 

    \begin{align*}
        f(x) = \begin{cases}
            \frac{1}{2} \left( 1 + \theta x \right) & -1 \leq x \leq 1 \\ 
            0 & otherwise
        \end{cases}
    \end{align*}

    and where $\theta \in [-1, 1]$. Find $\mathbb{E}\left[ X_i \right] $. What
    is the value of $\mathbb{E}\left[ \overline{X} \right] $? If $\hat{\theta} =
    3\overline{X}$, is it an unbiased estimator of $\theta$?
\end{problem}

\end{quote}
\normalsize


\small
\begin{quote}

\textit{(1)} By definition, 

\begin{align*}
    \mathbb{E}\left[ X_i \right] &=\frac{1}{2} \int_{\mathbb{R}} x + \theta x^2 ~
    dx \\ 
                                 &= \frac{1}{2} \left( \int_{-1}^{1} x ~ dx +
                                 \theta \int_{-1}^{1} x^2 ~ dx \right)  \\ 
                                 &= \frac{1}{2} \left( \theta \left[ \frac{1}{3}
                                 + \frac{1}{3} \right]  \right) \\ 
                                 &=\frac{\theta}{3}
\end{align*}

\textit{(2)} Recall that $\overline{X} = \frac{1}{n} \sum X_i$. Then 

\begin{align*}
    \mathbb{E}\left[ \overline{X} \right] = \frac{1}{n}\sum \mathbb{E}\left[ X_i
    \right]  = \frac{\theta}{3}
\end{align*}

\textit{(3)} Since $\mathbb{E}\left[ \overline{X} \right] = \frac{\theta}{3}$ we
have that $\mathbb{E}\left[ 3 \overline{X} \right] = 3 \mathbb{E}\left[
\overline{X} \right]  = \theta$. Thus, by definition, the estimator is unbiased.




\end{quote}
\normalsize

\pagebreak 

\subsection{Final}

\begin{problem}
    En la producción de cierto artículo se pueden presentar sólo dos tipos de
    defectos $A$ y $B$. Se sabe que $A$ ocurre en un $5\%$ de los artículos; $B$
    se presenta en un $3\%$ de los artículos; y ambos ocurren juntos en un $1\%$
    de los artículos.

    \textit{(1)} Dar la probabilidad de que un artículo tomado al azar presente
    $a.$ solamente el defecto tipo $A$, $b.$ al menos un defecto, $c.$ ningún
    defecto. 

    \textit{(2)} Sea $Y$ la variable que cuenta el número de defectos
    encontrados en el artículo elegido al azar. Dé la PDF y la CDF de $Y$.
    Calcule el valor esperado de $X = 2 - Y^2$.
\end{problem}


\small
\begin{quote}

\textit{(1.a)} Sea $\Omega = \left\{ O, A, B, A \cap B \right\} $ y $\mathcal{A}
= \mathcal{P}(\Omega)$. El problema nos da $P(A), P(B), P(A \cap B)$. Nos
interesa ahora $P(A - B)$. Evidentemente $A - B \in \mathcal{A}$ y por lo tanto
está bien definida la probabilidad. Veamos que

\begin{align*}
    P(A - B) &= P(A) - P(A \cap B) \\ 
             &= .05 - .01 \\ 
             &= .04
\end{align*}

\textit{(1.b)} El conjunto deseado es ${A} \cup {B}$, pero
sabemos que ${A}, {B}$ no son disjuntos. Entonces usamos el
hecho de que $P({A} \cup {B}) = P({A}) + P({B})
- P({A} \cap {B})$. Esto da fácilmente $.05 + .03 - .01 = .07$. 

\textit{(1.c)} La probabilidad de que un elemento tenga algún error cualquiera
es la probabilidad de que tenga solamente un error de tipo $A$, solamente un
error de tipo $B$, o ambos. Esto es, $P(\overline{O}) = .04 + .03 + .01 = .08$.
Luego $P(O) = .92$. (Otra forma de verlo es tomar directamente $P(A \cup B) =
P(A) + P(B) - P(A \cap B) = .08$).

\textit{(2)} Sabemos por el punto \textit{(1)} que $P(Y = 2) = .01, P(Y = 1) =
.07, P(Y = 0) = .98$. Es decir, 

\begin{align*}
    p(y)_Y = \begin{cases}
        .92 & y = 0 \\ 
        .07 & y = 1 \\ 
        .01 & y = 2
    \end{cases}
\end{align*}

Esto implica que 

\begin{align*}
    F(y)_Y = \begin{cases}
        0 &= .92 \\ 
        1  &= .99 \\ 
        2 &= 1
    \end{cases}
\end{align*}

El valor esperado de $2 - Y^2$ es $2 - \mathbb{E}[Y^2]$. Es fácil observar que

\begin{align*}
    \mathbb{E}\left[ Y^2 \right]  = .07^2 + .01^2 \times 2 = .0053
\end{align*}

Luego el valor esperado de $2 - Y^2$ es $1.9947$.

\end{quote}
\normalsize

\pagebreak

\begin{problem}
    Una unidad de radar es usada para medir la velocidad de los automóviles en
    una vía durante la hora de mayor congestionamiento. La velocidad de los
    automóviles está normalmente distribuida con distribución $\mathcal{N}(100,
    8.5)$. \textit{(1)} Dé la probabilidad de que un auto elegido al azar viaje
    a una velocidad de a lo sumo $85$. \textit{(2)} Dé la probabilidad de que
    viaje a una velocidad entre $58$ y $110$. \textit{(3)} Dé la probabilidad de
    que uno de diez automóviles elegidos al azar viaje a una velocidad mayor a
    $88$.
\end{problem}

\textit{(1)} Estandarizamos la variable y utilizamos la distribución normal
estándar. Si $X \sim \mathcal{N}(100, 8.5)$ denota la variable de interés
(velocidad de un vehículo en el contexto del problema),

\begin{align*}
    P(X \leq 85) = \Phi \left( \frac{88 - 100}{8.5} \right) = \Phi(-1.411)
\end{align*}

La tabla de la distribución estándar da $\Phi(-1.764) = .079$. Es decir, la
probabilidad de que un vehículo viaje a a lo sumo $85$ km/h es $7.9\%$.

\textit{(2)} Según la misma lógica, 

\begin{align*}
    P(58 \leq X \leq 110) &= \Phi \left( \frac{110 - 100}{8.5} \right) - \Phi
    \left( \frac{58 - 100}{8.5} \right) \\ &= \Phi(1.176) - \Phi(-4.941) \\
                                           &=.879 - 0 \\ &=.879
\end{align*}

\textit{(3)} Digamos que el evento de obtener un vehículo que viaje a más de
$88$ km/h es un éxito, y cualquier otro caso un fallo. Evidentemente la cantidad
de vehículos que superan $88$ km/h en una muestra de diez sigue
sigue una distribución binomial $Y \sim \mathcal{B}\left( P(X
\geq 88), 10 \right) $. Se nos pide la probabilidad de que haya exactamente un
éxito. Calculemos $p = P(X \geq 88)$. Evidentemente esto es $1 - P(X \leq 88) =
1 - .079 = .921$. Se sigue que  

\begin{align*}
    P(Y = 1) &= \binom{10}{1}\cdot .921 \cdot (1 - .921)^{9} \\ 
             &= 10 \cdot .921 \cdot 0 \\ 
             &\approx 0
\end{align*}

\pagebreak 

\begin{problem}
    Sea $X_i \sim \mathcal{N}(\mu, \sigma)$. Asumamos una muestra de $X_1,
    \ldots, X_{18}$  una muestra con media muestral $\overline{X} = 99.45$ y
    desviación estándar $s_2 =
    1.3$. Dé estimaciones de máxima verosimilitud para la media, la varianza y
    el percentil $5\%$. Construya un intervalo de confianza del $99\%$ para la
    media poblacional.
\end{problem}

Haremos solo el intervalo de confianza. La varianza poblacional es desconocida y
la cantidad de datos es menor a $30$. Usaremos la distribución $t$ de Student.
Recordemos que la media muestral sigue una distribución $\mathcal{N}(\mu,
\frac{\sigma}{\sqrt{n} })$. Usaremos el estadístico

\begin{align*}
    t = \frac{\overline{X} - \mu}{1.3}\sqrt{18} 
\end{align*}

Sabemos que $t$ sigue una distribución $t$ de Student con $17$ grados de
libertad. Queremos determinar el intervalo 

\begin{align*}
    \overline{X} \pm \frac{1.3}{\sqrt{18} } t_{.005}
\end{align*}

(Vea que $\alpha = .01 \Rightarrow \frac{\alpha}{2} = .005$). Usando la table de
la distribución $t$ de Student, tenemos 

\begin{align*}
    \overline{X} \pm \frac{1.3}{\sqrt{18} } 2.567 = \mu \pm 0.30 \times 2.567
\end{align*}

Esto resulta en

\begin{align*}
    99.45 \pm .7701 = \left[ 98.6799, 100.2201 \right] 
\end{align*}

\pagebreak 

\begin{problem}
    En el diseño de mascarillas de bomberos se prueba un conjunto de $120$
    mascarillas. $48$ fallaron la prueba. Dé un intervalo de conianza del $90\%$
    para $p$. Determine el tamaño de muestra necesario para que un intervalo de
    confianza del $90\%$ tenga una longitud de a lo sumo la mitad de la obtenida
    en el item anterior, independientemente del valor de $\hat{p}$.

    Si se quiere determinar si hay suficiente evidencia para decir que $p$ es
    menor a $0.5$, plantee las hipótesis, establezca la región de rechazo con
    nivel de significación del $5\%$, calcule el $p$-valor y tome una decisión
    dado $\alpha = 0.01$.
\end{problem}

\textit{(1)} Tenemos $\hat{p} = \frac{48}{120} = 0.4$. Para muestras
suficientemente grandes, el estimador sigue una distribución normal. Sabemos que
la desviación estándar de este estimador es 

\begin{align*}
    \hat{s} = \sqrt{\frac{0.4 \cdot 0.6}{120}} = 0.044
\end{align*}

Entonces, usamos el estadístico

\begin{align*}
    Z = \frac{0.4 - p}{0.044}
\end{align*}

que sigue una distribución normal estándar y calculamos el intervalo 

\begin{align*}
    0.4 \pm 0.044 z_{.05} &= 0.4 \pm 0.044 \times 1.645 \\ 
    &=0.4 \pm 0.072 \\ 
    &= [.328, .472]
\end{align*}

\textit{(2)} La longitud del intervalo obtenido es $.144$. El tamaño de muestra
rnecesario para que el intervalo tenga una longitud de $.\frac{144}{2} = .072$,
independientemente del valor de $\overline{p}$,
es dada por la ecuación 

\begin{align*}
    \bar{p} + \sqrt{\frac{\bar{p}(1-\bar{p})}{{n} }} 1.645 - \left(\bar{p}
    - \sqrt{\frac{\bar{p}(1-\bar{p})}{{n} }} 1.645\right) &\leq .072 \\ 
    2 \sqrt{\frac{\hat{p}(1 - \hat{p})}{n }} &\leq .072 \\ 
    \hat{p}(1 - \hat{p}) &\leq
                                                    .001 n \\ 
                                                    \frac{\hat{p}(1 -
\hat{p})}{.001} &\leq n
\end{align*}

Si hacemos $u = \hat{p}(1 - \hat{p})$ y observamos que $\frac{1}{.001}
= \frac{1}{\frac{1}{1000}}=1000$, tenemos que $n \geq 1000u$ es el tamaño de
muestra necesario para que el intervalo tenga la longitud deseada o menos. En el
caso particular de nuestra $\hat{p} = 0.4$, deberíamos tener $240$
observaciones. Observe que esto es el doble de las observaciones que tenemos $(n
= 120)$. Esto tiene sentido, pues se nos pidió reducir la longitud del intervalo
a la mitad.


\textit{(3)} Hagamos la prueba de hipótesis. Sea $H_0 : p = 0.5$. La hipótesis
alternativa será $H_1 : p < 0.5$.

Asuma que la hipótesis nula es verdadera. ¿Cuál es la probabilidad de haber
encontrado $\hat{p} = 0.4$ en este caso? Si la hipótesis nula fuera
verdadera, la desviación estándar debería ser $\sqrt{\frac{0.5^2}{120}} = .045
$. El valor observado $\hat{p}$ estaría entonces a

\begin{align*}
    z = \frac{0.4 - 0.5}{.045} = -2.222
\end{align*}

desviaciones estándar de la media. El $p$-valor será el área de la distribución
normal estándar a la izquierda de $-2.222$---es decir, la probabilidad de
observar un valor tan o más extremo que $-2.222$---. Tomamos el área a la
izquierda porque la hipótesis alternativa es que $p$ es \textit{menor} a es
$0.5$. Entonces, la tabla de la distribución normal nos dice que $p\text{-valor}
= .0132$. Como esto es superior a $\alpha = 0.01$, no rechazamos la hipótesis
nula.

\pagebreak 

\begin{problem}
    Sea $X_1, \ldots, X_n$ una muestra con $n \geq 3$ y $X_i \sim
    \text{Poisson}(\lambda)$. \textit{(1)} Encuentre el estimador de $\lambda$
    usando el método de los momentos. \textit{(2)} Encuentre el estimador de
    $\lambda$ usando máxima verosimilitud. \textit{(3)} Considere los
    estimadores 

    \begin{align*}
        \overline{\lambda_1} = X_1, \overline{\lambda_2} = \frac{X_1 + X_n}{2},
        \overline{\lambda_3} = \frac{X_1 + 2X_2 + X_3}{3}, \overline{\lambda_4}
        = \overline{X}
    \end{align*}

    ¿Cuál es insesgado? ¿Cuál tiene menor varianza?
\end{problem}

Recuerde que si $X \sim \text{Poisson}(\lambda)$ entonces $f(x) = e^{-\lambda}
\frac{\lambda^x}{x!}$ para $x \geq 0$.

\textit{(1)} El primer momento muestral es $\overline{X}$. El primer momento (o
la esperanza) de una Poisson es su parámetro $\lambda$. Igualando ambos
obtenemos $\overline{X} = \lambda$ y vemos que la media muestral es un estimador
por el método de los momentos de $\lambda$. 

\textit{(2)} Usando máxima verosimilitud, observemos que 

\begin{align*}
    \mathcal{L}(\lambda \mid X_1, \ldots, X_n) &= \prod_{i=1}^{n} f(x_i \mid
    \lambda)   \\ 
\end{align*}

Maximizar la expresión de arriba equivale a maximizar su logaritmo. En
consecuencia, observamos que

\begin{align*}
    \ln \left[\prod_{i=1}^{n} e^{-\lambda} \frac{\lambda^{x_i}}{x_i!}\right] &=
    \sum_{i=1}^{n} \ln \left( e^{-\lambda} \frac{\lambda^{x_i}}{x_i!} \right) 
    \\
    &=
    \sum_{i=1}^{n} \left[\ln(e^{-\lambda}) +    \ln\left(    \lambda^{x_i}
    \right)-\ln\left(x_i!\right)\right]\\ 
    &= \sum_{i=1}^{n} \left[- \lambda + \ln(\lambda^{x_i}) - \ln\left( x_i !
    \right) \right] \\ 
    &=- \lambda n + \sum_{i=1}^{n} \ln(\lambda^{x_i}) - \ln(x_i!)
\end{align*}

Sea $\Lambda$ la expresión arriba. Entonces

\begin{align*}
    \frac{\partial \Lambda}{\partial \lambda } &= -n + \sum_{i=1}^{n}
    \frac{\partial \ln(u) }{\partial u } \frac{\partial u}{\partial \lambda} &
    \{u = \lambda^{x_i}\} \\ 
                                                                             &=
                                                                             -n
                                                                             +
                                                                             \sum_{i=1}^{n}
                                                                             \frac{1}{\lambda^{x_i}}x_i
                                                                             \lambda^{x_i
                                                                             -
                                                                         1} \\ 
    &= -n + \sum_{i=1}^{n} \frac{x_i}{\lambda} \\ 
    &= -n + \frac{1}{\lambda}  \sum_{i=1}^{n} x_i
\end{align*}

Encontramos los puntos críticos respecto a $\lambda$ tomando 

\begin{align*}
    -n + \frac{1}{\lambda} \sum x_i = 0 \Rightarrow \sum x_i = \lambda n
\end{align*}

o equivalentemente 

\begin{align*}
    \lambda = \frac{1}{n}\sum x_i = \overline{X}
\end{align*}

Que este punto es un máximo se sigue de que la distribución de Poisson es
cóncava y carece de mínimo. 

\textit{(3)} Observe que $\mathbb{E}[X_1] = \lambda$,
$\frac{1}{2}\mathbb{E}\left[ X_1 + X_2 \right] = \frac{1}{2}2\lambda = \lambda$,
y $\frac{1}{3}\mathbb{E}\left[ X_1 + 2X_2+X_3 \right] = \frac{1}{3} \left(
\lambda + 2 \lambda + \lambda \right) = \frac{4\lambda}{3} $. Se sigue que el
primer y segundo estimador son insesgados y el tercero es sesgado. Ya hemos
establecido que el primer momento muestral $\overline{X}$ es insesgado en el
punto \textit{(1)}.

Observe que 

\begin{align*}
    \mathbb{V}\left[ X_1 \right] = \lambda \\ 
    \frac{1}{4}\mathbb{V}\left[ X_1 + X_2 \right] = \frac{1}{4}\lambda \\ 
    \frac{1}{9}\left[ \lambda + \frac{1}{4}\lambda + \lambda \right] = \left[
    \frac{1}{9} \frac{9\lambda}{4} \right] = \frac{\lambda}{4}
\end{align*}

El mejor estimador es el segundo, porque de los insesgados es el que tiene menor
varianza.

\subsection{Final 2021-07-26}

\begin{problem}
    In making a certain article, two types of defect exist: Type I and Type II.
    Type I occurs $5\%$ of the times; type II occurs $10\%$ of the times. We can
    assume the occurrence of one defect is independent of the occurrence of the
    other. A random article is selected. \textit{(1)} What is the probability
    that it is flawed? \textit{(2)} Assuming it is flawed, what is the
    probability that it only contains a Type I defect?
\end{problem}


\small
\begin{quote}

\textit{(1)} The probability that it is flawed is simply $P(\text{Type I} \cup
\text{Type II}) = P(\text{Type I}) +
P(\text{Type II}) - P(\text{Type I} \cap  \text{Type 2})$. Since the defects are
independent, this gives $.05 + .1 - .05 \cdot .1 = .145 $.

\textit{(2)} For brevity, let $A$ denote the event of a Type I defect, $B$ the
event of a Type II defect. Then we want to find 

\begin{align*}
    P( A \cap \neg B \mid A \cup B ) &= \frac{P\left( \left( A \cap \neg B
    \right) \cap \left( A \cup B \right)   \right) }{P(A \cup B)} \\ 
                                     &= \frac{P\left( (A \cap \neg B \cap A)
                                     \cup (A \cap  \neg B \cap B)  \right)
                                 }{.145} \\ 
                                 &= \frac{P\left( \left( A \cap \neg B \right)
                                 \cup \emptyset  \right) }{.145} \\ 
                                 &= \frac{P\left( A \cap \neg B \right) }{.145}
                                 \\ 
                                 &= \frac{.05 \cdot (1 - .1)}{.145} \\ 
                                 &=.310
\end{align*}

In other words, assuming that an item is flawed, the probability that its flaw
is of Type I is $31\%$.

\end{quote}
\normalsize

\pagebreak

\begin{problem}
    Let $X$ a random var with CDF 

    \begin{align*}
        F(x) = \begin{cases}
            0 & x < -2 \\ 
            \frac{( x+2 )^2}{8} & -2 \leq x < 0 \\ 
            1 - \frac{(-x+2)^2}{8} & 0 \leq x < 2 \\ 
            1 & x \geq 2
        \end{cases}
    \end{align*}

    \textit{(1)} Is $X$ continuous or discrete? Justify. \textit{(2)} Find the
    PDF or PMF of $X$. \textit{(3)} Find the $25$ percentile of $X$.
    \textit{(4)} Find the expected value and standard deviation of $X$.
\end{problem}


\small
\begin{quote}

\textit{(1)} $X$ is said to be discrete if there is some finite or countably infinite set $A$
s.t. $P(X \in A) = 1$. Evidently $P(X \in A) = 1$ if and only if $A =  \left[
    c,
2\right] $ with $c \in \mathbb{R}$ and $c \leq -2$. Any set of this form is
infinite. Then $X$ is continuous. 

\textit{(2)} The PDF will be the case-to-case derivative of the CDF: 

\begin{align*}
    f(x) = \begin{cases}
        0 & x < - 2 \\
        \frac{2(x+2)}{8} & -2 \leq x < 0 \\ 
        1 + \frac{ 2(-x+2) }{8} & 0 \leq x < 2 \\ 
        0 & x \geq 2
    \end{cases}
\end{align*}

To make sure this is correct, you can cerify that $\int_{\mathbb{R}}f(x) ~ dx =
1$ (I skip this). 

\textit{(3)} We first need to determine which "part" of the function $f$
contains the 25 percentile. Lets integrate the first "part": 

\begin{align*}
    \frac{1}{4}\int_{-2}^{0}(x+2) ~ dx = \frac{1}{2}
\end{align*}

Since $50\%$ of the probability lies within the region $(-2, 0]$, clearly the 25
percentile is within this region. Now observe that

\begin{align*}
    \frac{1}{4} \int_{-2}^{t}(x+2) ~ dx &= \frac{1}{4}\left[ \left(\frac{t^2}{2}
            - 2\right)
    + \left( 2t + 4  \right) \right]  \\ 
                                        &= \frac{t^2}{8}-\frac{1}{2} +
                                        \frac{t}{2} + 1 \\ 
\end{align*}

We want to find the $t$ that contains $25\%$ of the distribution. Solving the
equation 

\begin{align*}
    \frac{t^2}{8} - \frac{1}{2} + \frac{t}{2} + 1 &= .25  \\ 
    \frac{t^2 + 4t}{8} &= -.25 \\ 
    t^2 + 4t &= -2 \\ 
    t^2 + 4t + 2 &= 0
\end{align*}

The roots are $-2 \pm \sqrt{2}$ . Obviously, $-2 - \sqrt{2} $ falls out of the
range we are interested in. Then $-2 + \sqrt{2} $ is the $25$ percentile.

\textit{(4)} We skip the calculations but give the formula. The expected value
is 

\begin{align*}
    \mathbb{E}\left[ X \right]  = \frac{1}{4} \int_{-2}^{0} (x+2)x~dx +
    \int_{0}^{2}x~dx + \frac{1}{4}\int_0^{2}(-x + 2)x ~dx = 2
\end{align*}

Then

\begin{align*}
    \mathbb{V}\left[ X \right] &= \mathbb{E}\left[ (X - 2)^2 \right]  \\ 
                               &= \frac{1}{4} \int_{-2}^{0} (x+2)(x-2)^2~dx +
    \int_{0}^{2}(x-2)^2~dx + \frac{1}{4}\int_0^{2}(-x + 2)(x-2)^2 ~dx \\&=
    \frac{22}{3}
\end{align*}

\end{quote}
\normalsize

\pagebreak 

\begin{problem}
    Let $X \sim (40, 8), Y \sim (30, 6)$. \textit{(1)} Give the
    probability that $Y \in [17.52, 33.84]$. \textit{(2)} Give the probability
    that $Y \leq X$. \textit{(3)} Ten draws $Y_1, \ldots, Y_{10}$ are taken;
    what is the probability that only three of the ten draws exceeds $33.84$?
    And what the probability that $\overline{Y}$ is inferior to $33.84$?
\end{problem}



\small
\begin{quote}

\textit{(1)} Such probability is $\Phi( \frac{33.84 - 30}{6} ) - 
\Phi( \frac{17.52 -30 }{6} ) = \Phi(.64) - \Phi(-2.08)$. Using the $z$-score
table we observe that this gives $.738 - .018 = .72$. The probability is $72\%$.

\textit{(2)} Observe that $Y \leq X \iff Y - X \leq 0$. Using the properties of
normal distributions, we have that $Z = Y - X \sim \mathcal{N}(-10,  14)$. Then
we require to compute only $\Phi( \frac{10}{14} ) = \Phi(0.714) = .761$.

\textit{(3)} The experiment is binomial with $p = P(X > 33.84) = 1 - P(X \leq
33.84) = 1 - .72 = .28$ and $n = 10$. Then the desired event has probability

\begin{align*}
    \binom{10}{3}(.28)^{3}(.72)^{7}= .264
\end{align*}

We know $\overline{Y} \sim \mathcal{N}(30, \frac{6}{\sqrt{10} })$. Then
$P(\overline{Y} \leq 33.84)$ is given by 

\begin{align*}
    \Phi(\frac{3.84 \times \sqrt{10} }{6}) = \Phi(2.023) = .978
\end{align*}

\end{quote}
\normalsize

\pagebreak 

\begin{problem}
    An article says only one out of three people get a job after college. A
    study found $85$ out of $200$ people got jobs. \textit{(1)} Build a $98\%$
    confidence interval for the true proportion. \textit{(2)} Can you conclude
    with a significance level $\alpha = .02$ that the proportion is
    \textit{greater} than the one published in the article?
\end{problem}


\small
\begin{quote}

We have $\hat{p} = .425$. The standard deviation of this estimator is 

\begin{align*}
    s_{\hat{p}} = \sqrt{ \frac{(.425)(.575)}{200} }  = .035
\end{align*}

Since the estimator approximates a normal distribution with sufficiently large
samples, we build the confidence interval $\hat{p} \pm
s_{\hat{p}} \cdot z_{.01} = .425 \pm .035 \cdot 2.32$. This gives the
interval $\left[ 0.3438, 0.5062 \right] $.

\textit{(2)} Let $H_0 :p = 0.33, H_a : p > 0.33$. Let us assume $H_0$ holds. We
ask: What is the probability of having a value as extreme or more than $\overline{p} = .425$ under
this assumption? In other words, what is the area under the curve of the
distribution of $p$, under hypothesis $H_0$, to the right of $.425$? Since

\begin{align*}
    \Phi( \frac{.425 - .33}{.035} ) = \Phi(2.71) = .996
\end{align*}

is the area to the left of $.425$, $1 - .996 = .004$ is the area to the right.
Incidentally, this is the $p$-value. Since the $p$-value is less than $.02$ we
reject the null hypothesis and accept the alternative hypothesis.

\end{quote}
\normalsize

\pagebreak 

\begin{problem}[]
    $25$ measures of the amount of a substance were made with a mean $7975$ and
    $s_n = 74$. Assume the random variable follows $\mathcal{N}(\mu, \sigma)$.
    \textit{(1)} Give a MLE estimator of $\sqrt{\mu}, \sigma^2 $ and $P(X \leq
    7990)$. Justify. \textit{(2)} Give a confidence interval of $95\%$ for
    $\mu$. \textit{(3)} Is there evidence to conclude that $\mu > 7950$?
    ($\alpha = .05$)
    \textit{(4)} Assume $\sigma = 73$, is there evidence to conclude that $\mu >
    7950$? ($\alpha = .05$)
\end{problem}


\small
\begin{quote}

\textit{(1)} It is a property of MLE estimation that it satisfies
\textit{functional invariance}. This means that if $\hat{\theta}$ is the MLE of
$\theta$, then $g(\hat{\theta})$ is the MLE of $g(\theta)$. We know
$\overline{X}$ is the MLE of $\mu$. Then $\sqrt{\overline{X}} $ is the MLE of
$\sqrt{\mu} $. The same principle gives that $s_n^2$ is the MLE estimator of
$\sigma^2$. Once more, the same priniciples give that the MLE estimator of $P(X
\leq 7990)$ is $\Phi\left( \frac{7990 - \overline{X}}{s_n} \right)$.

\textit{(2)} We must use the $t$-Student distribution with $24$ degrees of
freedom because $n = 25 \leq 30$ and
$\sigma$ is unknown. We then have the interval $\overline{X} \pm s_n \cdot
t_{.025}$. This gives $\left[ 7975 - 74 \cdot 2.064, 7975 + 74 \cdot 2.064
\right] = \left[ 7822.264, 8117.736 \right]  $. 

\textit{(3)} Let $H_0 : \mu = 7950, H_a : \mu > 7950$. Assuming $H_0$, the
probability of observing a value as extreme or more (to the right) than $7975$
is 

\begin{align*}
    1 - \Phi( \frac{7975 - 7950}{74} ) = 1 - \Phi(0.338) = 1 - .62930 = 0.3707
\end{align*}

This is the $p$-value. There isn't enough evidence to reject the null
hypothesis.

\textit{(4)} If we assume $\sigma = 73$, then

    
\begin{align*}
    1 - \Phi( \frac{7975 - 7950}{73} ) = 1 - \Phi(0.342) 
\end{align*}

and we still do not reject.


\end{quote}
\normalsize

\pagebreak 

\begin{problem}
    Let $X1, \ldots, X_n$ a random sample with uniform distribution
    $\mathcal{U}\left[\theta; \theta + 1  \right] $, with $\theta > 0$.
    \textit{(1)} Consider $\hat{\theta} = \max X_i$ an estimator of $\theta$
    whose PDF is 

    \begin{align*}
        f_{\hat{\theta}}(x) = \begin{cases}
            n(x - \theta)^{n-1} & x \in (\theta, \theta + 1) \\ 
            0 & otherwise
        \end{cases}
    \end{align*}

    Find the expected value of $\hat{\theta}$.  \textit{(2)} Find the method of
    moments estimator of $\theta$. Is it unbiased? \textit{(3)} Let
    $\hat{\theta}_2 = \hat{\theta} - \frac{n}{n+1}$. Es it an unbiased estimator
    of $\theta$? 
\end{problem}


\small
\begin{quote}

\textit{(1)} By definition, 

\begin{align*}
    \mathbb{E}\left[ \hat{\theta} \right] = n\int_{\theta}^{\theta+1} x(x -
    \theta)^{n-1} ~ dx
\end{align*}

Let $u = x - \theta$ s.t. $x = u + \theta$ and $du = dx$. 

\begin{align*}
    \mathbb{E}\left[ \hat{\theta} \right] &= n\int_{u(\theta)}^{u(\theta+1)} (u +
    \theta)u^{n-1} ~ du \\ 
                                          &= n\int_{0}^{1} u^{n} + \theta u^{n-1}
                                          ~ du \\ 
                                          &= \frac{n}{n+1} + \theta
\end{align*}

\textit{(2)} The first sample moment is $\overline{X}$. The first moment of the
uniform distribution is $\frac{a + b}{2}$, or in our case $\frac{2\theta +
1}{2}$. Then 

\begin{align*}
    \overline{X} = \theta + \frac{1}{2} \Rightarrow \theta = \overline{X} -
    \frac{1}{2}
\end{align*}

is the method of moments estimator of $\theta$. Observe that 

\begin{align*}
    \mathbb{E}\left[ \overline{X} - \frac{1}{2} \right] &= \mathbb{E}\left[
    \overline{X} \right] -\frac{1}{2} \\ 
                                                        &= \frac{1}{n}\sum \mathbb{E}
                                                        \left[ X_i \right]  -
                                                        \frac{1}{2} \\ 
                                                        &= \theta + \frac{1}{2}
                                                        - \frac{1}{2} \\ 
                                                        &= \theta
\end{align*}

The estimator is by definition unbiased. 

\textit{(3)} Observe that 

\begin{align*}
    \mathbb{E} \left[ \hat{\theta} - \frac{n}{n+1} \right] &= \mathbb{E}\left[
    \hat{\theta} \right]  - \frac{n}{n+1} \\ 
                                                           &= \frac{n}{n+1} +
                                                           \theta -
                                                           \frac{n}{n+1} \\ 
                                                           &= \theta
\end{align*}

It is unbiased.

\end{quote}
\normalsize

\pagebreak 

\subsection{Final 2022-02-10}

\begin{problem}
    A box contains $400$ items of which $176$ are worth $70\$$,120 are worth
    $50\$$, and the rest are worth $30\$$. A randomly chosen item is selected
    and sold at $50\$$. \textit{(1)} What is the probability of having sold an
    item worth at least $50\$$? \textit{(2)} What is the probability of having
    sold one that worth $70\$$ if we know that it was worth \textit{at least}
    $50\$$? \textit{(3)} Let $G$ denote the variable denoting how much
    was lost/won in the sell. Find its PMF, its CDF, its expected value and its
    standard deviation.
\end{problem}


\small
\begin{quote}

    Let $\Omega = \left\{ A_{1}, \ldots, A_{176}, B_{1}, \ldots, B_{120}, C_{1},
    .., C_{104}\right\} $ denote the events of finding an arbitrary item of each
    worth. Let $\mathcal{A} = \mathcal{P}(\Omega)$ be the associated
    $\sigma$-algebra. Let $P(X) = \frac{1}{400}$ for any $X \subseteq
    \mathcal{A}$ s.t. $|X| = 1$.

    \textit{(1)} Let $P(A) = P(A_1 \cup \ldots \cup A_{176})$, and the same for
    $P(B), P(C)$. $P(A \cup B) = P(A) + P(B) = \frac{176}{400} + \frac{120}{400}
    = .74$.

    \textit{(2)} We are asked for $P(A \mid A \cup B)$. We know this is 

    \begin{align*}
        \frac{P\left( A \cap \left( A \cup B \right)  \right)}{P(A \cup B)} &=
        \frac{P(A)}{.74} = .594
    \end{align*}


    \textit{(3)} Clearly, $G : \left\{ -20, 0, 20 \right\} \mapsto \mathbb{R}$. In
    particular, $p_{G}(-20) = .44, p_{G}(0) = .3, p_{G}(20) = .26$. This entirely
    defines the PMF. The CDF is 
    
    \begin{align*}
        F_{G}(x) = \begin{cases}
            .44 & x = -20 \\ 
             .74 & x = 0 \\ 
             1 &= x = 20
        \end{cases}
    \end{align*}

    Now, $\mathbb{E}\left[ G \right] = \sum_{y \in   \mathcal{D}_{G}} yp_G(y) =
    -20(.44) + 20(.26) = -3.6$. The variance is 

    \begin{align*}
        \mathbb{E}\left[ (G + 3.6)^2 \right] &= \sum_{y \in Im(G)} (y +
        3.6)^2p_G(y) \\ 
                                             &= 118.34 + 144.80 \\ 
                                             &= 263.14
    \end{align*}

    The standard deviation is then $\sqrt{263.14} = 16.22 $.
    

\end{quote}
\normalsize

\pagebreak 

\begin{problem}
    The time (in hours) required for an event is a random variable $X$ with PDF 

    \begin{align*}
        f(x) = \begin{cases}
            c x^2 + x & 0 < x < 1 \\ 
            0 & otherwise
        \end{cases}
    \end{align*}

    \textit{(1)} Find the value of $c$; \textit{(2)} find the probability that
    the event occurs in less than half an hour; \textit{(3)} find the
    probability that the event takes at least half an hour assuming it takes at
    least $15$ minutes to occur. \textit{(4)} Find the expected value and
    variance of $X$.
\end{problem}


\small
\begin{quote}

We require that 

\begin{align*}
    c \int_{0}^{1} x^2 ~ dx + \int_{0}^{1} x~ dx &= 1
\end{align*}

Observe that $c\int_0^1 x^2 ~ dx + \int_{0}^1 x ~ dx = \frac{c}{3} +
\frac{1}{2}$. Then the equation gives $c =\frac{3}{2}$. We should preserve in
mind the fact that the CDF of $f$ is

\begin{align*}
    F(x) = \int f(x) ~ dx = \frac{x^3}{2} + \frac{x^2}{2} + C
\end{align*}

The probability that the event occurs in less than half an hour is
$F(\frac{1}{2}) = \frac{1}{16} + \frac{1}{8} = \frac{3}{16}$. The probability
that it takes \textit{at least} half an hour assuming it takes \textit{at least}
fifteen minutes is 

\begin{align*}
    P( X \geq \frac{1}{2} \mid X \geq \frac{1}{4}) &= 
    \frac{ P(X \geq \frac{1}{2} \cap X \geq \frac{1}{4}) }{P(X \geq
    \frac{1}{4})} \\ 
                                                   &= \frac{P(X \geq
                                                   \frac{1}{2})}{P(X \geq
                                               \frac{1}{4})} \\ 
                                                   &= \frac{ 1 - F(\frac{1}{2})
                                                   }{1 - F(\frac{1}{4})} \\ 
                                                   &=0.845
\end{align*}

\end{quote}
\normalsize


\pagebreak

\begin{problem}
    Wires in a computer are supposed to have a resistance between of $0.12$ and
    $0.14$ ohms. Assume two companies, $A$ and $B$, produce wires with normally
    distributed resistances. In the case of $A$, $X \sim \mathcal{N}(.13,
    .005)$; in the case of $B$, $Y \sim \mathcal{N}(.125, .005)$.

    \textit{(1)} Which company has more probability of producing good wires? 

    \textit{(2)} Find the probability that company $A$ produces a wire with more
    resistance than one produced by $B$.

    \textit{(3)} Nine wires are randomly selected from $B$. Find the probability
    that \textit{at least} eight are good, and find the $30$ percentile for the
    sample mean.
\end{problem}


\small
\begin{quote}

\textit{(1)} Intuitively, it must be $A$, because its mean is right in between
the standard limits, and the standard deviations are the same. But we can prove
this. Observe that $P(X \in [.12, .14]) = \Phi \left( \frac{.14 - .13}{.005}
\right) - \Phi\left( \frac{.12 - .13}{.005} \right) $. This gives 

\begin{align*}
    \Phi(2) - \Phi(-2) = .975 -.022 = .953
\end{align*}

The same logic gives $P(Y \in [.12, .13]) = \Phi\left( \frac{.14 - .125}{.005}
    \right) -
\Phi\left( \frac{.12 - .125}{.005} \right) $, which yields

\begin{align*}
    \Phi(3) - \Phi(-1) = .998 - .158 = .84
\end{align*}

This proves $A$ is more likely to satisfy the standards.

\textit{(2)} Observe that $P(X > Y) = P(X - Y > 0)$. Using the properties of
normal distribution, we know $Z = X - Y \sim \mathcal{N}(.005, .01)$. Then 

\begin{align*}
    P(Z > 0) &= 1 - P(Z \leq 0)  \\ 
             &= 1 - \Phi\left( -\frac{.005}{.01} \right) \\ 
             &= 1 - \Phi\left(-\frac{1}{2}\right) \\ 
             &= 1 - .308 \\ 
             &= .692
\end{align*}

\textit{(3)} This can be modeled as a binomial experiment with $n = 10$ and $p =
P(Y \in [.12. 14]) = .84$. The probability that at least eight successes or
occur is the probability that eight, nine or ten successes occur. In other
words, it is 

\begin{align*}
    &\binom{10}{8}(.84)^{8}(.16)^2 + 
    \binom{10}{9}(.94)^{8}(.16)^1 + 
    \binom{10}{10}(.84)^{10}(.16)^0 
\end{align*}

We do not calculate this here because it is simply a matter of grabbing the
calculator.

Now, we must recall that given a sample $Y_1, \ldots, Y_{10}$ we have
$\overline{X} \sim (.125, \frac{.005}{\sqrt{10} })$, or rather $\overline{X}
\sim (.125, \approx .0015)$. The $30$ percentile of this distribution will be 

\begin{align*}
    \arg_z ~ \int_0^{z} f_{\overline{X}}(t) ~ dt = .3
\end{align*}

In the standard normal distribution, it is the $z$-score $-0.5$ that which
contains $30\%$ of the distribution "behind". Therefore, we simply require to
find the solution of 

\begin{align*}
    &\frac{y - .125}{.005}\sqrt{10}  &= -\frac{1}{2} \\ 
    &(y - .125)\sqrt{10}  &= -.0025 \\ 
    &y - .125 &= .0008 \\ 
    & y &= .1258
\end{align*}

In other words, $F_{\overline{X}}(.1258) = .3$. So, this is the percentile $30$.

\end{quote}
\normalsize

\pagebreak 

\begin{problem}
    According to the ministry, $30\%$ of women in a country smoke. A sample of $1200$ women is taken;
    $312$ were smokers. \textit{(1)} Build a confidence interval of $95\%$ for
    $p$. \textit{(2)} If an interval is made with these data and it has a length
    of $.03$, what is the confidence of the interval? Comparing this with the
    one of \textit{(1)}, say which one is more precise. \textit{(3)} Is there
    sufficient evidence to say the proportion of women in the country differs
    from what the ministry says?
\end{problem}


\small
\begin{quote}

\textit{(1)} We have $\hat{p} = .26$. With this large sample size, $\hat{p} \sim
\mathcal{N}(1200p, .012)$. Thus, our desired confidence interval is given by
$.26 \pm z_{.025} ~ .012$. Since $z_{.025} = -1.96$ we have 

\begin{align*}
    CI = .26 \pm 1.96 (.012) = [.23648, .28352]
\end{align*}

\textit{(2)} Any interval built with these values is of the form $.26 \pm
z_{\frac{\alpha}{2}} ~ (.012)$. We ask for the value of $\alpha$ s.t. $.26 +
z_{\frac{\alpha}{2}}(.012) - (.26 - z_{\frac{\alpha}{2}}(.012)) = .03$. This gives 

\begin{align*}
    2z_{\frac{\alpha}{2}} = .03 \Rightarrow z_{\frac{\alpha}{2}} = .015
\end{align*}

Now we must remember the relationship betwenn $z_{\alpha}$ and $\alpha$. In
particular, we recall that $z_{\alpha} = u$ if and only if $\Phi(u) = \alpha$.
In our particular case, $\Phi(.015) = 1 - \frac{\alpha}{2}$. Then $\Phi(0.15) =
1 - .55962 = \frac{\alpha}{2} \Rightarrow \frac{\alpha}{2} = .44038 \Rightarrow
\alpha=.88076$. In other words, we would be dealing with a confidence interval
with $88.076\%$ confidence. This interval would be more precise than the
previous one, because it would be smaller---hence the loss in confidence. 

\textit{(3)} Let $H_0 : p = .3, H_a : p \neq .3$. Let us assume that the null
hypothesis holds for a moment. For a $p$-value of $.01$ in a two-tailed test, we
must test with an $\alpha$ level of $.005$. This approximately corresoponds to a
$z$-score of $\pm 2.567$. Under $H_0$, the standard deviation is
$\sqrt{\frac{.3(.7)}{1200}} = .013 $. Observing that $.3 \pm 2.567(.013)$ gives
$.26, .33$, respectively, we see that

\begin{align*}
    P(\hat{p} < .26 \cup  \hat{p} > .33) &= 1 - P( \hat{p} \in [.26, .33] ) \\ 
                                         &=1 - [\Phi( 2.307 ) -
                                         -1 \Phi(-3.076)] \\ 
                                         &= 1 - (.98928 - .00135) \\ 
                                         &= 1- .98793 \\ 
                                         &= .01207
\end{align*}

This is the $p$-value. Since this value is greater than $.01$, we do not reject
the null hypothesis.

\end{quote}
\normalsize

\pagebreak

\begin{problem}
    Let $X \sim \mathcal{N}(\mu, \sigma )$. A random sample $X_1, \ldots,
    X_{15}$ is obtained with $\overline{X} = 10.88$ and $s_{n-1} = 2.082$.
    \textit{(1)} Give MLE estimators of $(u + 3 \sigma)$, the $90$ percentile
    of $X$, and $P(X \geq 9)$. \textit{(2)} Find estimators of $\mu, \sigma^2$
    via the method of moments. \textit{(3)} Find a confidence interval of $99\%$
    for $\mu$. \textit{(4)} Is there sufficient evidence to conclude from the
    sample that $\mu > 9$?
\end{problem}


\small
\begin{quote}

\textit{(1)} Recall that ML estimators are functionally invariant. From this
follows that the MLE of $u + 3\sigma$ is $\overline{X} + 3 s_{n-1}$. The same
principle gives as best estimation of $P(X \geq 9)$:

\begin{align*}
    \Phi \left( \frac{9 - 10.88}{2.082} \right) =
    \Phi(-.902) = .178
\end{align*}

Furthermore, let us strictly pose $X' \sim \mathcal{N}(10.88, 2.082)$. Then we
find as estimator the 90 percentile of this distribution (functional invariance
of MLE). We solve

\begin{align*}
    \Phi\left( \frac{z - 10.88}{2.082} \right) = .90 \iff \left( \frac{z -
    10.88}{2.082} \right) \approx 1.285
\end{align*}

Then we have $z = 13.55537$ the $90$ percentile.


\textit{(2)} The first sample moment is $\overline{X}$ and the first moment is
$\mu$, so immediately we have $\hat{\mu} = \overline{X}$. I don't remember the
second moment of $\mathcal{N}$ so will derive it. The simplest way of deriving
it is to remember that $\mathbb{V}\left[ X \right] = \mathbb{E}\left[ X^2
\right] - \mathbb{E}\left[ X \right]^2 \Rightarrow \mathbb{E}\left[ X^2 \right]=
\sigma^2 + \mu^2$. Then we make 

\begin{align*}
    &\sigma^2 + \mu^2 = \frac{1}{n} \sum X_i^2 \\ 
    &\sigma^2 = \frac{1}{n} \sum X_i^2 - \mu^2 \\ 
    &\sigma^2 =  \frac{1}{n} \sum (X_i - \mu)^2 \\ 
    &\sigma^2 = s_n^2
\end{align*}


\textit{(3)} Since $\alpha = .01$ we have $\frac{\alpha}{2} = .005$. We know
$z_{.005} = 2.58$ Then the desired interval is $10.88 \pm 2.082 \cdot 2.58 = [
5.50844, 16.12156  ] $.

\textit{(4)} Let $H_0 : \mu = 9, H_a : \mu > 9$. The test statistic we use is $t
= \frac{10.88 - 9}{2.082} \sqrt{15}  = 3.497$. The probability of finding a sample mean as extreme
or more (to the right) than $10.88$, under the null hypothesis, is $1 -
F(3.497)$, where $F$ is the CDF of the $t$ distribution. For $14$ degrees of
freedom, that value $2.977$ already has a probability of $.005$, so our $p$
value on this occasion is $p < .005$. So we reject the null hypothesis.


\end{quote}
\normalsize

\pagebreak 

\begin{problem}

    Let $X$ have PDF 

    \begin{align*}
        f(x; \theta) = \begin{cases}
            \frac{x}{\theta} \exp (-\frac{x^2}{2\theta}) & x > 0 \\ 
            0 & otherwise
        \end{cases}
    \end{align*}

    and $X_1, \ldots, X_n$ a random sample. (1) Prove $\mathbb{E}\left[ X^2 \right]
    = 2 \theta$. (2) Find $k$ s.t. $\hat{\theta} = k \sum X_i^2$ is unbiased.
    
\end{problem}


\small
\begin{quote}

\textit{(1)} Let $u = -\frac{x^2}{2\theta}$, so that $du = -\frac{x}{\theta}
dx$ and $x^2 = -u 2\theta$. Then

\begin{align*}
    \int \frac{x^3}{\theta} \exp\left( -\frac{x^2}{2\theta} \right) &= -\int
    x^2  e^u \left(-\frac{x}{\theta}\right) ~ dx \\ 
                                                                   &= -\int
                                                                   (-u
                                                                   2\theta)e^u
                                                                   ~ du
\end{align*}

Using integration by parts, this gives 

\begin{align*}
    2\theta \left[ ue^u - e^u \right] &= 2\theta \left[ -\frac{x^2}{2\theta}
    e^{-\frac{x^2}{2\theta}} - e^{-\frac{x^2}{2\theta}}\right]  \\ 
                                       &= -x^2 e^{-\frac{x^2}{2\theta}} -
                                       2\theta e^{-\frac{x^2}{2\theta}}
\end{align*}


Since $e^{ -\frac{x^2}{2\theta} } \to 0$ when $x \to \infty$ we have 

\begin{align*}
    \int_0^{\infty} f(x)x^2 ~ dx &= \lim_{t \to \infty} \left[ \left( t^2 e^{-\frac{t^2}{2\theta}} + 2\theta
    e^{-\frac{t^2}{2\theta}} \right)  - (0 - 2\theta)\right]  \\ 
    &= 2\theta ~ \blacksquare
\end{align*}

\textit{(2)} Observe that 

\begin{align*}
    \mathbb{E}\left[ k \sum X_i^2  \right] &= k \sum \mathbb{E}\left[ X_i^2
    \right]  \\ 
                                           &= k  n 2\theta
\end{align*}

For this estimator to be unbiased, we require 

\begin{align*}
    kn2\theta = \theta \iff k = \frac{1}{2n}
\end{align*}

\end{quote}
\normalsize


\pagebreak 

\subsection{Final 2021-12-21}

\begin{problem}
    Let $A, B, C$ be events in a sample space $\Omega$. Let $P(A) = .1, P(B) =
    .08, P(C) = .12$. Assume $A, B, C$ are independent. \textit{(1)} Give the
    probability that all $A, B, C$ occur. \textit{(2)} Give the probability that
    $A$ and not $B$ occur. \textit{(3)} Give the probability that exactly one of
    the three events happen. \textit{(4)} Give the probability that $C$ occurs
    assuming exactly two events occurred.
\end{problem}


\small
\begin{quote}

\textit{(1)} Since they are independent this probability is simply $.1 \times
.08 \times .12 = .00096$.

\textit{(2)} $P(A \cap \overline{B}) = .1 \times .92$.

\textit{(3)} Observe that

\begin{align*}
    P\left(A \cap (\overline{B} \cap \overline{C})\right) &= .1 \times .92
    \times . 88 = .08096\\
    P\left(B \cap (\overline{A} \cap \overline{C})\right) &= .08 \times .9
    \times . 88 = .06336\\
    P\left(C \cap (\overline{A} \cap \overline{B})\right) &= .12 \times .9
    \times .92 = .09936
\end{align*}

The events of each of these probabilities are obviously mutually exclusive. Then
the probability that either of these events occur is their sum, which gives
$.24368$.

\textit{(4)} Obviously if we assume $C$ is one of the two events which occurred
the probability of $C$ is $1$. So let us inspect the case $P(C \mid A \cap
B)$. This gives 

\begin{align*}
    \frac{P \left( C \cap (A \cap B) \right) }{P(A \cap B)} =
    \frac{.00096}{.008} = .12
\end{align*}

\end{quote}
\normalsize

\pagebreak 

\begin{problem}
    Let $X$ a r.v. with CDF 

    \begin{align*}
        F(x) = \begin{cases}
            a & x < 1 \\ 
            b(x-1)^2 & x \in [1, 5) \\ 
            c & x \geq 5
        \end{cases}
    \end{align*}

    with $a, b, c$ constants. \textit{(1)} Find the value of the constants.
    \textit{(2)} Find the PDF of $X$. \textit{(3)} Find the expected value and
    variance of $X$. \textit{(4)} Find the expected value of $W = 7X^2 - 8X$.
\end{problem}


\small
\begin{quote}

\textit{(1)} To satisfy the definition of a CDF, we require that $a = 0$ and $c
= 1$, and

\begin{align*}
    F(5) - F(1) = 1 \iff 16b = 1 \Rightarrow b = \frac{1}{16}
\end{align*}

\textit{(2)} The PDF is $f(x) = \frac{1}{16}(2x - 2) = \frac{1}{8}(x - 1)$ in $[1, 5)$ and zero
otherwise. We can verify that $\frac{1}{8}\int_1^{5} (x - 1) ~ dx =
\frac{1}{8}\left[ 12 - 4 \right] = 1$.

\textit{(3)} 

\begin{align*}
    \mathbb{E}\left[ X \right]  &= \frac{1}{8}\int_1^{5} x^2 - x ~ dx \\ 
    &= \frac{1}{8} \left[ \frac{124}{3} - 12   \right] \\ 
    &= 3.666
\end{align*}

Furthermore,

\begin{align*}
    \mathbb{E}\left[ X^2 \right]  &= \frac{1}{8} \int_1^{5} x^3 - x^2 ~ dx \\ 
    &= \frac{1}{8} \left[ \frac{624}{4} - \frac{124}{3} \right]  \\ 
    &= \frac{43}{3}
\end{align*}

Then $\mathbb{V}\left[ X \right] = \frac{44}{3} - (\frac{11}{3})^2 \approx
\frac{8}{9}$. 

\textit{(4)} We have 

\begin{align*}
    \mathbb{E}\left[ 7X^2 - 8x \right] &= 7\mathbb{E}\left[ X^2 \right] -8
    \mathbb{E}\left[ X \right]  \\ 
                                       &=7 \frac{43}{3} - 8 \frac{11}{3} \\ 
                                       &= 9 \frac{2}{3}
\end{align*}

\end{quote}
\normalsize




































\end{document}

